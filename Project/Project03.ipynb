{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simon-mellergaard/RL/blob/main/Project/Project03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions\n",
        "\n",
        "This notebook contains the third project assignment. You work on the project in groups. You should already have formed a group. If not, do it on Brightspace under *My Courses > Groups*. To get started, do the following steps:\n",
        "\n",
        "1) One student in the group makes a copy of this notebook.\n",
        "2) Share it with your group using *Share* top-right. Here, add group members under people with access.\n",
        "3) Moreover, under *Share > General access*, choose *Anyone with the link > Commenter*. Copy this link and paste it below.\n",
        "4) Work with the notebook and solve the tasks.\n",
        "5) Hand in the notebook by downloading it: File > Download > Download .ipynb. Next, on BS under *Project*, upload the file (I need that for the exam). Moreover, add the shared link of your Colab notebook as a link when you handin too.\n",
        "6) After hand-in do peer grading (see BS under *Project*)\n",
        "\n",
        "Sharing link: [add your link]\n",
        "\n",
        "**Deadlines**\n",
        "\n",
        "* Hand-in solution 7/12/25\n",
        "* Peer grading 13/12/25\n"
      ],
      "metadata": {
        "id": "tHx3pQBrT8ob"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlrET7d3WOQ2"
      },
      "source": [
        "# Notebook pre steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0vEJAmyWKPb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a252c34-e725-4308-8baa-81d20d04d39a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dfply in /usr/local/lib/python3.12/dist-packages (0.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from dfply) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from dfply) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->dfply) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->dfply) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->dfply) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->dfply) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "#@title Installations\n",
        "\n",
        "# install missing packages\n",
        "!pip install -q dfply\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "from plotnine import *\n",
        "import joblib\n",
        "import gzip\n",
        "import gdown\n",
        "import warnings\n",
        "from scipy.stats import norm\n",
        "from dfply import *\n",
        "import itertools\n",
        "from IPython.display import Markdown\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem description\n",
        "\n",
        "Consider the [Hour-to-hour electricity market problem](https://colab.research.google.com/drive/1BFI6iRLTnrHgxgFayC89SKhuM3XcIp7s?usp=sharing) that we solved using a Markov decision process by discretising the state and action space.\n",
        "\n",
        "Here, we are going to solve the problem using RL with approximation. How to solve the problem depends on which environment your are using. You may use one of the following environment for the problem:\n",
        "\n",
        "1. The environment given in [Project 2](https://colab.research.google.com/drive/1HhQbV2ZvfMsEv-RkoGS6Sj0mjzC9QYF8?usp=sharing) which uses a discretization of both states and actions,\n",
        "2. The environment given in the section \"Environment with discrete actions\" below.\n",
        "3. The environment given in the section \"Environment with continuous actions\" below."
      ],
      "metadata": {
        "id": "2ygAx1kWRwyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In all environments we represent state and actions using tuples $s_t = (s^l_t,s^c_t,s^w_t,s^p_t)$ and $a_t = (a^b_t, a^w_t, a^c_t)$. Moreover, we use settings:"
      ],
      "metadata": {
        "id": "U7srZ__4K6pG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment with discrete actions\n",
        "\n",
        "We here use an environment where\n",
        "\n",
        "1. The battery only can be charged/discharged in units of $\\delta$, e.g. $\\delta = 1$ MWh. That is, $a^b_t \\in \\{-b^{\\max}, \\ldots, -\\delta, 0, \\delta, ..., b^{\\max} \\}$\n",
        "2. Due to 1, the battery level is $s^l_t \\in \\{ 0, \\delta, 2\\delta, ..., b^{\\max} \\}$.\n",
        "3. The same units are used for the wind energy production $a^w_t \\in \\{ 0, \\delta, 2\\delta, ..., p^{\\max} \\}$.\n",
        "4. Due to 1-3, the commitment levels are $s^c_t \\in \\{ -b^\\max, \\ldots, -\\delta, 0, \\delta, \\ldots, b^{\\max} + p^{\\max}\\}$ since we can at most buy energy for an empty battery and we can at most sell max wind production plus a full battery. That is, $a^c_t$ have these commitment levels.\n",
        "\n",
        "Note $\\delta$ must satisfy that $b^{\\max}/\\delta$ and $p^{\\max}/\\delta$ are integer. Moreover, when calculating $E(x)$, its value must be rounded down so contained in the set defined for $a^w_t$.\n",
        "\n",
        "To specify the discrete actions we only need to specify sets for the wind and commitment levels. Hence the battery levels will automatically be discrete too.\n",
        "\n",
        "The environment is implemented below.\n"
      ],
      "metadata": {
        "id": "BAhwuZfxYSV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnvEnergyDiscrete:\n",
        "    \"\"\"\n",
        "    Environment for the hour-to-hour electricity market problem with discrete actions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, settings, a_w_tuple, a_c_tuple, seed = 25328):\n",
        "        \"\"\"\n",
        "        Initialize the environment.\n",
        "\n",
        "        Args:\n",
        "            settings (dict): A dictionary containing problem settings\n",
        "            a_w_tuple (list): A list of possible wind usage\n",
        "            a_c_tuple (list): A list of possible commitment levels\n",
        "\n",
        "\n",
        "            seed (int): A seed for the random number generator\n",
        "        \"\"\"\n",
        "        self.settings = settings\n",
        "\n",
        "        ## price model\n",
        "        # Google Drive direct download URL (using the 'uc?id=' format)\n",
        "        url = 'https://drive.google.com/uc?id=1cGhte06iiWZnaRLPyj5D8ZzWX7gqPsIR'\n",
        "        # Output filename for the downloaded file\n",
        "        output_filename = 'prices_ar1.pkl.gz'\n",
        "        # Download the file from Google Drive\n",
        "        gdown.download(url, output_filename, quiet=True)\n",
        "        # Load the model from the downloaded file\n",
        "        with gzip.open(output_filename, \"rb\") as f:\n",
        "            model_price = joblib.load(f)\n",
        "        print(f\"Price model loaded successfully from {output_filename}\")\n",
        "        self.model_price = model_price\n",
        "\n",
        "        ## wind model\n",
        "        # Google Drive direct download URL (using the 'uc?id=' format)\n",
        "        url = 'https://drive.google.com/uc?id=1TJ1ACzev40QbeUlXBbDicYU3kEyiH1nB'\n",
        "        # Output filename for the downloaded file\n",
        "        output_filename = 'wind_log_ar1.pkl.gz'\n",
        "        # Download the file from Google Drive\n",
        "        gdown.download(url, output_filename, quiet=True)\n",
        "        # Load the model from the downloaded file\n",
        "        with gzip.open(output_filename, \"rb\") as f:\n",
        "            model_wind = joblib.load(f)\n",
        "        print(f\"Wind model loaded successfully from {output_filename}\")\n",
        "        self.model_wind = model_wind\n",
        "\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "        self.a_c_tuple = a_c_tuple\n",
        "        self.a_w_tuple = a_w_tuple\n",
        "\n",
        "    def reset_rng(self, seed):\n",
        "        \"\"\"\n",
        "        Reset the random number generator.\n",
        "\n",
        "        Args:\n",
        "            seed (int): A seed for the random number generator\n",
        "        \"\"\"\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "    def get_left(self, sorted_list, a):\n",
        "        \"\"\"\n",
        "        Given a sorted list and a number 'a', finds the largest number 'b' in\n",
        "        the list so b <= a\n",
        "\n",
        "        Args:\n",
        "            sorted_list (list): A list of numbers sorted in ascending order.\n",
        "            a (float or int): The number to find the bounds for.\n",
        "\n",
        "        Returns:\n",
        "            float or int: The largest number 'b' in the list so b <= a.\n",
        "        \"\"\"\n",
        "        # Iterate through the sorted list to find the bounds\n",
        "        for i in range(len(sorted_list)-1, -1, -1):\n",
        "            if sorted_list[i] <= a:\n",
        "                return sorted_list[i]\n",
        "        return None\n",
        "\n",
        "    def get_actions(self, s: tuple) -> list:\n",
        "        \"\"\"\n",
        "        Get all possible actions for a given state.\n",
        "        \"\"\"\n",
        "        s_l, s_c, s_w, _ = s\n",
        "        b_max = self.settings['b_max']\n",
        "        # Determine a_b and a_w\n",
        "        e_max = self.get_left(self.a_w_tuple, self.energy(s_w)) # max energy we can sell/produce (e.g. if energy(s_w) = 1.25 but a_w_tuple is [0,1,2,...] then can only sell 1)\n",
        "        if s_c >= 0:\n",
        "            if s_c >= e_max:\n",
        "                a_b = min(s_l, s_c - e_max)\n",
        "                a_w = e_max\n",
        "            else: # s_c < e_max:\n",
        "                a_b = -min(b_max - s_l, e_max - s_c)\n",
        "                a_w = s_c - a_b\n",
        "        else: # s_c < 0:\n",
        "            a_b = -min(b_max - s_l, - s_c + e_max)\n",
        "            a_w = max(0, - a_b + s_c)\n",
        "\n",
        "        actions = [(float(a_b), float(a_w), float(a_c)) for a_c in self.a_c_tuple if a_c >= -(b_max - (s_l - a_b))]\n",
        "        # if -(b_max - (s_l - a_b)) > -10:\n",
        "        #     print(f\"s_c: {s_c},  s_w: {s_w}, e_max: {e_max}\")\n",
        "        #     print(f\"s_l: {s_l}, a_b: {a_b}, a_w: {a_w}, min a_c: {-(b_max - (s_l - a_b))}\")\n",
        "        #     print(min([a[2] for a in actions]))\n",
        "        return actions\n",
        "\n",
        "    def state_variables_to_str(self, s_l, s_c, s_w, s_p):\n",
        "        \"\"\"\n",
        "        String representaion of a state.\n",
        "        \"\"\"\n",
        "        return \"{'s_l': \" + str(s_l) + \", 's_c': \" + str(s_c) + \", 's_w': \" + str(s_w) + \", 's_p': \" + str(s_p) + \"}\"\n",
        "\n",
        "    def action_variables_to_str(self, a_b, a_w, a_c):\n",
        "        \"\"\"\n",
        "        String representaion of an action.\n",
        "        \"\"\"\n",
        "        return \"{'a_b': \" + str(a_b) + \", 'a_w': \" + str(a_w) + \", 'a_c': \" + str(a_c) + \"}\"\n",
        "\n",
        "    def power(self, wind_speed):\n",
        "        \"\"\"\n",
        "        The power output of a wind turbine given a wind speed.\n",
        "        \"\"\"\n",
        "        p_max = self.settings['p_max']\n",
        "        w_cut_in = self.settings['w_cut_in']\n",
        "        w_rated = self.settings['w_rated']\n",
        "        w_cut_out = self.settings['w_cut_out']\n",
        "\n",
        "        if wind_speed < w_cut_in:\n",
        "            return 0\n",
        "        elif w_cut_in <= wind_speed <= w_rated:\n",
        "            return p_max * ((wind_speed - w_cut_in) / (w_rated - w_cut_in)) ** 3\n",
        "        elif w_rated < wind_speed <= w_cut_out:\n",
        "            return p_max\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def energy(self, w, time_period_length = 1):\n",
        "        \"\"\"\n",
        "        The energy output of a wind turbine over one hour given a wind speed (MWh).\n",
        "        \"\"\"\n",
        "        return self.power(w) * time_period_length\n",
        "\n",
        "    def energy_inverse(self, energy_target, time_period_length = 1):\n",
        "        \"\"\"\n",
        "        Find the wind speed that produces a given amount of energy, assuming wind is between w_cut_in and w_rated.\n",
        "\n",
        "        Args:\n",
        "            energy_target (float): The target energy output in MWh.\n",
        "            time_period_length (float, optional): The time period length in hours. Defaults to 1.\n",
        "\n",
        "        Returns:\n",
        "            float: The wind speed that produces the given energy.\n",
        "        \"\"\"\n",
        "        p_max = self.settings['p_max']\n",
        "        w_cut_in = self.settings['w_cut_in']\n",
        "        w_rated = self.settings['w_rated']\n",
        "\n",
        "        if energy_target < 0 or energy_target > p_max:\n",
        "            return None\n",
        "\n",
        "        def find_wind_speed(w):\n",
        "            return self.energy(w, time_period_length) - energy_target\n",
        "\n",
        "        sol = root_scalar(find_wind_speed, bracket=[w_cut_in, w_rated], method='brentq')\n",
        "        return sol.root if sol.converged else None\n",
        "\n",
        "\n",
        "    def get_reward(self, s_p, s_c, a_b, a_w):\n",
        "        \"\"\"\n",
        "        Calculate the reward for given state-action values.\n",
        "        \"\"\"\n",
        "        c_plus = self.settings['c_plus']\n",
        "        c_minus = self.settings['c_minus']\n",
        "        e = a_b + a_w\n",
        "        if s_p >= 0:\n",
        "            return s_c * s_p - c_plus * abs(s_c - e)\n",
        "        if s_p < 0:\n",
        "            return s_c * s_p - c_minus * abs(s_c - e)\n",
        "        raise ValueError(\"Reward can not be calculated.\")\n",
        "\n",
        "    def generate_price_next_hour(self, price):\n",
        "        \"\"\"\n",
        "        Generates a random sample of the price for the next hour based on an AR(1) model.\n",
        "\n",
        "        Args:\n",
        "            price (float): The current price.\n",
        "\n",
        "        Returns:\n",
        "            float: A random sample of the next hour's price.\n",
        "        \"\"\"\n",
        "        phi = self.model_price.params.iloc[1]\n",
        "        intercept = self.model_price.params.iloc[0]\n",
        "        sigma = self.model_price.resid.std()\n",
        "        # The mean of the next price's distribution is the predicted value\n",
        "        mean_next_price = intercept + phi * price\n",
        "        # The standard deviation of the next price's distribution is the residual standard deviation\n",
        "        std_dev_next_price = sigma\n",
        "        # Generate a random sample from a normal distribution\n",
        "        price_next_sample = self.rng.normal(loc=mean_next_price, scale=std_dev_next_price)\n",
        "        return price_next_sample\n",
        "\n",
        "    def generate_wind_next_hour(self, wind):\n",
        "        \"\"\"\n",
        "        Generates a random sample of the wind speed for the next hour based on an AR(1) model.\n",
        "\n",
        "        Args:\n",
        "            wind (float): The current wind speed.\n",
        "\n",
        "        Returns:\n",
        "            float: A random sample of the next hour's wind speed.\n",
        "        \"\"\"\n",
        "        # Add a small epsilon to prevent log(0) if wind is zero.\n",
        "        # This ensures numerical stability for the log-transformed AR(1) model.\n",
        "        min_positive_wind = 1e-6 # A very small positive number to avoid log(0)\n",
        "        log_wind = np.log(max(wind, min_positive_wind))\n",
        "        phi = self.model_wind.params.iloc[1]\n",
        "        intercept = self.model_wind.params.iloc[0]\n",
        "        sigma = self.model_wind.resid.std()\n",
        "        # The mean of the next wind speed's distribution is the predicted value\n",
        "        log_mean_next_wind = intercept + phi * log_wind\n",
        "        # The standard deviation of the next wind speed's distribution is the residual standard deviation\n",
        "        std_dev_next_wind = sigma\n",
        "        # Generate a random sample from a normal distribution\n",
        "        log_wind_next_sample = self.rng.normal(loc=log_mean_next_wind, scale=std_dev_next_wind)\n",
        "        return np.exp(log_wind_next_sample)\n",
        "\n",
        "    def get_step(self, s: tuple, a: tuple):\n",
        "        \"\"\"\n",
        "        Compute one-step transition and reward from (s, a).\n",
        "\n",
        "        Args:\n",
        "            s: State label\n",
        "            a: Action label\n",
        "\n",
        "        Returns:\n",
        "            tuple: (sN, r)\n",
        "        \"\"\"\n",
        "        s_l, s_c, s_w, s_p = s\n",
        "        a_b, a_w, a_c = a\n",
        "        s_l_next = float(s_l - a_b)\n",
        "        s_c_next = float(a_c)\n",
        "        s_w_next = float(self.generate_wind_next_hour(s_w)) # update current values\n",
        "        s_p_next = float(self.generate_price_next_hour(s_p))\n",
        "        s_next = [s_l_next, s_c_next, s_w_next, s_p_next]\n",
        "        reward = float(self.get_reward(s_p, s_c, a_b, a_w))\n",
        "        return s_next, reward\n"
      ],
      "metadata": {
        "id": "zEeuLBrEeApv",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us create an instance of the class:"
      ],
      "metadata": {
        "id": "yXDCWizeML_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_discrete_set(min_val, max_val, step):\n",
        "    \"\"\"\n",
        "    Generates a set of discrete values from min_val to max_val with step size.\n",
        "\n",
        "    Args:\n",
        "        min_val (float): The minimum value in the set.\n",
        "        max_val (float): The maximum value in the set.\n",
        "        step (float): The step size between consecutive values.\n",
        "\n",
        "    Returns:\n",
        "        set: A set of discrete values {min_val,â€¦,âˆ’ð›¿,0,ð›¿,...,max_val}.\n",
        "    \"\"\"\n",
        "    # Ensure step is a divisor of min_val\n",
        "    if not np.isclose(min_val % step, 0):\n",
        "        raise ValueError(\"min_val argument must be divisible by step.\")\n",
        "    if not np.isclose(max_val % step, 0):\n",
        "        raise ValueError(\"max_val argument must be divisible by step.\")\n",
        "    return np.arange(min_val, max_val + step, step)\n",
        "\n",
        "step_mwh = 2\n",
        "b_max = settings['b_max']\n",
        "p_max = settings['p_max']\n",
        "c_max = settings['c_max']\n",
        "a_w_tuple = generate_discrete_set(0, p_max, step_mwh)\n",
        "a_c_tuple = generate_discrete_set(-b_max, c_max, step_mwh)\n",
        "\n",
        "envD = EnvEnergyDiscrete(settings, a_w_tuple = a_w_tuple, a_c_tuple = a_c_tuple)"
      ],
      "metadata": {
        "id": "GTxNoH6Btdr9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb55bc7b-4798-40c8-eead-d229187917b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Price model loaded successfully from prices_ar1.pkl.gz\n",
            "Wind model loaded successfully from wind_log_ar1.pkl.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1\n",
        "\n",
        "Consider the code below and explain the output."
      ],
      "metadata": {
        "id": "p2z3BDinOuy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "import random\n",
        "\n",
        "s = [4.0, 6.0, 12.4, 34.7] #  s = (s_l, s_c, s_w, s_p)\n",
        "for _ in range(5):\n",
        "    actions = envD.get_actions(s)  # a = (a_b, a_w, a_c)\n",
        "    a = random.choice(actions)\n",
        "    print(f\"(s_l, s_c, s_w, s_p): {s}, (a_b, a_w, a_c): {a} ->\", end = \" \")\n",
        "    s, r = envD.get_step(s, a)\n",
        "    s = [round(v,1) for v in s]\n",
        "    a = [round(v,0) for v in a]\n",
        "    print(f\"{s}, Rew: {r:1.1f}\")\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "Your comments\n",
        "\"\"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "RxOweN7Ysx9t",
        "outputId": "41b17468-e189-48d0-a0dd-839b70e925f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(s_l, s_c, s_w, s_p): [4.0, 6.0, 12.4, 34.7], (a_b, a_w, a_c): (-4.0, 10.0, 10.0) -> [8.0, 10.0, 28.0, 41.7], Rew: 208.2\n",
            "(s_l, s_c, s_w, s_p): [8.0, 10.0, 28.0, 41.7], (a_b, a_w, a_c): (8.0, 0.0, 12.0) -> [0.0, 12.0, 32.4, 44.8], Rew: 317.0\n",
            "(s_l, s_c, s_w, s_p): [0.0, 12.0, 32.4, 44.8], (a_b, a_w, a_c): (0.0, 0.0, 18.0) -> [0.0, 18.0, 52.6, 47.9], Rew: -62.4\n",
            "(s_l, s_c, s_w, s_p): [0.0, 18.0, 52.6, 47.9], (a_b, a_w, a_c): (0.0, 0.0, -2.0) -> [0.0, -2.0, 116.1, 52.0], Rew: -37.8\n",
            "(s_l, s_c, s_w, s_p): [0.0, -2.0, 116.1, 52.0], (a_b, a_w, a_c): (-2.0, 0.0, 2.0) -> [2.0, 2.0, 11.8, 60.6], Rew: -104.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nYour comments\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment with continuous actions\n",
        "\n",
        "We here use an environment where both actions and states are continuous.\n",
        "\n",
        "The environment is implemented below.\n"
      ],
      "metadata": {
        "id": "Fiop29XKNbgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnvEnergyCont:\n",
        "    \"\"\"\n",
        "    Environment for the hour-to-hour electricity market problem with continuous actions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, settings, seed = 25328):\n",
        "        \"\"\"\n",
        "        Initialize the environment.\n",
        "\n",
        "        Args:\n",
        "            settings (dict): A dictionary containing problem settings\n",
        "            seed (int): A seed for the random number generator\n",
        "        \"\"\"\n",
        "        self.settings = settings\n",
        "\n",
        "        ## price model\n",
        "        # Google Drive direct download URL (using the 'uc?id=' format)\n",
        "        url = 'https://drive.google.com/uc?id=1cGhte06iiWZnaRLPyj5D8ZzWX7gqPsIR'\n",
        "        # Output filename for the downloaded file\n",
        "        output_filename = 'prices_ar1.pkl.gz'\n",
        "        # Download the file from Google Drive\n",
        "        gdown.download(url, output_filename, quiet=True)\n",
        "        # Load the model from the downloaded file\n",
        "        with gzip.open(output_filename, \"rb\") as f:\n",
        "            model_price = joblib.load(f)\n",
        "        print(f\"Price model loaded successfully from {output_filename}\")\n",
        "        self.model_price = model_price\n",
        "\n",
        "        ## wind model\n",
        "        # Google Drive direct download URL (using the 'uc?id=' format)\n",
        "        url = 'https://drive.google.com/uc?id=1TJ1ACzev40QbeUlXBbDicYU3kEyiH1nB'\n",
        "        # Output filename for the downloaded file\n",
        "        output_filename = 'wind_log_ar1.pkl.gz'\n",
        "        # Download the file from Google Drive\n",
        "        gdown.download(url, output_filename, quiet=True)\n",
        "        # Load the model from the downloaded file\n",
        "        with gzip.open(output_filename, \"rb\") as f:\n",
        "            model_wind = joblib.load(f)\n",
        "        print(f\"Wind model loaded successfully from {output_filename}\")\n",
        "        self.model_wind = model_wind\n",
        "\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "    def reset_rng(self, seed):\n",
        "        \"\"\"\n",
        "        Reset the random number generator.\n",
        "\n",
        "        Args:\n",
        "            seed (int): A seed for the random number generator\n",
        "        \"\"\"\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "    def check_s(self, s):\n",
        "        \"\"\"\n",
        "        Check if a state is valid.\n",
        "        \"\"\"\n",
        "        s_l, s_c, s_w, _ = s\n",
        "        assert s_l >= 0 and s_l <= self.settings['b_max']\n",
        "        assert s_c >= -self.settings['b_max'] and s_c <= self.settings['c_max']\n",
        "        assert s_w >= 0\n",
        "\n",
        "    def check_a(self, a):\n",
        "        \"\"\"\n",
        "        Check if an action is valid.\n",
        "        \"\"\"\n",
        "        a_b, a_w, a_c = a\n",
        "        assert a_b >= -self.settings['b_max'] and a_b <= self.settings['b_max']\n",
        "        assert a_w >= 0\n",
        "        assert a_c >= -self.settings['b_max'] and a_c <= self.settings['c_max']\n",
        "\n",
        "    def get_action_info(self, s: tuple) -> tuple:\n",
        "        \"\"\"\n",
        "        Get fixed actions for a given state and the minimum a_c value to choose.\n",
        "        \"\"\"\n",
        "        self.check_s(s)\n",
        "        s_l, s_c, s_w, _ = s\n",
        "        b_max = self.settings['b_max']\n",
        "        # Determine a_b and a_w\n",
        "        e_max = self.energy(s_w) # max wind energy we can sell/produce\n",
        "        if s_c >= 0:\n",
        "            if s_c >= e_max:\n",
        "                a_b = min(s_l, s_c - e_max)\n",
        "                a_w = e_max\n",
        "            else: # s_c < e_max:\n",
        "                a_b = -min(b_max - s_l, e_max - s_c)\n",
        "                a_w = s_c - a_b\n",
        "        else: # s_c < 0:\n",
        "            a_b = -min(b_max - s_l, - s_c + e_max)\n",
        "            a_w = max(0, - a_b + s_c)\n",
        "        # print(f\"s_c: {s_c},  s_w: {s_w}, e_max: {e_max}\")\n",
        "        # print(f\"s_l: {s_l}, a_b: {a_b}, a_w: {a_w}, min a_c: {-(b_max - (s_l - a_b))}\")\n",
        "        a_c_min = -(b_max - (s_l - a_b))  # no need to buy power that cannot be stored\n",
        "        return float(a_b), float(a_w), float(a_c_min)\n",
        "\n",
        "    def state_variables_to_str(self, s_l, s_c, s_w, s_p):\n",
        "        \"\"\"\n",
        "        String representaion of a state.\n",
        "        \"\"\"\n",
        "        return \"{'s_l': \" + str(s_l) + \", 's_c': \" + str(s_c) + \", 's_w': \" + str(s_w) + \", 's_p': \" + str(s_p) + \"}\"\n",
        "\n",
        "    def action_variables_to_str(self, a_b, a_w, a_c):\n",
        "        \"\"\"\n",
        "        String representaion of an action.\n",
        "        \"\"\"\n",
        "        return \"{'a_b': \" + str(a_b) + \", 'a_w': \" + str(a_w) + \", 'a_c': \" + str(a_c) + \"}\"\n",
        "\n",
        "    def power(self, wind_speed):\n",
        "        \"\"\"\n",
        "        The power output of a wind turbine given a wind speed.\n",
        "        \"\"\"\n",
        "        p_max = self.settings['p_max']\n",
        "        w_cut_in = self.settings['w_cut_in']\n",
        "        w_rated = self.settings['w_rated']\n",
        "        w_cut_out = self.settings['w_cut_out']\n",
        "\n",
        "        if wind_speed < w_cut_in:\n",
        "            return 0\n",
        "        elif w_cut_in <= wind_speed <= w_rated:\n",
        "            return p_max * ((wind_speed - w_cut_in) / (w_rated - w_cut_in)) ** 3\n",
        "        elif w_rated < wind_speed <= w_cut_out:\n",
        "            return p_max\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def energy(self, w, time_period_length = 1):\n",
        "        \"\"\"\n",
        "        The energy output of a wind turbine over one hour given a wind speed (MWh).\n",
        "        \"\"\"\n",
        "        return self.power(w) * time_period_length\n",
        "\n",
        "    def get_reward(self, s_p, s_c, a_b, a_w):\n",
        "        \"\"\"\n",
        "        Calculate the reward for given state-action values.\n",
        "        \"\"\"\n",
        "        c_plus = self.settings['c_plus']\n",
        "        c_minus = self.settings['c_minus']\n",
        "        e = a_b + a_w\n",
        "        if s_p >= 0:\n",
        "            return s_c * s_p - c_plus * abs(s_c - e)\n",
        "        if s_p < 0:\n",
        "            return s_c * s_p - c_minus * abs(s_c - e)\n",
        "        raise ValueError(\"Reward can not be calculated.\")\n",
        "\n",
        "    def generate_price_next_hour(self, price):\n",
        "        \"\"\"\n",
        "        Generates a random sample of the price for the next hour based on an AR(1) model.\n",
        "\n",
        "        Args:\n",
        "            price (float): The current price.\n",
        "\n",
        "        Returns:\n",
        "            float: A random sample of the next hour's price.\n",
        "        \"\"\"\n",
        "        phi = self.model_price.params.iloc[1]\n",
        "        intercept = self.model_price.params.iloc[0]\n",
        "        sigma = self.model_price.resid.std()\n",
        "        # The mean of the next price's distribution is the predicted value\n",
        "        mean_next_price = intercept + phi * price\n",
        "        # The standard deviation of the next price's distribution is the residual standard deviation\n",
        "        std_dev_next_price = sigma\n",
        "        # Generate a random sample from a normal distribution\n",
        "        price_next_sample = self.rng.normal(loc=mean_next_price, scale=std_dev_next_price)\n",
        "        return price_next_sample\n",
        "\n",
        "    def generate_wind_next_hour(self, wind):\n",
        "        \"\"\"\n",
        "        Generates a random sample of the wind speed for the next hour based on an AR(1) model.\n",
        "\n",
        "        Args:\n",
        "            wind (float): The current wind speed.\n",
        "\n",
        "        Returns:\n",
        "            float: A random sample of the next hour's wind speed.\n",
        "        \"\"\"\n",
        "        # Add a small epsilon to prevent log(0) if wind is zero.\n",
        "        # This ensures numerical stability for the log-transformed AR(1) model.\n",
        "        min_positive_wind = 1e-6 # A very small positive number to avoid log(0)\n",
        "        log_wind = np.log(max(wind, min_positive_wind))\n",
        "        phi = self.model_wind.params.iloc[1]\n",
        "        intercept = self.model_wind.params.iloc[0]\n",
        "        sigma = self.model_wind.resid.std()\n",
        "        # The mean of the next wind speed's distribution is the predicted value\n",
        "        log_mean_next_wind = intercept + phi * log_wind\n",
        "        # The standard deviation of the next wind speed's distribution is the residual standard deviation\n",
        "        std_dev_next_wind = sigma\n",
        "        # Generate a random sample from a normal distribution\n",
        "        log_wind_next_sample = self.rng.normal(loc=log_mean_next_wind, scale=std_dev_next_wind)\n",
        "        return np.exp(log_wind_next_sample)\n",
        "\n",
        "    def get_step(self, s: tuple, a: tuple):\n",
        "        \"\"\"\n",
        "        Compute one-step transition and reward from (s, a).\n",
        "\n",
        "        Args:\n",
        "            s: State label\n",
        "            a: Action label\n",
        "\n",
        "        Returns:\n",
        "            tuple: (sN, r)\n",
        "        \"\"\"\n",
        "        self.check_s(s)\n",
        "        self.check_a(a)\n",
        "        s_l, s_c, s_w, s_p = s\n",
        "        a_b, a_w, a_c = a\n",
        "        s_l_next = float(s_l - a_b)\n",
        "        s_c_next = float(a_c)\n",
        "        s_w_next = float(self.generate_wind_next_hour(s_w)) # update current values\n",
        "        s_p_next = float(self.generate_price_next_hour(s_p))\n",
        "        s_next = [s_l_next, s_c_next, s_w_next, s_p_next]\n",
        "        self.check_s(s_next)\n",
        "        reward = float(self.get_reward(s_p, s_c, a_b, a_w))\n",
        "        return s_next, reward\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oAzjxQbBNbgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us create an instance of the class:"
      ],
      "metadata": {
        "id": "0VAZpd2ENbgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envC = EnvEnergyCont(settings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a78e6dd9-8acc-40da-8102-01c7db44fdbf",
        "id": "c-BJ8zw5Nbgk"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Price model loaded successfully from prices_ar1.pkl.gz\n",
            "Wind model loaded successfully from wind_log_ar1.pkl.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2\n",
        "\n",
        "Consider the code below and explain the output."
      ],
      "metadata": {
        "id": "5dnDYGzANbgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "import random\n",
        "\n",
        "s = [4.6, 12.7, 12.4, 34.7] #  s = (s_l, s_c, s_w, s_p)\n",
        "for _ in range(5):\n",
        "    a_b, a_w, a_c_min = envC.get_action_info(s)  # a = (a_b, a_w, a_c)\n",
        "    a_c = random.uniform(a_c_min, c_max)\n",
        "    a = (a_b, a_w, a_c)\n",
        "    a = [round(v,1) for v in a]\n",
        "    print(f\"(s_l, s_c, s_w, s_p): {s}, (a_b, a_w, a_c): {a} ->\", end = \" \")\n",
        "    s, r = envC.get_step(s, a)\n",
        "    s = [round(v,1) for v in s]\n",
        "    print(f\"{s}, Rew: {r:1.1f}\")\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "Your comments\n",
        "\"\"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "10099acd-c7c7-45ff-afa6-cbb5f53f849a",
        "id": "y04v9mRRNbgk"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(s_l, s_c, s_w, s_p): [4.6, 12.7, 12.4, 34.7], (a_b, a_w, a_c): [2.7, 10.0, -7.5] -> [1.9, -7.5, 28.0, 41.7], Rew: 440.7\n",
            "(s_l, s_c, s_w, s_p): [1.9, -7.5, 28.0, 41.7], (a_b, a_w, a_c): [-7.5, 0.0, 12.8] -> [9.4, 12.8, 32.4, 44.8], Rew: -312.8\n",
            "(s_l, s_c, s_w, s_p): [9.4, 12.8, 32.4, 44.8], (a_b, a_w, a_c): [9.4, 0.0, 9.2] -> [0.0, 9.2, 52.6, 47.9], Rew: 403.4\n",
            "(s_l, s_c, s_w, s_p): [0.0, 9.2, 52.6, 47.9], (a_b, a_w, a_c): [0.0, 0.0, 16.7] -> [0.0, 16.7, 116.1, 52.0], Rew: -19.3\n",
            "(s_l, s_c, s_w, s_p): [0.0, 16.7, 116.1, 52.0], (a_b, a_w, a_c): [0.0, 0.0, -2.4] -> [0.0, -2.4, 11.8, 60.6], Rew: 33.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nYour comments\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3 - Solving the problem\n",
        "\n",
        "Our objective is to maximise the average reward. Pick one of the environments above and solve the problem. Note in theory by having continuous actions, you don't limit you action space and you should be able to get a better objective value.\n",
        "\n",
        "For solving the problem, you have different options:\n",
        "\n",
        "1. Implement classes for state-value and policy approximation and apply an actor-critic algorithm, similar to what we have done in the course.\n",
        "2. Convert the environment into a Gym-style RL environment so it can be used with an RL library. Then, utilise the RL library to solve the problem. Note that in most RL libraries, the objective is discounted reward, which we have argued is not the most suitable choice for continuing problems. However, if you use a discount rate close to 1 (such as 0.9999), you can mimic the average reward objective.\n",
        "\n",
        "State and clearly connect your choices to (also holds if you use Deep RL):\n",
        "\n",
        "* The average reward objective.\n",
        "* Theory regarding state-value approximations.\n",
        "* Theory about policy approximations.\n",
        "* Theory used which has not been presented in the course.\n",
        "\n",
        "ChatGPT (or similar) can be useful for initial guidance, but remember to critically evaluate and understand the reasoning behind your choices. If you require GPU acceleration in Colab, then change the runtime type to GPU-T4 (the dropdown beside RAM in the upper right corner).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fVprzFs-f1ES"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}