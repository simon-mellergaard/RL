{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simon-mellergaard/RL/blob/main/Project/Project02_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions\n",
        "\n",
        "This notebook contains the first project assignment. You work on the project in groups. You should already have formed a group. If not, do it on Brightspace under *My Courses > Groups*. To get started, do the following steps:\n",
        "\n",
        "1) One student in the group makes a copy of this notebook.\n",
        "2) Share it with your group using *Share* top-right. Here, add group members under people with access.\n",
        "3) Moreover, under *Share > General access*, choose *Anyone with the link > Commenter*. Copy this link and paste it below.\n",
        "4) Work with the notebook and solve the tasks.\n",
        "5) Hand in the notebook by downloading it: File > Download > Download .ipynb. Next, on BS under *Project*, upload the file (I need that for the exam). Moreover, add the shared link of your Colab notebook as a link when you handin too.\n",
        "6) After hand-in do peer grading (see BS under *Project*)\n",
        "\n",
        "Sharing link: [add your link]\n",
        "\n",
        "**Deadlines**\n",
        "\n",
        "* Hand-in solution 9/11/25\n",
        "* Peer grading 15/11/25\n"
      ],
      "metadata": {
        "id": "tHx3pQBrT8ob"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlrET7d3WOQ2"
      },
      "source": [
        "# Notebook pre steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0vEJAmyWKPb"
      },
      "outputs": [],
      "source": [
        "#@title Installations\n",
        "\n",
        "# ALWAYS SAVE YOUR OWN COPY OF THIS NOTEBOOK: File > Save a copy in Drive\n",
        "# IF DANISH MENU DO: Hj√¶lp > Engelsk version\n",
        "\n",
        "# To clear output do: Edit > Clear all outputs\n",
        "\n",
        "## Useful shortscuts\n",
        "# Run current cell: Cmd+Enter\n",
        "# Run current cell and goto next: Shift+Enter\n",
        "# Run selection: Cmd+Shift+Enter\n",
        "\n",
        "# install missing packages\n",
        "# !pip install pymdptoolbox\n",
        "!pip install dfply\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "from plotnine import *\n",
        "import joblib\n",
        "import gzip\n",
        "import gdown\n",
        "import warnings\n",
        "from scipy.stats import norm\n",
        "from dfply import *\n",
        "import itertools\n",
        "from IPython.display import Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem description\n",
        "\n",
        "Consider the [Hour-to-hour electricity market problem](https://colab.research.google.com/drive/1BFI6iRLTnrHgxgFayC89SKhuM3XcIp7s?usp=sharing) that we solved using a Markov decision process by discretising the state and action space.\n",
        "\n",
        "Here, we are going to solve the problem using model-free RL."
      ],
      "metadata": {
        "id": "2ygAx1kWRwyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discretisation class\n",
        "\n",
        "All methods for discretisation have been added to a class, that can be used to discretise the state and action space.\n"
      ],
      "metadata": {
        "id": "BAhwuZfxYSV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Discretisation class\n",
        "\n",
        "class Discretise:\n",
        "    \"\"\"\n",
        "    Methods for discretising continuous variables.\n",
        "\n",
        "    Intervals are assumed to be of type [a,b[\n",
        "    \"\"\"\n",
        "\n",
        "    def generate_discrete_set(self, min_val, max_val, step):\n",
        "        \"\"\"\n",
        "        Generates a set of discrete values from min_val to max_val with step size.\n",
        "\n",
        "        Args:\n",
        "            min_val (float): The minimum value in the set.\n",
        "            max_val (float): The maximum value in the set.\n",
        "            step (float): The step size between consecutive values.\n",
        "\n",
        "        Returns:\n",
        "            set: A set of discrete values {min_val,‚Ä¶,‚àíùõø,0,ùõø,...,max_val}.\n",
        "        \"\"\"\n",
        "        # Ensure step is a divisor of min_val\n",
        "        if not np.isclose(min_val % step, 0):\n",
        "            raise ValueError(\"min_val argument must be divisible by step.\")\n",
        "        if not np.isclose(max_val % step, 0):\n",
        "            raise ValueError(\"max_val argument must be divisible by step.\")\n",
        "        return np.arange(min_val, max_val + step, step)\n",
        "\n",
        "\n",
        "    def generate_intervals(self, min_val, max_val, length, add_inf = False, add_neg_inf = False):\n",
        "        \"\"\"\n",
        "        Splits a given interval into n evenly spaced subintervals of type ]a,b] and returns midpoints and their corresponding bounds.\n",
        "\n",
        "        Args:\n",
        "            min_val (float): The minimum bound value of the first interval.\n",
        "            max_val (float): The maximum bound value of the last interval.\n",
        "            length (float): The interval length.\n",
        "            add_inf (bool): Add infinity interval too (midpoint is calculated based on the last interval).\n",
        "            add_neg_inf (bool): Add negative infinity interval too (midpoint is calculated based on the first interval).\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing:\n",
        "                - midpoints (list): Sorted list of midpoints in the intervals.\n",
        "                - intervals_dict (dict): Mapping of midpoints to a dictionary containing 'ab' (bounds) and 'length'.\n",
        "                - lower_inf (bool): Indicates if a negative infinity interval was added.\n",
        "                - upper_inf (bool): Indicates if an infinity interval was added.\n",
        "        \"\"\"\n",
        "        if not np.isclose((max_val - min_val) % length, 0):\n",
        "            raise ValueError(\"(max_val - min_val) must be divisible by length.\")\n",
        "\n",
        "        bounds = np.arange(min_val, max_val + length, length)\n",
        "        midpoints = np.arange(min_val + length/2, max_val + length/2, length)\n",
        "        midpoints = midpoints.tolist()\n",
        "        bounds = bounds.tolist()\n",
        "\n",
        "        intervals_dict = {}\n",
        "        for i in range(len(midpoints)):\n",
        "            intervals_dict[midpoints[i]] = {'ab': [bounds[i], bounds[i+1]], 'length': length}\n",
        "\n",
        "        if add_neg_inf:\n",
        "            neg_inf_midpoint = bounds[0] - 1  # Calculate midpoint for the negative infinity interval\n",
        "            intervals_dict[neg_inf_midpoint] = {'ab': [-np.inf, bounds[0]], 'length': np.inf}\n",
        "            midpoints.insert(0, neg_inf_midpoint)\n",
        "\n",
        "        if add_inf:\n",
        "            inf_midpoint = bounds[-1] + 1  # Calculate midpoint for the infinity interval\n",
        "            intervals_dict[inf_midpoint] = {'ab': [bounds[-1], np.inf], 'length': np.inf}\n",
        "            midpoints.append(inf_midpoint)\n",
        "\n",
        "        return {\"midpoints\": midpoints, 'intervals_dict': intervals_dict, \"lower_inf\": add_neg_inf, \"upper_inf\": add_inf}\n",
        "\n",
        "\n",
        "    def get_interval(self, point, intervals, as_string = False):\n",
        "        \"\"\"\n",
        "        Return interval bounds given a point and interval.\n",
        "\n",
        "        Args:\n",
        "            point (float): A point.\n",
        "            intervals (dict): A dictionary\n",
        "            as_string (bool): If true return the interval as a string.\n",
        "        \"\"\"\n",
        "        midpoints = intervals['midpoints']\n",
        "        intervals_dict = intervals['intervals_dict']\n",
        "        lower_inf = intervals['lower_inf']\n",
        "        upper_inf = intervals['upper_inf']\n",
        "\n",
        "        # Check negative infinity interval\n",
        "        if lower_inf and point < intervals_dict[midpoints[0]]['ab'][1]:\n",
        "            if as_string:\n",
        "                return str(intervals_dict[midpoints[0]]['ab'])\n",
        "            else:\n",
        "                return intervals_dict[midpoints[0]]['ab']\n",
        "\n",
        "        # Check positive infinity interval\n",
        "        if upper_inf and point >= intervals_dict[midpoints[-1]]['ab'][0]:\n",
        "            if as_string:\n",
        "                return str(intervals_dict[midpoints[-1]]['ab'])\n",
        "            else:\n",
        "                return intervals_dict[midpoints[-1]]['ab']\n",
        "\n",
        "        # Check regular intervals\n",
        "        for midpoint in midpoints:\n",
        "            lower_bound, upper_bound = intervals_dict[midpoint]['ab']\n",
        "            if lower_bound <= point < upper_bound:\n",
        "                if as_string:\n",
        "                    return str(intervals_dict[midpoint]['ab'])\n",
        "                else:\n",
        "                    return intervals_dict[midpoint]['ab']\n",
        "\n",
        "        return None # Point is not in any defined interval\n",
        "\n",
        "\n",
        "    def get_midpoint(self, point, intervals, interior = False):\n",
        "        \"\"\"\n",
        "        Return interval midpoint given a point and intervals.\n",
        "\n",
        "        Args:\n",
        "            point (float): A point.\n",
        "            intervals (dict): A dictionary\n",
        "            interior (bool): If true only return the midpoint if the point is in the interior of an interval.\n",
        "        \"\"\"\n",
        "        midpoints = intervals['midpoints']\n",
        "        intervals_dict = intervals['intervals_dict']\n",
        "        lower_inf = intervals['lower_inf']\n",
        "        upper_inf = intervals['upper_inf']\n",
        "\n",
        "        # Check negative infinity interval\n",
        "        if lower_inf and point < intervals_dict[midpoints[0]]['ab'][1]:\n",
        "            return midpoints[0]\n",
        "\n",
        "        # Check positive infinity interval\n",
        "        if not interior:\n",
        "            if upper_inf and point >= intervals_dict[midpoints[-1]]['ab'][0]:\n",
        "                return midpoints[-1]\n",
        "        else:\n",
        "            if upper_inf and point > intervals_dict[midpoints[-1]]['ab'][0]:\n",
        "                return midpoints[-1]\n",
        "\n",
        "        # Check regular intervals\n",
        "        if not interior:\n",
        "            for midpoint in midpoints:\n",
        "                lower_bound, upper_bound = intervals_dict[midpoint]['ab']\n",
        "                if lower_bound <= point < upper_bound:\n",
        "                    return midpoint\n",
        "        else:\n",
        "            for midpoint in midpoints:\n",
        "                lower_bound, upper_bound = intervals_dict[midpoint]['ab']\n",
        "                if lower_bound < point < upper_bound:\n",
        "                    return midpoint\n",
        "\n",
        "        return None # Point is not in any defined interval\n",
        "\n",
        "\n",
        "    def add_interval(self, a, b, intervals):\n",
        "        \"\"\"\n",
        "        Add an interval to a list of intervals.\n",
        "        This function assumes intervals sorted by midpoint.\n",
        "\n",
        "        Args:\n",
        "            a (float): The lower bound of the interval to add.\n",
        "            b (float): The upper bound of the interval to add.\n",
        "            intervals (dict): The existing dictionary of intervals.\n",
        "        \"\"\"\n",
        "        if a >= b:\n",
        "            raise ValueError(\"Lower bound must be smaller than upper bound.\")\n",
        "        if self.get_midpoint(a, intervals, interior = True) is not None or self.get_midpoint(b, intervals, interior = True) is not None:\n",
        "            raise ValueError(\"Upper or lower bound is already in an interval.\")\n",
        "        for midpoint in intervals['midpoints']:\n",
        "            if a <= midpoint <= b:\n",
        "                raise ValueError(\"Upper or lower bound is already in an interval.\")\n",
        "        if a == np.inf:\n",
        "            raise ValueError(\"Lower bound cannot be infinity.\")\n",
        "        if b == -np.inf:\n",
        "            raise ValueError(\"Upper bound cannot be -infinity.\")\n",
        "\n",
        "        if a == -np.inf:\n",
        "            intervals['lower_inf'] = True\n",
        "            midpoint = b - 1\n",
        "            length = np.inf\n",
        "        if b == np.inf:\n",
        "            intervals['upper_inf'] = True\n",
        "            midpoint = a + 1\n",
        "            length = np.inf\n",
        "        if not a == -np.inf and not b == np.inf:\n",
        "            midpoint = a + (b - a) / 2\n",
        "            length = b - a\n",
        "        new_interval_info = {'ab': [a, b], 'length': length}\n",
        "\n",
        "        # Find the correct insertion point for the midpoint\n",
        "        insert_index = 0\n",
        "        for i, existing_midpoint in enumerate(intervals['midpoints']):\n",
        "            if midpoint > existing_midpoint:\n",
        "                insert_index = i + 1\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        intervals['midpoints'].insert(insert_index, midpoint)\n",
        "        intervals['intervals_dict'][midpoint] = new_interval_info\n",
        "\n",
        "\n",
        "    def remove_interval(self, point, intervals):\n",
        "        \"\"\"\n",
        "        Remove an interval from the dictionary based on a point.\n",
        "\n",
        "        Args:\n",
        "            point (float): A point of the interval to remove.\n",
        "            intervals (dict): The dictionary of intervals.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the interval was found and removed, False otherwise.\n",
        "        \"\"\"\n",
        "        midpoint = self.get_midpoint(point, intervals)\n",
        "        if not midpoint:\n",
        "            return False\n",
        "\n",
        "        if intervals['intervals_dict'][midpoint]['ab'][0] == -np.inf:\n",
        "            intervals['lower_inf'] = False\n",
        "        if intervals['intervals_dict'][midpoint]['ab'][1] == np.inf:\n",
        "            intervals['upper_inf'] = False\n",
        "\n",
        "        # Remove from midpoints list\n",
        "        intervals['midpoints'].remove(midpoint)\n",
        "\n",
        "        # Remove from intervals_dict\n",
        "        del intervals['intervals_dict'][midpoint]\n",
        "\n",
        "        return True\n",
        "\n",
        "\n",
        "    def join_intervals(self, a, b, intervals):\n",
        "        \"\"\"\n",
        "        Join two intervals if they are adjacent.\n",
        "\n",
        "        Args:\n",
        "            a (float): The lower bound of the first interval.\n",
        "            b (float): The upper bound of the second interval.\n",
        "            intervals (dict): The dictionary of intervals.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the intervals were joined, False otherwise.\n",
        "        \"\"\"\n",
        "        a = self.get_midpoint(a, intervals)\n",
        "        b = self.get_midpoint(b, intervals)\n",
        "        if not a or not b:\n",
        "            raise ValueError(\"At least one of the points is not in an interval.\")\n",
        "        if a == b:\n",
        "            raise ValueError(\"Points are in the same interval\")\n",
        "            return False\n",
        "        if (a > b):\n",
        "            a, b = b, a\n",
        "        a_idx = intervals['midpoints'].index(a)\n",
        "        b_idx = intervals['midpoints'].index(b)\n",
        "        if (a_idx != b_idx - 1):\n",
        "            return False\n",
        "        a_ab = intervals['intervals_dict'][a]['ab']\n",
        "        b_ab = intervals['intervals_dict'][b]['ab']\n",
        "        new_ab = [a_ab[0], b_ab[1]]\n",
        "        if a_ab[0] == -np.inf:\n",
        "            new_midpoint = b_ab[1] - 1\n",
        "            new_length = np.inf\n",
        "        if b_ab[1] == np.inf:\n",
        "            new_midpoint = a_ab[0] + 1\n",
        "            new_length = np.inf\n",
        "        if not a_ab[0] == -np.inf and not b_ab[1] == np.inf:\n",
        "            new_length = b_ab[1] - a_ab[0]\n",
        "            new_midpoint = a_ab[0] + new_length/2\n",
        "        new_interval = {'ab': new_ab, 'length': new_length}\n",
        "        intervals['intervals_dict'][new_midpoint] = new_interval\n",
        "        del intervals['intervals_dict'][a]\n",
        "        del intervals['intervals_dict'][b]\n",
        "        intervals['midpoints'][a_idx] = new_midpoint\n",
        "        intervals['midpoints'].remove(b)\n",
        "        return True\n",
        "\n",
        "\n",
        "    def get_closest(self, lst, a):\n",
        "        \"\"\"\n",
        "        Given a number finds the closest number in a list (if ties choose the smallest).\n",
        "\n",
        "        Args:\n",
        "            lst (list): The list of numbers to search within.\n",
        "            a (float or int): The target number to find the closest value to.\n",
        "\n",
        "        Returns:\n",
        "            float or int: The number from the list that is closest to a.\n",
        "        \"\"\"\n",
        "        idx = min(range(len(lst)), key=lambda i: abs(lst[i]-a))\n",
        "        return lst[idx]\n",
        "\n",
        "    def get_left(self, sorted_list, a):\n",
        "        \"\"\"\n",
        "        Given a sorted list and a number 'a', finds the largest number 'b' in\n",
        "        the list so b <= a\n",
        "\n",
        "        Args:\n",
        "            sorted_list (list): A list of numbers sorted in ascending order.\n",
        "            a (float or int): The number to find the bounds for.\n",
        "\n",
        "        Returns:\n",
        "            list: If 'a' is equal to a number in the list, returns a.\n",
        "                If 'a' is less than the smallest number, returns right.\n",
        "                If 'a' is greater than the largest number, returns left.\n",
        "                If 'a' is between two numbers in the list, returns left or right depending on `return_left`.\n",
        "        \"\"\"\n",
        "        # Iterate through the sorted list to find the bounds\n",
        "        for i in range(len(sorted_list)-1, -1, -1):\n",
        "            if sorted_list[i] <= a:\n",
        "                return sorted_list[i]\n",
        "        return None\n",
        "\n",
        "    def get_right(self, sorted_list, a):\n",
        "        \"\"\"\n",
        "        Given a sorted list and a number 'a', find the smallest number 'b' in\n",
        "        the list so a <= b\n",
        "\n",
        "        Args:\n",
        "            sorted_list (list): A list of numbers sorted in ascending order.\n",
        "            a (float or int): The number to find the bounds for.\n",
        "\n",
        "        Returns:\n",
        "            list: If 'a' is equal to a number in the list, returns a.\n",
        "                If 'a' is less than the smallest number, returns right.\n",
        "                If 'a' is greater than the largest number, returns left.\n",
        "                If 'a' is between two numbers in the list, returns left or right depending on `return_left`.\n",
        "        \"\"\"\n",
        "        # Iterate through the sorted list to find the bounds\n",
        "        for i in range(len(sorted_list)):\n",
        "            if sorted_list[i] >= a:\n",
        "                return sorted_list[i]\n",
        "        return None\n",
        "\n",
        "    def get_left_and_right(self, sorted_list, a):\n",
        "        \"\"\"\n",
        "        Given a sorted list and a number 'a', finds the left and right numbers\n",
        "        'b' and 'c' in the list such that b <= a <= c.\n",
        "\n",
        "        Args:\n",
        "            sorted_list (list): A list of numbers sorted in ascending order.\n",
        "            a (float or int): The number to find the bounds for.\n",
        "\n",
        "        Returns:\n",
        "            list: [b,c]\n",
        "        \"\"\"\n",
        "        # Handle edge cases: a is less than the smallest or greater than the largest\n",
        "        if a < sorted_list[0]:\n",
        "            return [None, sorted_list[0]]\n",
        "        if a > sorted_list[-1]:\n",
        "            return [sorted_list[-1], None]\n",
        "\n",
        "        # Iterate through the sorted list to find the bounds\n",
        "        for i in range(len(sorted_list) - 1):\n",
        "            left = sorted_list[i]\n",
        "            right = sorted_list[i+1]\n",
        "            if left <= a <= right:\n",
        "                return [left, right]\n",
        "\n",
        "        # This part should ideally not be reached if edge cases are handled and the list is sorted\n",
        "        return None\n",
        "\n",
        "    def get_interval_midpoint(self, a, intervals):\n",
        "        \"\"\"\n",
        "        Return interval midpoint given a point and interval.\n",
        "\n",
        "        Args:\n",
        "            midpoint (float): The midpoint of the interval.\n",
        "            intervals (dict): A dictionary\n",
        "        \"\"\"\n",
        "        midpoints = intervals['midpoints']\n",
        "        intervals_dict = intervals['intervals_dict']\n",
        "        for midpoint in midpoints:\n",
        "            lower_bound, upper_bound = intervals_dict[midpoint]['ab']\n",
        "            if lower_bound <= a < upper_bound:\n",
        "                return midpoint\n",
        "        return None\n",
        "\n"
      ],
      "metadata": {
        "id": "YlC07Ud_YRkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run code"
      ],
      "metadata": {
        "id": "QO0V62r_LvFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dis = Discretise()\n",
        "tmp = dis.generate_discrete_set(0, 10, 2)\n",
        "n = 5\n",
        "print(dis.get_left(tmp, n))\n",
        "print(dis.get_right(tmp, n))"
      ],
      "metadata": {
        "id": "yS6vf2zzL257"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1\n",
        "\n",
        "Explain functions `get_left` and `get_right`.\n",
        "\n",
        "- What is input?\n",
        "- What is returned?\n",
        "- What is the result if `n` is -1, 0 or 14?"
      ],
      "metadata": {
        "id": "qTmmouEYL_or"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "# Your code\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "Your comments\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "Q3GZLcj4MpAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem environment\n",
        "\n",
        "To model the problem, we need to define an environment class:"
      ],
      "metadata": {
        "id": "VdGxVRt5NO6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title EnvEnergy environment class\n",
        "\n",
        "from scipy import integrate\n",
        "import numpy as np\n",
        "from scipy.stats import norm, lognorm\n",
        "from scipy.optimize import root_scalar\n",
        "import ast\n",
        "import pandas as pd\n",
        "import itertools\n",
        "\n",
        "class EnvEnergy:\n",
        "    \"\"\"\n",
        "    Environment for the hour-to-hour electricity market problem.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, settings, seed = 25328):\n",
        "        \"\"\"\n",
        "        Initialize the environment.\n",
        "\n",
        "        Args:\n",
        "            settings (dict): A dictionary containing problem settings\n",
        "            seed (int): A seed for the random number generator\n",
        "        \"\"\"\n",
        "        self.settings = settings\n",
        "\n",
        "        ## price model\n",
        "        # Google Drive direct download URL (using the 'uc?id=' format)\n",
        "        url = 'https://drive.google.com/uc?id=1cGhte06iiWZnaRLPyj5D8ZzWX7gqPsIR'\n",
        "        # Output filename for the downloaded file\n",
        "        output_filename = 'prices_ar1.pkl.gz'\n",
        "        # Download the file from Google Drive\n",
        "        gdown.download(url, output_filename, quiet=True)\n",
        "        # Load the model from the downloaded file\n",
        "        with gzip.open(output_filename, \"rb\") as f:\n",
        "            model_price = joblib.load(f)\n",
        "        print(f\"Price model loaded successfully from {output_filename}\")\n",
        "        self.model_price = model_price\n",
        "\n",
        "        ## wind model\n",
        "        # Google Drive direct download URL (using the 'uc?id=' format)\n",
        "        url = 'https://drive.google.com/uc?id=1TJ1ACzev40QbeUlXBbDicYU3kEyiH1nB'\n",
        "        # Output filename for the downloaded file\n",
        "        output_filename = 'wind_log_ar1.pkl.gz'\n",
        "        # Download the file from Google Drive\n",
        "        gdown.download(url, output_filename, quiet=True)\n",
        "        # Load the model from the downloaded file\n",
        "        with gzip.open(output_filename, \"rb\") as f:\n",
        "            model_wind = joblib.load(f)\n",
        "        print(f\"Wind model loaded successfully from {output_filename}\")\n",
        "        self.model_wind = model_wind\n",
        "\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.dis = Discretise()  # methods for discretisation\n",
        "\n",
        "        # To store current state values for wind and price to use in get_time_step_data (since continuous and interval)\n",
        "        self.s_p = None\n",
        "        self.s_w = None\n",
        "\n",
        "    def reset_rng(self, seed):\n",
        "        \"\"\"\n",
        "        Reset the random number generator.\n",
        "\n",
        "        Args:\n",
        "            seed (int): A seed for the random number generator\n",
        "        \"\"\"\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "    def set_discretisation(self, s_l_set, s_c_set, s_w_inv, s_p_inv, a_b_set, a_w_set, a_c_set):\n",
        "        \"\"\"\n",
        "        Set the discretisation.\n",
        "\n",
        "        Args:\n",
        "            s_l_set (list): A list of possible battery levels\n",
        "            s_c_set (list): A list of possible commitment levels\n",
        "            s_w_inv (dict): A dictionary containing the intervals for wind\n",
        "            s_p_inv (dict): A dictionary containing the intervals for price\n",
        "            a_b_set (list): A list of possible battery usage\n",
        "            a_w_set (list): A list of possible wind usage\n",
        "            a_c_set (list): A list of possible commitment levels\n",
        "        \"\"\"\n",
        "        self.s_l_set = s_l_set\n",
        "        self.s_c_set = s_c_set\n",
        "        self.s_w_inv = s_w_inv\n",
        "        self.s_p_inv = s_p_inv\n",
        "        self.a_b_set = a_b_set\n",
        "        self.a_w_set = a_w_set\n",
        "        self.a_c_set = a_c_set\n",
        "\n",
        "    def get_states(self):\n",
        "        \"\"\"\n",
        "        Get all possible states.\n",
        "        \"\"\"\n",
        "        return [self.state_variables_to_str(s_l, s_c, s_w, s_p) for s_l in self.s_l_set for s_c in self.s_c_set for s_w in self.s_w_inv['midpoints'] for s_p in self.s_p_inv['midpoints']]\n",
        "\n",
        "    def get_actions(self, s: dict) -> list:\n",
        "        \"\"\"\n",
        "        Get all possible actions for a given state.\n",
        "        \"\"\"\n",
        "        b_max = self.settings['b_max']\n",
        "        # Determine a_b and a_w\n",
        "        e_max = self.dis.get_left(self.a_w_set, self.energy(s['s_w'])) # max energy we can sell/produce (e.g. if energy(s['s_w']) = 1.25 but a_w_set is [0,1,2,...] then can only sell 1)\n",
        "        if s['s_c'] >= 0:\n",
        "            if s['s_c'] >= e_max:\n",
        "                a_b = min(s['s_l'], s['s_c'] - e_max)\n",
        "                a_w = e_max\n",
        "            else: # s['s_c'] < e_max:\n",
        "                a_b = -min(b_max - s['s_l'], e_max - s['s_c'])\n",
        "                a_w = s['s_c'] - a_b\n",
        "        else: # s['s_c'] < 0:\n",
        "            a_b = -min(b_max - s['s_l'], - s['s_c'] + e_max)\n",
        "            a_w = max(0, - a_b + s['s_c'])\n",
        "\n",
        "        actions = [self.action_variables_to_str(a_b, a_w, a_c) for a_c in self.a_c_set]\n",
        "        # actions = [str_to_dict(a) for a in actions]\n",
        "        return actions\n",
        "\n",
        "    def reset_current_state(self):\n",
        "        \"\"\"\n",
        "        Reset the current price and wind speed to None. Hence, assume no previous\n",
        "        state (we start our sample run).\n",
        "        \"\"\"\n",
        "        self.s_p = None\n",
        "        self.s_w = None\n",
        "\n",
        "    def set_current_state(self, s):\n",
        "        \"\"\"\n",
        "        Set the current price and wind speed given a state (dict)\n",
        "        \"\"\"\n",
        "        self.s_p = s['s_p']\n",
        "        self.s_w = s['s_w']\n",
        "\n",
        "    def state_variables_to_str(self, s_l, s_c, s_w, s_p):\n",
        "        \"\"\"\n",
        "        String representaion of a state.\n",
        "        \"\"\"\n",
        "        return \"{'s_l': \" + str(s_l) + \", 's_c': \" + str(s_c) + \", 's_w': \" + str(s_w) + \", 's_p': \" + str(s_p) + \"}\"\n",
        "\n",
        "    def action_variables_to_str(self, a_b, a_w, a_c):\n",
        "        \"\"\"\n",
        "        String representaion of an action.\n",
        "        \"\"\"\n",
        "        return \"{'a_b': \" + str(a_b) + \", 'a_w': \" + str(a_w) + \", 'a_c': \" + str(a_c) + \"}\"\n",
        "\n",
        "    def str_to_dict(self, str):\n",
        "        \"\"\"\n",
        "        Convert a string representation of a dictionary to a dictionary.\n",
        "\n",
        "        Args:\n",
        "            str (str): String representation of a dictionary.\n",
        "\n",
        "        Returns:\n",
        "            dict: Converted dictionary.\n",
        "        \"\"\"\n",
        "        return ast.literal_eval(str)\n",
        "\n",
        "    def dict_to_str(self, d):\n",
        "        \"\"\"\n",
        "        Convert a dictionary to a string representation of a dictionary.\n",
        "\n",
        "        Args:\n",
        "            d (dict): Dictionary to be converted.\n",
        "\n",
        "        Returns:\n",
        "            str: String representation of the dictionary.\n",
        "        \"\"\"\n",
        "        return str(d)\n",
        "\n",
        "    def power(self, wind_speed):\n",
        "        \"\"\"\n",
        "        The power output of a wind turbine given a wind speed.\n",
        "        \"\"\"\n",
        "        p_max = self.settings['p_max']\n",
        "        w_cut_in = self.settings['w_cut_in']\n",
        "        w_rated = self.settings['w_rated']\n",
        "        w_cut_out = self.settings['w_cut_out']\n",
        "\n",
        "        if wind_speed < w_cut_in:\n",
        "            return 0\n",
        "        elif w_cut_in <= wind_speed <= w_rated:\n",
        "            return p_max * ((wind_speed - w_cut_in) / (w_rated - w_cut_in)) ** 3\n",
        "        elif w_rated < wind_speed <= w_cut_out:\n",
        "            return p_max\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def energy(self, w, time_period_length = 1):\n",
        "        \"\"\"\n",
        "        The energy output of a wind turbine over one hour given a wind speed (MWh).\n",
        "        \"\"\"\n",
        "        return self.power(w) * time_period_length\n",
        "\n",
        "    def energy_inverse(self, energy_target, time_period_length = 1):\n",
        "        \"\"\"\n",
        "        Find the wind speed that produces a given amount of energy, assuming wind is between w_cut_in and w_rated.\n",
        "\n",
        "        Args:\n",
        "            energy_target (float): The target energy output in MWh.\n",
        "            time_period_length (float, optional): The time period length in hours. Defaults to 1.\n",
        "\n",
        "        Returns:\n",
        "            float: The wind speed that produces the given energy.\n",
        "        \"\"\"\n",
        "        p_max = self.settings['p_max']\n",
        "        w_cut_in = self.settings['w_cut_in']\n",
        "        w_rated = self.settings['w_rated']\n",
        "\n",
        "        if energy_target < 0 or energy_target > p_max:\n",
        "            return None\n",
        "\n",
        "        def find_wind_speed(w):\n",
        "            return self.energy(w, time_period_length) - energy_target\n",
        "\n",
        "        sol = root_scalar(find_wind_speed, bracket=[w_cut_in, w_rated], method='brentq')\n",
        "        return sol.root if sol.converged else None\n",
        "\n",
        "\n",
        "    # def get_left_or_right(self, sorted_list, a, return_left = True):\n",
        "    #     \"\"\"\n",
        "    #     Given a sorted list and a number 'a', finds the left and right numbers\n",
        "    #     in the list that 'a' is between.\n",
        "    #     \"\"\"\n",
        "    #     # Handle edge cases: a is less than the smallest or greater than the largest\n",
        "    #     if a < sorted_list[0]:\n",
        "    #         return sorted_list[0]\n",
        "    #     if a > sorted_list[-1]:\n",
        "    #         return sorted_list[-1]\n",
        "\n",
        "    #     # Iterate through the sorted list to find the bounds\n",
        "    #     for i in range(len(sorted_list) - 1):\n",
        "    #         left = sorted_list[i]\n",
        "    #         right = sorted_list[i+1]\n",
        "\n",
        "    #         if left <= a <= right:\n",
        "    #             # If a is exactly in the list or between two numbers\n",
        "    #             if a == left or a == right:\n",
        "    #                 return a\n",
        "    #             elif return_left:\n",
        "    #                 return left\n",
        "    #             else:\n",
        "    #                 return right\n",
        "\n",
        "    #     # This part should ideally not be reached if edge cases are handled and the list is sorted\n",
        "    #     return None\n",
        "\n",
        "    def get_reward(self, s_p, s_c, a_b, a_w):\n",
        "        \"\"\"\n",
        "        Calculate the reward for given state-action values.\n",
        "        \"\"\"\n",
        "        c_plus = self.settings['c_plus']\n",
        "        c_minus = self.settings['c_minus']\n",
        "        e = a_b + a_w\n",
        "        if s_p >= 0:\n",
        "            return s_c * s_p - c_plus * abs(s_c - e)\n",
        "        if s_p < 0:\n",
        "            return s_c * s_p - c_minus * abs(s_c - e)\n",
        "        raise ValueError(\"Reward can not be calculated.\")\n",
        "\n",
        "    def generate_price_next_hour(self, price):\n",
        "        \"\"\"\n",
        "        Generates a random sample of the price for the next hour based on an AR(1) model.\n",
        "\n",
        "        Args:\n",
        "            price (float): The current price.\n",
        "\n",
        "        Returns:\n",
        "            float: A random sample of the next hour's price.\n",
        "        \"\"\"\n",
        "        phi = self.model_price.params.iloc[1]\n",
        "        intercept = self.model_price.params.iloc[0]\n",
        "        sigma = self.model_price.resid.std()\n",
        "        # The mean of the next price's distribution is the predicted value\n",
        "        mean_next_price = intercept + phi * price\n",
        "        # The standard deviation of the next price's distribution is the residual standard deviation\n",
        "        std_dev_next_price = sigma\n",
        "        # Generate a random sample from a normal distribution\n",
        "        price_next_sample = self.rng.normal(loc=mean_next_price, scale=std_dev_next_price)\n",
        "        return price_next_sample\n",
        "\n",
        "    def generate_wind_next_hour(self, wind):\n",
        "        \"\"\"\n",
        "        Generates a random sample of the wind speed for the next hour based on an AR(1) model.\n",
        "\n",
        "        Args:\n",
        "            wind (float): The current wind speed.\n",
        "\n",
        "        Returns:\n",
        "            float: A random sample of the next hour's wind speed.\n",
        "        \"\"\"\n",
        "        log_wind = np.log(wind)\n",
        "        phi = self.model_wind.params.iloc[1]\n",
        "        intercept = self.model_wind.params.iloc[0]\n",
        "        sigma = self.model_wind.resid.std()\n",
        "        # The mean of the next wind speed's distribution is the predicted value\n",
        "        log_mean_next_wind = intercept + phi * log_wind\n",
        "        # The standard deviation of the next wind speed's distribution is the residual standard deviation\n",
        "        std_dev_next_wind = sigma\n",
        "        # Generate a random sample from a normal distribution\n",
        "        log_wind_next_sample = self.rng.normal(loc=log_mean_next_wind, scale=std_dev_next_wind)\n",
        "        return np.exp(log_wind_next_sample)\n",
        "\n",
        "    # def get_interval_midpoint(self, a, intervals):\n",
        "    #     \"\"\"\n",
        "    #     Return interval midpoint given a point and interval.\n",
        "\n",
        "    #     Args:\n",
        "    #         midpoint (float): The midpoint of the interval.\n",
        "    #         intervals (dict): A dictionary\n",
        "    #     \"\"\"\n",
        "    #     midpoints = intervals['midpoints']\n",
        "    #     intervals_dict = intervals['intervals_dict']\n",
        "    #     for midpoint in midpoints:\n",
        "    #         lower_bound, upper_bound = intervals_dict[midpoint]['ab']\n",
        "    #         if lower_bound <= a < upper_bound:\n",
        "    #             return midpoint\n",
        "    #     return None\n",
        "\n",
        "    def get_time_step_data(self, s, a):\n",
        "        \"\"\"\n",
        "        Compute one-step transition and reward from (s, a).\n",
        "\n",
        "        Args:\n",
        "            s: State label\n",
        "            a: Action label\n",
        "\n",
        "        Returns:\n",
        "            Dict with:\n",
        "                - 'r': float reward.\n",
        "                - 'sN': next state label \"xN,yN\".\n",
        "        \"\"\"\n",
        "        a = self.str_to_dict(a)\n",
        "        a_b = a['a_b']\n",
        "        a_w = a['a_w']\n",
        "        a_c = a['a_c']\n",
        "        s = self.str_to_dict(s)\n",
        "        s_l = s['s_l']\n",
        "        s_c = s['s_c']\n",
        "        if not self.s_p: self.s_p = s['s_p']  # initialize if starting state\n",
        "        if not self.s_w: self.s_w = s['s_w']\n",
        "\n",
        "        # # check\n",
        "        # s_w = s['s_w']\n",
        "        # s_p = s['s_p']\n",
        "        # if self.dis.get_interval_midpoint(self.s_p, self.s_p_inv) != s_p:\n",
        "        #     print(\"Current price:\", self.s_p)\n",
        "        #     print(\"State midpoint:\", s_p)\n",
        "        #     print(\"State interval:\", self.s_p_inv['intervals_dict'][s_p])\n",
        "        #     raise ValueError(\"Invalid p state.\")\n",
        "        # if self.dis.get_interval_midpoint(self.s_w, self.s_w_inv) != s_w:\n",
        "        #     print(\"Current wind:\", self.s_w)\n",
        "        #     print(\"State midpoint:\", s_w)\n",
        "        #     print(\"State interval:\", self.s_w_inv['intervals_dict'][s_w])\n",
        "        #     raise ValueError(\"Invalid w state.\")\n",
        "\n",
        "        s_p = self.s_p  # assume that current value of s_p is stored in self.s_p\n",
        "        s_w = self.s_w\n",
        "        # print(\"Current wind and price:\", s_w, \"|\", s_p)\n",
        "\n",
        "        s_l_next = s_l - a_b\n",
        "        s_c_next = a_c\n",
        "        self.s_w = self.generate_wind_next_hour(self.s_w) # update current values\n",
        "        self.s_p = self.generate_price_next_hour(self.s_p)\n",
        "        # print(\"New wind and price:\", s_w, \"|\", s_p)\n",
        "        s_p_next = self.dis.get_interval_midpoint(self.s_p, self.s_p_inv)\n",
        "        s_w_next = self.dis.get_interval_midpoint(self.s_w, self.s_w_inv)\n",
        "        s_next = self.state_variables_to_str(s_l_next, s_c_next, s_w_next, s_p_next)\n",
        "        reward = self.get_reward(s_p, s_c, a_b, a_w)\n",
        "        return {\"r\": reward, \"sN\": s_next}"
      ],
      "metadata": {
        "id": "zEeuLBrEeApv",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2\n",
        "\n",
        "Explain what the method `get_time_step_data` does.\n",
        "\n",
        "- What is input?\n",
        "- What is the role of `self.s_p` and `self.s_w`?\n",
        "- What is the purpose of the `generate_` methods?\n",
        "- What is returned?"
      ],
      "metadata": {
        "id": "p2z3BDinOuy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "Your comments here.\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "GTxNoH6Btdr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create environment\n",
        "\n",
        "\n",
        "We consider the following problem parameters and create the environment."
      ],
      "metadata": {
        "id": "qoMpn3KvF9WD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Problem parameters\n",
        "\n",
        "settings = {\n",
        "    'b_max': 10,  # battery capacity (MWh)\n",
        "    'p_max': 10,  # max power output wind (MW)\n",
        "    'c_max': 10 + 10,  # max commitment (MWh) # b_max + p_max\n",
        "    'w_cut_in': 3,  # cut in wind speed (m/s)\n",
        "    'w_rated': 12,  # rated wind speed (m/s)\n",
        "    'w_cut_out': 25,  # cut out wind speed (m/s)\n",
        "    'c_plus': 50,  # EUR/MWh\n",
        "    'c_minus': 50, # EUR/MWh\n",
        "}\n",
        "env = EnvEnergy(settings)"
      ],
      "metadata": {
        "id": "k9EAn5jJVFwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the environment need a discretisation to work."
      ],
      "metadata": {
        "id": "c0ckwwWsVvLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Discretisation used\n",
        "\n",
        "## Discretisation parameters\n",
        "step_mwh = 2  # step_mwh (float): Step size for energy resolution.\n",
        "\n",
        "## Create discretisation\n",
        "b_max = settings['b_max']\n",
        "p_max = settings['p_max']\n",
        "c_max = settings['c_max']\n",
        "w_cut_in = settings['w_cut_in']\n",
        "w_rated = settings['w_rated']\n",
        "w_cut_out = settings['w_cut_out']\n",
        "dis = Discretise()\n",
        "\n",
        "# Action variables\n",
        "a_b_set = dis.generate_discrete_set(-b_max, b_max, step_mwh)\n",
        "a_w_set = dis.generate_discrete_set(0, p_max, step_mwh)\n",
        "a_c_set = dis.generate_discrete_set(-b_max, c_max, step_mwh)\n",
        "print(\"Battery usage a_b:\", a_b_set)\n",
        "print(\"Wind usage a_w:\", a_w_set)\n",
        "print(\"Commitment t+1 a_c:\", a_c_set)\n",
        "\n",
        "# Battery (MWh)\n",
        "s_l_set = dis.generate_discrete_set(0, b_max, step_mwh)\n",
        "print(\"Battery level s_l:\", s_l_set)\n",
        "\n",
        "# Commitment (MWh)\n",
        "s_c_set = dis.generate_discrete_set(-b_max, c_max, step_mwh)\n",
        "print(\"Commitment s_c:\", s_c_set)\n",
        "\n",
        "# Wind (m/s)\n",
        "val = [round(env.energy_inverse(e), 1) for e in a_w_set]  # wind speeds corresponding to a_w_set\n",
        "print(\"\\nWind speeds corresponding to a_w_set:\", val)   # want these as midpoints in intervals\n",
        "s_w_inv = dis.generate_intervals(0, val[1], val[1])    # zero commitment\n",
        "for i, v in enumerate(val):\n",
        "    if i > 1:\n",
        "        dis.add_interval(val[i-1], v, s_w_inv)\n",
        "dis.add_interval(w_rated, w_cut_out, s_w_inv)\n",
        "dis.add_interval(w_cut_out, np.inf, s_w_inv)\n",
        "e_prod = [env.energy(w) for w in s_w_inv['midpoints']] # possible energy production given wind intervals\n",
        "print(\"Possible energy production given wind intervals:\", e_prod)\n",
        "print(\"What we can sell given intervals:\", [env.dis.get_left(a_w_set, e) for e in e_prod])  # what we can sell\n",
        "print(\"Wind s_w:\")\n",
        "pprint(s_w_inv)\n",
        "\n",
        "# Prices (Euro)\n",
        "s_p_inv = dis.generate_intervals(-40, 15, 5, add_inf = True, add_neg_inf = True)\n",
        "print(\"\\nPrices s_p:\")\n",
        "pprint(s_p_inv)\n",
        "\n",
        "env.set_discretisation(s_l_set, s_c_set, s_w_inv, s_p_inv, a_b_set, a_w_set, a_c_set)"
      ],
      "metadata": {
        "id": "htFl5RhVF90c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3\n",
        "\n",
        "Explain the output.\n",
        "\n",
        "- What levels of energy can be sold/bought?\n",
        "- Why are these specific wind intervals used?\n",
        "- Why are price intervals small around zero?\n",
        "- How is the discretisation added to the environment?  "
      ],
      "metadata": {
        "id": "YcuTMIYVPE5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "Your comments here.\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "Q7_kk50KPE5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us test the environment:"
      ],
      "metadata": {
        "id": "3_8as79nZz_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"price:\")\n",
        "pprint(s_p_inv)\n",
        "print(\"wind:\")\n",
        "pprint(s_w_inv)\n",
        "print()\n",
        "states = env.get_states()\n",
        "s = states[2300]\n",
        "actions = env.get_actions(env.str_to_dict(s))\n",
        "a = env.dict_to_str(np.random.choice(actions))\n",
        "env.reset_current_state()\n",
        "for i in range(5):\n",
        "    print(\"State:\", s)\n",
        "    print(\"Action:\", a)\n",
        "    res = env.get_time_step_data(s, a)\n",
        "    print(\"Reward and new state:\", res)\n",
        "    print(\"Current price (in the inverval):\", env.s_p)\n",
        "    print(\"Current wind (in the inverval):\", env.s_w, \"\\n\")\n",
        "    s = res['sN']\n",
        "    a = env.dict_to_str(np.random.choice(env.get_actions(env.str_to_dict(s))))"
      ],
      "metadata": {
        "id": "2rTF5_07Mrrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4\n",
        "\n",
        "Explain the output.\n",
        "\n",
        "- Why is calling `env.reset_current_state()` important when start exploring?\n",
        "- Which policy is used here to select actions?\n",
        "- Check that the current price is in the interval of the new state.\n",
        "- How is the reward calculated? That is, which variables are used, and how do these relate to the discretisation?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WB8SE81bPFG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "Your comments here.\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "8wsbclRxPFG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MDP solution\n",
        "\n",
        "Before solving the problem using model-free RL. Let us consider the optimal policy given a discount rate of 0.5, found using the MDP and policy iteration."
      ],
      "metadata": {
        "id": "OsUJUCJhe8xH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_mdp_05 = pd.read_csv('https://drive.google.com/uc?id=1-hkOOFmtQ4bBWA4h81r0UGU26fRop6n8', index_col = False)\n",
        "df_mdp_05 >> head(2)"
      ],
      "metadata": {
        "id": "RB1ZW2ARffA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot some results\n",
        "dat = (\n",
        "    df_mdp_05\n",
        "        >> mask((X.s_l == 0) | (X.s_l == 2) | (X.s_l == 5) | (X.s_l == 10))\n",
        "        >> mask((X.s_c == -10) | (X.s_c == -2) | (X.s_c == 0) | (X.s_c == 2) | (X.s_c == 10) | (X.s_c == 20))\n",
        "        >> mask(X.steady_pr > 0)\n",
        ")\n",
        "dat['a_c'] = pd.Categorical(dat['a_c'])  # Convert 'a_c' to a categorical variable for discrete scaling\n",
        "pt = (\n",
        "    ggplot(dat, aes(\"s_w_idx\", \"s_p_idx\", label = \"a_c\", color=\"a_c\"))\n",
        "    + geom_point(aes(shape = 'imbalance', color='imbalance'), size = 10, alpha = 0.2)\n",
        "    + geom_label(size = 9)\n",
        "    + facet_wrap(\"~ s_l + s_c\", labeller=\"label_both\")\n",
        "    # + facet_grid(rows = \"s_l\", cols = \"s_c\", labeller=\"label_both\")\n",
        "    # + theme(legend_position='none')\n",
        "    + labs(title = f\"Optimal policy MDP\", x = \"Wind speed (m/s)\", y = \"Price (EUR/MWh)\")\n",
        "    + theme(figure_size=(20,15), axis_text_x=element_text(rotation=90, hjust=1), legend_position=\"bottom\")\n",
        "    # + guides(color=guide_legend(nrow=2, byrow=True))\n",
        "    + guides(color=False)\n",
        "    + scale_color_discrete()\n",
        "    + scale_x_continuous(breaks=dat['s_w_idx'].unique(), labels=dat['s_w_str'].unique())\n",
        "    + scale_y_continuous(breaks=dat['s_p_idx'].unique(), labels=dat['s_p_str'].unique())\n",
        ")\n",
        "pt.show()"
      ],
      "metadata": {
        "id": "_1ZxXGNei8JH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us calculate the min, average and max state-value."
      ],
      "metadata": {
        "id": "Ouwm-5APpdxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_mdp_05 >> summarise(min_v = X.v.min(), avg_v = X.v.mean(), max_v = X.v.max())"
      ],
      "metadata": {
        "id": "bF00mbvqpjpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create agent  \n",
        "\n",
        "We are now ready to create the agent and do model-free RL. First, we load the agent class."
      ],
      "metadata": {
        "id": "IkEcw87RL_RC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szh8HtKZl2n6"
      },
      "outputs": [],
      "source": [
        "#@title Generic RL agent\n",
        "\n",
        "import math  # math helpers\n",
        "import random  # tie-breaking choices\n",
        "from collections import defaultdict  # lazy nested dicts\n",
        "from typing import Optional, List, Dict, Any  # typing\n",
        "\n",
        "import numpy as np  # vector ops and RNG\n",
        "import pandas as pd  # tabular data\n",
        "\n",
        "\n",
        "try:\n",
        "    from plotnine import ggplot, aes, geom_col, geom_tile, labs, theme_minimal  # plotting\n",
        "except Exception:  # pragma: no cover\n",
        "    ggplot = None  # fallback if plotnine isn't installed\n",
        "\n",
        "\n",
        "class RLAgent:\n",
        "    \"\"\"\n",
        "    Tabular RL agent with:\n",
        "      - per-state action dictionaries {'q': value, 'n': visits}\n",
        "      - behavior policy pi (dict action->prob)\n",
        "      - state value v and state visit counter n\n",
        "\n",
        "    Uses defaultdict so states/actions can be created lazily.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        # model[state] = dict with keys:\n",
        "        #   'pi': policy dict(action->prob)\n",
        "        #   'v': state value\n",
        "        #   'n': state visit count\n",
        "        #   'actions': dict(action -> {'q': float, 'n': int})\n",
        "        self.model: Dict[str, Dict[str, Any]] = defaultdict(\n",
        "            lambda: {\n",
        "                \"pi\": None,     # policy probabilities\n",
        "                \"v\": float(\"nan\"),  # state value\n",
        "                \"n\": 0,         # state visits\n",
        "                \"actions\": defaultdict(lambda: {\"q\": 0.0, \"n\": 0}),  # actions\n",
        "            }\n",
        "        )  # core store\n",
        "\n",
        "    # ----------------------------- helpers ------------------------------------\n",
        "\n",
        "    def add_states(self, states: List[str]) -> None:\n",
        "        \"\"\"Force creation of states (defaultdict makes them auto-create).\"\"\"  # eager create\n",
        "        for s in states:\n",
        "            _ = self.model[str(s)]  # touch to ensure creation\n",
        "\n",
        "    def add_state_action(self, s: str, a: str) -> None:\n",
        "        \"\"\"Ensure a state and a specific action exist.\"\"\"  # lazy create\n",
        "        _ = self.model[str(s)][\"actions\"][str(a)]  # touch to ensure creation\n",
        "\n",
        "    def add_actions(self, s: str, actions: List[str]) -> None:\n",
        "        \"\"\"Force creation of actions in state s.\"\"\"  # batch add\n",
        "        for a in actions:\n",
        "            _ = self.model[str(s)][\"actions\"][str(a)]  # touch to ensure creation\n",
        "\n",
        "    def add_states_and_actions(self, df: pd.DataFrame) -> None:\n",
        "        \"\"\"Bulk add (state, action) pairs from DataFrame with columns 's' and 'a'.\"\"\"  # bulk\n",
        "        for s, a in zip(df[\"s\"].astype(str), df[\"a\"].astype(str)):\n",
        "            _ = self.model[s][\"actions\"][a]  # touch-create\n",
        "\n",
        "    # ----------------------------- setters ------------------------------------\n",
        "\n",
        "    def set_action_value(self, value: float = 0.0) -> None:\n",
        "        \"\"\"Set q(s,a) to constant for all actions.\"\"\"  # initializer/reset\n",
        "        for s in self.model:\n",
        "            for a in self.model[s][\"actions\"]:\n",
        "                self.model[s][\"actions\"][a][\"q\"] = float(value)  # assign\n",
        "\n",
        "    def set_state_value(self,\n",
        "                        states: Optional[List[str]] = None,\n",
        "                        value: float = 0.0) -> None:\n",
        "        \"\"\"Set v(s) for given states (all if None).\"\"\"  # V setter\n",
        "        states = states or list(self.model.keys())\n",
        "        for s in states:\n",
        "            self.model[s][\"v\"] = float(value)  # assign\n",
        "\n",
        "    def set_action_ctr_value(self, ctr_value: int = 0) -> None:\n",
        "        \"\"\"Set all action counters to ctr_value.\"\"\"  # reset N(s,a)\n",
        "        for s in self.model:\n",
        "            for a in self.model[s][\"actions\"]:\n",
        "                self.model[s][\"actions\"][a][\"n\"] = int(ctr_value)  # assign\n",
        "\n",
        "    def set_state_ctr_value(self, ctr_value: int = 0) -> None:\n",
        "        \"\"\"Set all state visit counters to ctr_value.\"\"\"  # reset N(s)\n",
        "        for s in self.model:\n",
        "            self.model[s][\"n\"] = int(ctr_value)  # assign\n",
        "\n",
        "    def set_action_value_single(self,\n",
        "                                s: str,\n",
        "                                a: str,\n",
        "                                value: float = 0.0,\n",
        "                                ctr_value: int = 0) -> None:\n",
        "        \"\"\"Set q(s,a) and n(s,a) for a single state-action.\"\"\"  # direct set\n",
        "        _ = self.model[str(s)][\"actions\"][str(a)]  # ensure exists\n",
        "        self.model[s][\"actions\"][a][\"q\"] = float(value)  # set q\n",
        "        self.model[s][\"actions\"][a][\"n\"] = int(ctr_value)  # set n\n",
        "\n",
        "    def set_random_eps_greedy_policy(self, eps: float) -> None:\n",
        "        \"\"\"Set œÄ(s) to random Œµ-greedy (random greedy action per state).\"\"\"  # init œÄ\n",
        "        for s in self.model:\n",
        "            actions = list(self.model[s][\"actions\"].keys())  # available actions\n",
        "            if not actions:\n",
        "                self.model[s][\"pi\"] = None  # no policy if no actions\n",
        "                continue  # skip\n",
        "            base = eps / len(actions)  # exploration mass\n",
        "            pi = {a: base for a in actions}  # flat\n",
        "            a_star = random.choice(actions)  # random greedy pick\n",
        "            pi[a_star] += 1.0 - eps  # exploitation bump\n",
        "            self.model[s][\"pi\"] = pi  # store\n",
        "\n",
        "    def set_eps_greedy_policy(self, eps: float, states: List[str] | str) -> None:\n",
        "        \"\"\"\n",
        "        Make policy epsilon-greedy w.r.t current q-values.\n",
        "        \"\"\"\n",
        "        states_list = [states] if isinstance(states, str) else list(states)\n",
        "        for s in states_list:\n",
        "            actions = list(self.model[s][\"actions\"].keys())\n",
        "            if not actions:\n",
        "                self.model[s][\"pi\"] = None  # no policy\n",
        "                continue  # skip\n",
        "            q_vals = np.array([self.model[s][\"actions\"][a][\"q\"] for a in actions])  # q-vector\n",
        "            max_mask = q_vals == q_vals.max()  # tie mask\n",
        "            idx = int(np.random.choice(np.flatnonzero(max_mask)))  # random tie\n",
        "            base = eps / len(actions)  # exploration mass\n",
        "            pi = {a: base for a in actions}  # flat init\n",
        "            pi[actions[idx]] += 1.0 - eps  # greedy bump\n",
        "            self.model[s][\"pi\"] = pi  # assign\n",
        "\n",
        "    def set_greedy_policy(self, states: Optional[List[str]] = None) -> None:\n",
        "        \"\"\"Set greedy deterministic policy from q-values.\"\"\"  # greedy œÄ\n",
        "        states = states or list(self.model.keys())\n",
        "        for s in states:\n",
        "            actions = list(self.model[s][\"actions\"].keys())\n",
        "            if not actions:\n",
        "                self.model[s][\"pi\"] = None  # no policy\n",
        "                continue  # skip\n",
        "            q_vals = [self.model[s][\"actions\"][a][\"q\"] for a in actions]  # q list\n",
        "            best = actions[int(np.argmax(q_vals))]  # greedy idx\n",
        "            self.model[s][\"pi\"] = {best: 1.0}  # point mass\n",
        "\n",
        "    def set_policy(self, states: List[str], pi: Dict[str, float]) -> None:\n",
        "        \"\"\"Set œÄ(s) explicitly for each s in states (probabilities need not be normalized).\"\"\"  # explicit œÄ\n",
        "        total = sum(pi.values())  # sum\n",
        "        norm = {a: (p / total) for a, p in pi.items()} if total > 0 else {a: 0.0 for a in pi}  # normalize\n",
        "        for s in states:\n",
        "            self.model[s][\"pi\"] = dict(norm)  # copy in\n",
        "\n",
        "    # ----------------------------- getters ------------------------------------\n",
        "\n",
        "    def get_info(self):\n",
        "        \"\"\"\n",
        "        Returns information about the agent.\n",
        "\n",
        "        Returns:\n",
        "            dict: The info.\n",
        "        \"\"\"\n",
        "        res = {}\n",
        "        res[\"states\"] = len(self.model)\n",
        "        res[\"actions\"] = sum([len(self.model[k][\"actions\"]) for k in self.model.keys()])\n",
        "        return res\n",
        "\n",
        "    def get_state_keys(self) -> List[str]:\n",
        "        return list(self.model.keys())  # all states\n",
        "\n",
        "    def get_action_keys(self, s: str) -> List[str]:\n",
        "        return list(self.model[s][\"actions\"].keys())  # actions in s\n",
        "\n",
        "    def get_action_info(self, s: str) -> Dict[str, Dict[str, Any]]:\n",
        "        \"\"\"Return shallow copy of the actions dict for state s.\"\"\"  # inspection\n",
        "        return dict(self.model[s][\"actions\"])  # shallow copy\n",
        "\n",
        "    def get_state_value_q(self, s: str) -> float:\n",
        "        \"\"\"Compute v_pi(s) = sum_a pi(a|s) q(s,a).\"\"\"  # V from Q & œÄ\n",
        "        pi = self.model[s][\"pi\"] or {}  # distribution\n",
        "        return float(sum(p * self.model[s][\"actions\"][a][\"q\"]\n",
        "                         for a, p in pi.items()))  # dot product\n",
        "\n",
        "    def get_state_values(self,\n",
        "                         states: Optional[List[str]] = None) -> pd.DataFrame:\n",
        "        \"\"\"Return DataFrame of (state, v). Uses dfply::mutate if available.\"\"\"  # tidy\n",
        "        states = states or list(self.model.keys())\n",
        "        df = pd.DataFrame({\"state\": states})  # seed\n",
        "        return pd.DataFrame({\n",
        "            \"state\": states,\n",
        "            \"v\": [self.model[s][\"v\"] for s in states],\n",
        "        })  # basic\n",
        "\n",
        "    def get_policy(self, states: Optional[List[str]] = None) -> pd.DataFrame:\n",
        "        \"\"\"Return DataFrame (state, action, pr) for current œÄ.\"\"\"  # tidy œÄ\n",
        "        states = states or list(self.model.keys())\n",
        "        rows = []  # collect\n",
        "        for s in states:\n",
        "            pi = self.model[s][\"pi\"] or {}  # distribution\n",
        "            for a, p in pi.items():\n",
        "                rows.append({\"state\": s, \"action\": a, \"pr\": float(p)})  # row\n",
        "        return pd.DataFrame(rows)  # frame\n",
        "\n",
        "    def get_state_action_q_mat(self) -> pd.DataFrame:\n",
        "        \"\"\"Return wide Q-matrix DataFrame (rows=states, cols=actions).\"\"\"  # matrix\n",
        "        states = list(self.model.keys())  # rows\n",
        "        actions = sorted({a for s in states for a in self.model[s][\"actions\"].keys()})  # unique cols\n",
        "        mat = pd.DataFrame(np.nan, index=states, columns=actions)  # init\n",
        "        for s in states:\n",
        "            for a, rec in self.model[s][\"actions\"].items():\n",
        "                mat.loc[s, a] = rec[\"q\"]  # fill\n",
        "        return mat  # matrix\n",
        "\n",
        "    def get_action_values(self,\n",
        "                          states: Optional[List[str]] = None) -> pd.DataFrame:\n",
        "        \"\"\"Return long-form DataFrame of q-values and counts.\"\"\"  # tidy Q\n",
        "        states = [states] if isinstance(states, str) else states\n",
        "        states = states or list(self.model.keys())\n",
        "        rows = []\n",
        "        for s in states:\n",
        "            for a, info in self.model[s][\"actions\"].items():\n",
        "                rows.append({\n",
        "                    \"state\": s,\n",
        "                    \"action\": a,\n",
        "                    \"q\": info[\"q\"],\n",
        "                    \"n\": info[\"n\"],\n",
        "                })\n",
        "        return pd.DataFrame(rows)  # frame\n",
        "\n",
        "    # ----------------------------- action selection ---------------------------\n",
        "\n",
        "    def get_action_ucb(self, s: str, coeff: float = 1.0) -> Optional[str]:\n",
        "        \"\"\"UCB1-like selection; updates n(s) and n(s,a).\"\"\"  # UCB\n",
        "        actions = list(self.model[s][\"actions\"].keys())  # available\n",
        "        if not actions:\n",
        "            return None  # no action\n",
        "        self.model[s][\"n\"] += 1  # visit state\n",
        "        qv = np.array([self.model[s][\"actions\"][a][\"q\"] for a in actions], dtype=float)  # q\n",
        "        na = np.array([max(1, self.model[s][\"actions\"][a][\"n\"]) for a in actions], dtype=float)  # counts\n",
        "        ns = float(self.model[s][\"n\"])  # state count\n",
        "        bonus = coeff * np.sqrt(np.log(ns + 1e-4) / na)  # exploration term\n",
        "        idx = int(np.argmax(qv + bonus))  # argmax\n",
        "        a = actions[idx]  # pick\n",
        "        self.model[s][\"actions\"][a][\"n\"] += 1  # visit action\n",
        "        return a  # chosen\n",
        "\n",
        "    def get_action_eg(self, s: str, eps: float) -> str:\n",
        "        \"\"\"Epsilon-greedy action selection (increments counters).\"\"\"  # Œµ-greedy\n",
        "        self.model[s][\"n\"] += 1  # visit state\n",
        "        actions = list(self.model[s][\"actions\"].keys())  # list\n",
        "        q = np.array([self.model[s][\"actions\"][a][\"q\"] for a in actions])  # q-values\n",
        "        max_mask = q == q.max()  # ties\n",
        "        idx_greedy = int(np.random.choice(np.flatnonzero(max_mask)))  # random tie\n",
        "        probs = np.full(len(actions), eps / len(actions), dtype=float)  # base mass\n",
        "        probs[idx_greedy] += 1.0 - eps  # greedy bump\n",
        "        idx = int(np.random.choice(np.arange(len(actions)), p=probs))  # sample\n",
        "        a = actions[idx]  # chosen action\n",
        "        self.model[s][\"actions\"][a][\"n\"] += 1  # visit action\n",
        "        return a  # return\n",
        "\n",
        "    def get_action_pi(self, s: str) -> Optional[str]:\n",
        "        \"\"\"Sample an action from stored pi(a|s) (increments counters).\"\"\"  # sample œÄ\n",
        "        self.model[s][\"n\"] += 1  # visit state\n",
        "        pi = self.model[s][\"pi\"] or {}  # distribution\n",
        "        if not pi:\n",
        "            return None  # no policy\n",
        "        actions, probs = zip(*pi.items())  # unpack\n",
        "        probs = np.array(probs, dtype=float)  # array\n",
        "        probs /= probs.sum() if probs.sum() > 0 else 1.0  # normalize\n",
        "        a = str(np.random.choice(list(actions), p=probs))  # draw\n",
        "        self.model[s][\"actions\"][a][\"n\"] += 1  # visit action\n",
        "        return a  # chosen\n",
        "\n",
        "    def get_max_action_value(self, s: str) -> float:\n",
        "        \"\"\"Return max_a Q(s,a).\"\"\"  # convenience\n",
        "        q = [rec[\"q\"] for rec in self.model[s][\"actions\"].values()]  # list\n",
        "        return float(max(q)) if q else float(\"nan\")  # handle empty\n",
        "\n",
        "    def get_exp_action_value(self, s: str) -> float:\n",
        "        \"\"\"Return E_{a~œÄ}[Q(s,a)] under current œÄ(s).\"\"\"  # expectation\n",
        "        pi = self.model[s][\"pi\"] or {}  # distribution\n",
        "        return float(sum(p * self.model[s][\"actions\"][a][\"q\"] for a, p in pi.items()))  # dot\n",
        "\n",
        "    # ----------------------------- learning -----------------------------------\n",
        "\n",
        "    def policy_eval_td0(self,\n",
        "                        env: Any,\n",
        "                        gamma: float = 1.0,\n",
        "                        alpha: float = 0.1,\n",
        "                        max_e: int = 1000,\n",
        "                        max_el: int = 10000,\n",
        "                        reset: bool = True,\n",
        "                        states: Optional[List[str]] = None) -> None:\n",
        "        \"\"\"\n",
        "        TD(0) policy evaluation of V(s). The environment used must implement:\n",
        "        get_time_step_data(s, a) -> {'r': float, 'sN': next_state or None}.\n",
        "\n",
        "        Args:\n",
        "            env: Environment with get_time_step_data method.\n",
        "            gamma: The discount factor.\n",
        "            alpha: Step-size parameter\n",
        "            max_e: Maximum number of iterations (episodes)\n",
        "            max_el: Maximum episode length.\n",
        "            reset: Reset action-values, state and action counters to 0.\n",
        "            states: Starting states. For each iteration, generate\n",
        "                an episode for each state. If `None` uses all states.\n",
        "        \"\"\"\n",
        "        if reset:\n",
        "            self.set_state_value(value=0.0)  # V=0\n",
        "        starts = states or self.get_state_keys()  # candidate starts\n",
        "        for _ in range(max_e):  # episodes\n",
        "            s = random.choice(starts)  # start\n",
        "            for i in range(max_el):  # steps\n",
        "                a = self.get_action_pi(s)  # act under œÄ\n",
        "                if a is None:  # no policy\n",
        "                    break  # abort\n",
        "                dat = env.get_time_step_data(s, a)  # env step\n",
        "                r, sN = dat[\"r\"], dat[\"sN\"]  # unpack\n",
        "                if sN is None:  # terminal\n",
        "                    break  # end\n",
        "                old_v = self.model[s][\"v\"]  # current V\n",
        "                td_target = r + gamma * self.model[sN][\"v\"]  # target\n",
        "                self.model[s][\"v\"] = old_v + alpha * (td_target - old_v)  # update\n",
        "                s = sN  # advance\n",
        "            if i + 1 == max_el:  # cap hit\n",
        "                break  # stop\n",
        "\n",
        "    def policy_eval_mc(self,\n",
        "                       env: Any,\n",
        "                       gamma: float = 1.0,\n",
        "                       theta: float = 0.1,\n",
        "                       min_ite: int = 100,\n",
        "                       max_ite: int = 2000,\n",
        "                       reset: bool = True,\n",
        "                       states: Optional[List[str]] = None,\n",
        "                       verbose: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        Every-visit Monte Carlo evaluation of V(s).\n",
        "        Env must implement: get_episode_pi(agent, s0) -> DataFrame with columns ['s','a','r'].  # contract\n",
        "        \"\"\"  # doc\n",
        "        if reset:\n",
        "            self.set_state_value(value=0.0)  # V=0\n",
        "            self.set_action_ctr_value(0)  # reset N(s,a)\n",
        "            self.set_state_ctr_value(0)  # reset N(s)\n",
        "        starts = states or self.get_state_keys()  # start set\n",
        "        for ite in range(1, max_ite + 1):  # iterations\n",
        "            delta = 0.0  # max change\n",
        "            for s0 in starts:  # episode per start\n",
        "                df = env.get_episode_pi(self, s0)  # generate under œÄ\n",
        "                if df is None or len(df) == 0:\n",
        "                    continue  # skip\n",
        "                if verbose:\n",
        "                    df['g'] = np.nan\n",
        "                    df['n_s'] = np.nan\n",
        "                    df['old_v'] = np.nan\n",
        "                    df['new_v'] = np.nan\n",
        "                g = 0.0  # return accumulator\n",
        "                for i in range(len(df) - 1, -1, -1):  # reverse pass\n",
        "                    s = str(df.iloc[i][\"s\"])  # state\n",
        "                    r = float(df.iloc[i][\"r\"])  # reward\n",
        "                    g = r + gamma * g  # return update\n",
        "                    n_s = max(1, self.model[s][\"n\"])  # denom\n",
        "                    old_v = self.model[s][\"v\"]  # prev\n",
        "                    step = 1.0 / n_s  # 1/N schedule\n",
        "                    self.model[s][\"v\"] = old_v + step * (g - old_v)  # update\n",
        "                    delta = max(delta, abs(old_v - self.model[s][\"v\"]))  # track\n",
        "                    if verbose:\n",
        "                        df.at[i,\"g\"] = g\n",
        "                        df.at[i,\"n_s\"] = n_s\n",
        "                        df.at[i,\"old_v\"] = old_v\n",
        "                        df.at[i,\"new_v\"] = self.model[s][\"v\"]\n",
        "                if verbose:\n",
        "                    print(\"Episode:\")\n",
        "                    print(df)  # trace\n",
        "            if delta < theta and ite >= min_ite:  # convergence\n",
        "                break  # stop\n",
        "        if ite == max_ite:\n",
        "            print(f\"Policy eval algorithm stopped at max iterations allowed: {max_ite}\")  # warn\n",
        "        print(f\"Policy eval algorithm finished in {ite} iterations.\")  # info\n",
        "\n",
        "    def gpi_on_policy_mc(self,\n",
        "                         env: Any,\n",
        "                         gamma: float = 1.0,\n",
        "                         theta: float = 0.1,\n",
        "                         min_ite: int = 100,\n",
        "                         max_ite: int = 1000,\n",
        "                         reset: bool = True,\n",
        "                         states: Optional[List[str]] = None,\n",
        "                         eps: float = 0.1,\n",
        "                         verbose: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        On-policy GPI via Every-Visit MC control on Q(s,a).\n",
        "        Env must implement: get_episode(agent, s0, eps) -> DataFrame ['s','a','r']\n",
        "        and update visit counters.\n",
        "\n",
        "        Args:\n",
        "            env (Any): Environment with get_episode method.\n",
        "            gamma (float, optional): Discount factor. Defaults to 1.0.\n",
        "            theta (float, optional): Convergence threshold. Defaults to 0.1.\n",
        "            min_ite (int, optional): Minimum number of iterations. Defaults to 100.\n",
        "            max_ite (int, optional): Maximum number of iterations. Defaults to 1000.\n",
        "            reset (bool, optional): Reset action-values, state and action counters to 0.\n",
        "            states (list, optional): Starting states. For each iteration, generate\n",
        "                an episiode for each state. If `None uses all states.\n",
        "            eps (float, optional): Epsilon for eps-greedy policy. Defaults to 0.1.\n",
        "            verbose (bool, optional): Print episode info. Defaults to False.\n",
        "        \"\"\"\n",
        "        if reset:\n",
        "            self.set_action_value(0.0)  # Q=0\n",
        "            self.set_action_ctr_value(0)  # reset N(s,a)\n",
        "            self.set_state_ctr_value(0)  # reset N(s)\n",
        "        self.set_eps_greedy_policy(eps, self.get_state_keys())  # init œÄ_Œµ\n",
        "        starts = states or self.get_state_keys()  # start pool\n",
        "        for ite in range(1, max_ite + 1):  # iterations\n",
        "            delta = 0.0  # track |ŒîV|\n",
        "            for s0 in starts:  # episode per start\n",
        "                df = env.get_episode(self, s0, eps)  # behavior inside env\n",
        "                if verbose:\n",
        "                    df['g'] = np.nan\n",
        "                    df['n_sa'] = np.nan\n",
        "                    df['old_q'] = np.nan\n",
        "                    df['step'] = np.nan\n",
        "                    df['new_q'] = np.nan\n",
        "                    df['old_v'] = np.nan\n",
        "                    df['new_v'] = np.nan\n",
        "                if df is None or len(df) == 0:\n",
        "                    continue  # skip\n",
        "                g = 0.0  # return\n",
        "                for i in range(len(df) - 1, -1, -1):  # reverse sweep\n",
        "                    s = str(df.iloc[i][\"s\"])  # state\n",
        "                    a = str(df.iloc[i][\"a\"])  # action\n",
        "                    r = float(df.iloc[i][\"r\"])  # reward\n",
        "                    g = r + gamma * g  # return update\n",
        "                    # step size: (1 / n_sa) ** 0.5 as in R  # schedule\n",
        "                    n_sa = max(1, self.model[s][\"actions\"][a][\"n\"])  # visits\n",
        "                    old_q = self.model[s][\"actions\"][a][\"q\"]  # prev Q\n",
        "                    old_v = self.get_state_value_q(s)  # V before update\n",
        "                    step = (1.0 / n_sa) ** 0.5  # step-size\n",
        "                    new_q = old_q + step * (g - old_q)  # MC update\n",
        "                    self.model[s][\"actions\"][a][\"q\"] = new_q  # MC update\n",
        "                    self.set_eps_greedy_policy(eps, [s])  # improve œÄ(s)\n",
        "                    new_v = self.get_state_value_q(s)  # V after\n",
        "                    delta = max(delta, abs(old_v - new_v))  # track\n",
        "                    if verbose:\n",
        "                        df.at[i,\"g\"] = g\n",
        "                        df.at[i,\"n_sa\"] = n_sa\n",
        "                        df.at[i,\"old_q\"] = old_q\n",
        "                        df.at[i,\"step\"] = step\n",
        "                        df.at[i,\"new_q\"] = new_q\n",
        "                        df.at[i,\"old_v\"] = old_v\n",
        "                        df.at[i,\"new_v\"] = new_v\n",
        "                if verbose:\n",
        "                    print(\"Episode:\")\n",
        "                    print(df)\n",
        "\n",
        "            if delta < theta and ite >= min_ite:\n",
        "                break  # stop\n",
        "        if ite == max_ite:\n",
        "            print(f\"GPI algorithm stopped at max iterations allowed: {max_ite}\")\n",
        "        print(f\"GPI algorithm finished in {ite} iterations.\")  # info\n",
        "\n",
        "    def gpi_on_policy_sarsa(self,\n",
        "                            env: Any,\n",
        "                            gamma: float = 1.0,\n",
        "                            max_e: int = 1000,\n",
        "                            max_el: int = 10000,\n",
        "                            reset: bool = True,\n",
        "                            states: Optional[List[str]] = None,\n",
        "                            eps: float = 0.1,\n",
        "                            alpha: float = 0.1,\n",
        "                            verbose: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        On-policy SARSA with fixed Œ±.\n",
        "        Env must implement: get_time_step_data(s, a) -> {'r': float, 'sN': next_state or None}.\n",
        "\n",
        "        Args:\n",
        "            env (Any): Environment with get_time_step_data method.\n",
        "            gamma (float, optional): Discount factor. Defaults to 1.0.\n",
        "            max_e (int, optional): Maximum number of episodes. Defaults to 1000.\n",
        "            max_el (int, optional): Maximum episode length. Defaults to 10000.\n",
        "            reset (bool, optional): Reset action-values, state and action counters to 0.\n",
        "            states (list, optional): Starting states. For each iteration, generate\n",
        "                an episode for each state. If `None uses all states.\n",
        "            eps (float, optional): Epsilon for Œµ-greedy policy. Defaults to 0.1.\n",
        "            alpha (float, optional): Step-size parameter. Defaults to 0.1.\n",
        "            verbose (bool, optional): Print episode info. Defaults to False.\n",
        "        \"\"\"\n",
        "        if reset:\n",
        "            self.set_action_value(0.0)  # Q=0\n",
        "            self.set_action_ctr_value(0)  # counts\n",
        "        self.set_eps_greedy_policy(eps, self.get_state_keys())  # init œÄ_Œµ\n",
        "        starts = states or self.get_state_keys()  # start pool\n",
        "        if verbose:\n",
        "            print(f\"Start GPI/SARSA with max episode length {max_el}:\")\n",
        "        for _ in range(max_e):  # episodes\n",
        "            s = random.choice(starts)  # pick start\n",
        "            a = self.get_action_pi(s)  # first action under œÄ\n",
        "            for i in range(max_el):  # steps\n",
        "                if a is None:\n",
        "                    break  # no action available\n",
        "                dat = env.get_time_step_data(s, a)  # env step\n",
        "                r, sN = dat[\"r\"], dat[\"sN\"]  # unpack\n",
        "                if sN is None:\n",
        "                    break  # terminal\n",
        "                aN = self.get_action_pi(sN)  # next action\n",
        "                old_q = self.model[s][\"actions\"][a][\"q\"]  # current\n",
        "                target = r + gamma * (self.model[sN][\"actions\"][aN][\"q\"] if aN is not None else 0.0)  # SARSA target\n",
        "                self.model[s][\"actions\"][a][\"q\"] = old_q + alpha * (target - old_q)  # update\n",
        "                if verbose:\n",
        "                    print(f\"(s,a,r+,s+,a+) = ({s}, {a}, {r}, {sN}, {aN}) q({s}, {a}): {old_q} -> {self.model[s]['actions'][a]['q']}\")  # trace\n",
        "                self.set_eps_greedy_policy(eps, [s])  # local improve\n",
        "                s, a = sN, aN  # advance\n",
        "            if i + 1 == max_el:\n",
        "                break  # hit max length of an episode, i.e. no episodes and stop\n",
        "        print(\"GPI/SARSA finished.\")  # info\n",
        "\n",
        "    def gpi_off_policy_q_learning(self,\n",
        "                                  env: Any,\n",
        "                                  gamma: float = 1.0,\n",
        "                                  max_e: int = 1000,\n",
        "                                  max_el: int = 10000,\n",
        "                                  reset: bool = True,\n",
        "                                  states: Optional[List[str]] = None,\n",
        "                                  eps: float = 0.1,\n",
        "                                  alpha: float = 0.1,\n",
        "                                  decreasing_alpha: bool = False,\n",
        "                                  verbose: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        Off-policy Q-learning with behavior œÄ_Œµ and greedy target.\n",
        "        Env must implement: get_time_step_data(s, a) -> {'r': float, 'sN': next_state or None}.\n",
        "\n",
        "        Args:\n",
        "            env (Any): Environment with get_time_step_data method.\n",
        "            gamma (float, optional): Discount factor. Defaults to 1.0.\n",
        "            max_e (int, optional): Maximum number of episodes. Defaults to 1000.\n",
        "            max_el (int, optional): Maximum episode length. Defaults to 10000.\n",
        "            reset (bool, optional): Reset action-values, state and action counters to 0.\n",
        "            states (list, optional): Starting states. For each iteration, generate\n",
        "                an episode for each state. If `None uses all states.\n",
        "            eps (float, optional): Epsilon for Œµ-greedy policy. Defaults to 0.1.\n",
        "            alpha (float, optional): Step-size parameter. Defaults to 0.1.\n",
        "            decreasing_alpha (bool, optional): Use a decreasing step-size equal to 1/n_{s,a} ** 0.5 (alpha ignored). Defaults to False.\n",
        "            verbose (bool, optional): Print episode info. Defaults to False.\n",
        "        \"\"\"\n",
        "        if reset:\n",
        "            self.set_action_value(0.0)  # Q=0\n",
        "            self.set_action_ctr_value(0)  # counts\n",
        "        self.set_eps_greedy_policy(eps, self.get_state_keys())  # behavior œÄ_Œµ\n",
        "        starts = states or self.get_state_keys()  # start pool\n",
        "        if decreasing_alpha:\n",
        "            for _ in range(max_e):  # episodes\n",
        "                s = random.choice(starts)  # start\n",
        "                for i in range(max_el):  # steps\n",
        "                    a = self.get_action_pi(s)  # behaviour action (increase counter too)\n",
        "                    if a is None:\n",
        "                        break  # no action\n",
        "                    dat = env.get_time_step_data(s, a)  # env step\n",
        "                    r, sN = dat[\"r\"], dat[\"sN\"]  # unpack\n",
        "                    if sN is None:\n",
        "                        break  # terminal\n",
        "                    q_next = [rec[\"q\"] for rec in self.model[sN][\"actions\"].values()]  # next Qs\n",
        "                    max_q = max(q_next) if q_next else 0.0  # greedy target\n",
        "                    old_q = self.model[s][\"actions\"][a][\"q\"]  # current\n",
        "                    self.model[s][\"actions\"][a][\"q\"] = old_q + (1/self.model[s][\"actions\"][a][\"n\"] ** 0.5) * (r + gamma * max_q - old_q)  # update\n",
        "                    if verbose:\n",
        "                        print(f\"(s,a,r+,s+) = ({s}, {a}, {r}, {sN}) q({s}, {a}): {old_q} -> {self.model[s]['actions'][a]['q']} (maxQ={max_q})\")  # trace\n",
        "                    self.set_eps_greedy_policy(eps, [s])  # refresh behavior at s\n",
        "                    s = sN  # advance\n",
        "                if i + 1 == max_el:\n",
        "                    break  # cap\n",
        "        else:\n",
        "            for _ in range(max_e):  # episodes\n",
        "                s = random.choice(starts)  # start\n",
        "                for i in range(max_el):  # steps\n",
        "                    a = self.get_action_pi(s)  # behaviour action (increase counter too)\n",
        "                    if a is None:\n",
        "                        break  # no action\n",
        "                    dat = env.get_time_step_data(s, a)  # env step\n",
        "                    r, sN = dat[\"r\"], dat[\"sN\"]  # unpack\n",
        "                    if sN is None:\n",
        "                        break  # terminal\n",
        "                    q_next = [rec[\"q\"] for rec in self.model[sN][\"actions\"].values()]  # next Qs\n",
        "                    max_q = max(q_next) if q_next else 0.0  # greedy target\n",
        "                    old_q = self.model[s][\"actions\"][a][\"q\"]  # current\n",
        "                    self.model[s][\"actions\"][a][\"q\"] = old_q + alpha * (r + gamma * max_q - old_q)  # update\n",
        "                    if verbose:\n",
        "                        print(f\"(s,a,r+,s+) = ({s}, {a}, {r}, {sN}) q({s}, {a}): {old_q} -> {self.model[s]['actions'][a]['q']} (maxQ={max_q})\")  # trace\n",
        "                    self.set_eps_greedy_policy(eps, [s])  # refresh behavior at s\n",
        "                    s = sN  # advance\n",
        "                if i + 1 == max_el:\n",
        "                    break  # cap\n",
        "        self.set_greedy_policy()  # finalize with greedy œÄ\n",
        "        print(\"GPI/Q-learning finished.\")  # info\n",
        "\n",
        "\n",
        "    def gpi_on_policy_exp_sarsa(self,\n",
        "                                env: Any,\n",
        "                                gamma: float = 1.0,\n",
        "                                max_e: int = 1000,\n",
        "                                max_el: int = 10000,\n",
        "                                reset: bool = True,\n",
        "                                states: Optional[List[str]] = None,\n",
        "                                eps: float = 0.1,\n",
        "                                alpha: float = 0.1,\n",
        "                                verbose: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        On-policy Expected SARSA with fixed Œ±.\n",
        "        Env must implement: get_time_step_data(s, a) -> {'r': float, 'sN': next_state or None}.\n",
        "\n",
        "        Args:\n",
        "            env (Any): Environment with get_time_step_data method.\n",
        "            gamma (float, optional): Discount factor. Defaults to 1.0.\n",
        "            max_e (int, optional): Maximum number of episodes. Defaults to 1000.\n",
        "            max_el (int, optional): Maximum episode length. Defaults to 10000.\n",
        "            reset (bool, optional): Reset action-values, state and action counters to 0.\n",
        "            states (list, optional): Starting states. For each iteration, generate\n",
        "                an episode for each state. If `None uses all states.\n",
        "            eps (float, optional): Epsilon for Œµ-greedy policy. Defaults to 0.1.\n",
        "            alpha (float, optional): Step-size parameter. Defaults to 0.1.\n",
        "            verbose (bool, optional): Print episode info. Defaults to False.\n",
        "        \"\"\"\n",
        "        if reset:\n",
        "            self.set_action_value(0.0)  # Q=0\n",
        "            self.set_action_ctr_value(0)  # counts\n",
        "        self.set_eps_greedy_policy(eps, self.get_state_keys())  # init œÄ_Œµ\n",
        "        starts = states or self.get_state_keys()  # start pool\n",
        "        for _ in range(max_e):  # episodes\n",
        "            s = random.choice(starts)  # start\n",
        "            for i in range(max_el):  # steps\n",
        "                a = self.get_action_pi(s)  # act under œÄ\n",
        "                if a is None:\n",
        "                    break  # no action\n",
        "                dat = env.get_time_step_data(s, a)  # env step\n",
        "                r, sN = dat[\"r\"], dat[\"sN\"]  # unpack\n",
        "                if sN is None:\n",
        "                    break  # terminal\n",
        "                exp_q = self.get_exp_action_value(sN)  # expectation under œÄ(sN)\n",
        "                old_q = self.model[s][\"actions\"][a][\"q\"]  # current\n",
        "                self.model[s][\"actions\"][a][\"q\"] = old_q + alpha * (r + gamma * exp_q - old_q)  # update\n",
        "                if verbose:\n",
        "                    print(f\"(s,a,r,sN) = ({s}, {a}, {r}, {sN}) q({s}, {a}): {old_q} -> {self.model[s]['actions'][a]['q']} (expQ={exp_q})\")  # trace\n",
        "                self.set_eps_greedy_policy(eps, [s])  # improve behavior at s\n",
        "                s = sN  # advance\n",
        "            if i + 1 == max_el:\n",
        "                break  # cap\n",
        "        print(\"GPI/Expected-SARSA finished.\")  # info"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we create an instance of the class and add states and actions."
      ],
      "metadata": {
        "id": "PjuwUtu4woZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = RLAgent()\n",
        "agent.add_states(env.get_states())\n",
        "for s in agent.get_state_keys():\n",
        "    s_dict = env.str_to_dict(s)\n",
        "    actions = env.get_actions(s_dict)\n",
        "    if isinstance(actions, str):\n",
        "        actions = [actions]\n",
        "    agent.add_actions(s, actions)"
      ],
      "metadata": {
        "id": "ivIIyj05-h67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply Q-learning\n",
        "\n",
        "We are now ready to apply model-free RL. Let us consider Q-learning with a small sample path of length 5, where we print the calculations."
      ],
      "metadata": {
        "id": "34Klf7XGDR9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing Q-learning:\")\n",
        "env.reset_current_state()\n",
        "agent.gpi_off_policy_q_learning(env, max_el = 5, verbose = True, gamma = 0.5)\n",
        "\n",
        "def get_result_df(agent = agent, env = env, df_mdp = df_mdp_05, exclude_unvisited = True):\n",
        "    \"\"\"\n",
        "    Results as a big data frame. Keep only the greedy action. Hence values in column\n",
        "    n (visites to (s, a))  may be smaller than those in column n_s (visites to s).\n",
        "    \"\"\"\n",
        "    df_result = (\n",
        "        agent.get_action_values()\n",
        "        >> left_join(agent.get_policy())\n",
        "        >> group_by(X.state)\n",
        "        >> mutate(n_s = X.n.sum()) # calc state visits\n",
        "        >> ungroup()\n",
        "        >> mask(X.pr.notna())  # keep only greedy policy (best q-value)\n",
        "        >> select(~X.pr)  # remove pr column with 1s\n",
        "    )\n",
        "    if exclude_unvisited:\n",
        "        df_result = df_result >> mask(X.n_s > 0)\n",
        "    df_result = df_result.join(df_result['state'].apply(env.str_to_dict).apply(pd.Series))\n",
        "    df_result = df_result.join(df_result['action'].apply(env.str_to_dict).apply(pd.Series))\n",
        "    df_result = (\n",
        "        df_result\n",
        "        >> left_join(df_mdp, by = \"state\")\n",
        "        >> mutate(optimal_a = (X.action_x == X.action_y))\n",
        "        >> mutate(states = lambda x: len(x))\n",
        "        >> mutate(visits = X.n_s.sum())\n",
        "        >> rename(s_l = 's_l_x', s_c = 's_c_x', s_w = 's_w_x', s_p = 's_p_x', a_b = 'a_b_x', a_c = 'a_c_x', a_w = 'a_w_x')\n",
        "    )\n",
        "    return (df_result)\n",
        "\n",
        "df_result = get_result_df()\n",
        "display(df_result)"
      ],
      "metadata": {
        "id": "n-Sm8YDrHJCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5\n",
        "\n",
        "Explain the output.\n",
        "\n",
        "- What is Q-learning doing (you may have a look at the method)?\n",
        "- What is the difference between the action columns ending with `_y` and those not in the data frame (such as `a_c` and `a_c_y`)?"
      ],
      "metadata": {
        "id": "LJT-RDBvY--8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "Your comments here.\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "6nwvZP4bY--_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get a number of how good the approximation is, we may calculate the root mean square (RMS) error $$\\sqrt{\\frac{1}{|S|} \\sum_{s} (q(s,a^*)-v_{\\pi^*}(s))^2}.$$ This estimate, do not take into account how much a state is visited. This can be done using a weighted RMS\n",
        "$$\\sqrt{ \\sum_{s}\\frac{n_s}{n} (q(s,a^*)-v_{\\pi^*}(s))^2}.$$ Here $n$ is the total number of visits to all states and $n_s$ the number of visits to state $s$. That is, a weighted RMS puts more weight on the error for states we have visited often. Note that if a state has not been visited, then it is not included in the RMS.\n",
        "\n",
        "In the following, let us focus on the weighted RMS and a normalized version."
      ],
      "metadata": {
        "id": "Tl6fjsGpRleJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v_min, v_max = df_mdp_05['v'].min(), df_mdp_05['v'].max()\n",
        "\n",
        "def get_rms(df_result, weighted=True):\n",
        "    \"\"\"\n",
        "    Compute weighted or unweighted root mean squared error between q and v.\n",
        "\n",
        "    Args:\n",
        "        df_result : DataFrame with columns ['q', 'v', 'n_s', 'visits', 'states']\n",
        "        weighted  : bool, if True weight by n_s/visits else uniform\n",
        "\n",
        "    Returns:\n",
        "        float RMS error\n",
        "    \"\"\"\n",
        "    if weighted:\n",
        "        w = df_result['n_s'] / df_result['visits']\n",
        "        mse = (w * (df_result['q'] - df_result['v']) ** 2).sum()\n",
        "    else:\n",
        "        n_states = df_result['states'].iloc[0] if 'states' in df_result else len(df_result)\n",
        "        mse = ((df_result['q'] - df_result['v']) ** 2).sum() / n_states\n",
        "\n",
        "    rms = np.sqrt(mse)\n",
        "    nrms = rms / (v_max - v_min)\n",
        "    return rms, nrms\n",
        "\n",
        "rms, nrms = get_rms(df_result)\n",
        "print(f\"Weighted RMS = {rms:.6f} | nRMS = {nrms:.6f}\")"
      ],
      "metadata": {
        "id": "4YObwRmDE5pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6\n",
        "\n",
        "Give an interpretation of the RMS value.\n",
        "\n"
      ],
      "metadata": {
        "id": "EQwOyWQXbURP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "Your comments here.\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "9ZX5Hk_0bURQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now apply Q-learning using a larger sample path."
      ],
      "metadata": {
        "id": "-UQ5oOrnJth9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nQ-learning:\")\n",
        "env.reset_current_state()\n",
        "env.reset_rng(seed=123)\n",
        "agent.gpi_off_policy_q_learning(env, max_el = 50000, reset = True, gamma = 0.5, eps = 0.1, alpha = 0.1)\n",
        "df_result = get_result_df()\n",
        "rms, nrms = get_rms(df_result, weighted=False)\n",
        "print(f\"Unweighted RMS = {rms:.6f} | nRMS = {nrms:.6f}\")"
      ],
      "metadata": {
        "id": "-e8rRaySJsuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7\n",
        "\n",
        "Examine the results.\n",
        "\n",
        "- How many times have states been visited (average and max)?\n",
        "- Why has the RMS value increased?\n",
        "- Assume that we apply a behavioural policy that visit all state-action pairs evenly. How many times would the $q(s,a)$ be updated if the number of samples in our sample path is two times the total number of actions?\n",
        "- Should the number of samples be higher or as is to get a good estimate of the optimal q-values?\n"
      ],
      "metadata": {
        "id": "IKhLAHNJkDZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "Your comments here.\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "fl5clO1hkDZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q8\n",
        "\n",
        "Consider the min, average and max state-values that we found given the MDP. How would Q-learning be affected by setting initial q-values to the min, average or max state-values found?\n",
        "\n"
      ],
      "metadata": {
        "id": "Qtudo7_bl0VT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "Your comments here.\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "LrsKgdQ4t4vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q9\n",
        "\n",
        "Run Q-learning as before. However, set the initial values to 350, i.e the average value of the MDP, which is approximately 350. Did your approximation improve?\n"
      ],
      "metadata": {
        "id": "z32HDAC1t8Rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "# Your code\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "Your comments here.\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "U_xm5eqTl0VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider code for plotting the current policy."
      ],
      "metadata": {
        "id": "LpOMHoWtmLmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_plot(dat = df_result):\n",
        "    dat = (\n",
        "        dat\n",
        "            >> mask((X.s_l == 0) | (X.s_l == 2) | (X.s_l == 5) | (X.s_l == 10))\n",
        "            >> mask((X.s_c == -10) | (X.s_c == -2) | (X.s_c == 0) | (X.s_c == 2) | (X.s_c == 10) | (X.s_c == 20))\n",
        "            >> mask(X.steady_pr > 0)\n",
        "    )\n",
        "    dat['a_c'] = pd.Categorical(dat['a_c'])  # Convert 'a_c' to a categorical variable for discrete scaling\n",
        "    pt = (\n",
        "        ggplot(dat, aes(\"s_w_idx\", \"s_p_idx\", label = \"a_c\", color=\"optimal_a\", size = 'n_s'))\n",
        "        + geom_point()\n",
        "        + geom_text(size=5, color = \"black\")\n",
        "        + facet_wrap(\"~ s_l + s_c\", labeller=\"label_both\")\n",
        "        # + facet_grid(rows = \"s_l\", cols = \"s_c\", labeller=\"label_both\")\n",
        "        # + theme(legend_position='none')\n",
        "        + labs(title = f\"Policy MDP\", x = \"Wind speed (m/s)\", y = \"Price (EUR/MWh)\")\n",
        "        + theme(figure_size=(20,15), axis_text_x=element_text(rotation=90, hjust=1), legend_position=\"bottom\")\n",
        "        # + guides(color=guide_legend(nrow=2, byrow=True))\n",
        "        # + guides(color=False)\n",
        "        + scale_color_discrete()\n",
        "        + scale_x_continuous(breaks=dat['s_w_idx'].unique(), labels=dat['s_w_str'].unique())\n",
        "        + scale_y_continuous(breaks=dat['s_p_idx'].unique(), labels=dat['s_p_str'].unique())\n",
        "    )\n",
        "    pt.show()\n",
        "get_plot()"
      ],
      "metadata": {
        "id": "iDZAejIdOU9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q10\n",
        "\n",
        "Comment on the plot."
      ],
      "metadata": {
        "id": "kk6m7SD4rwoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "Your comments here.\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "eBy0MRh2mLmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q11\n",
        "\n",
        "Experiment with the Q-learning algorithm. Try to find a better approximation by choosing good initial q-values, epsilon and alpha (you may also try a decreasing step-size - see the documentation for the method).\n",
        "\n",
        "Discuss your results.\n"
      ],
      "metadata": {
        "id": "YaeT72OYPFn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "Your comments here.\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "O1DBIr3dPFn-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}