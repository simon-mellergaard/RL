{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simon-mellergaard/RL/blob/main/project1_mdp_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions\n",
        "\n",
        "This notebook contains the first project assignment. You work on the project in groups. You should already have formed a group. If not, do it on Brightspace under *My Courses > Groups*. To get started, do the following steps:\n",
        "\n",
        "1) One student in the group makes a copy of this notebook.\n",
        "2) Share it with your group using *Share* top-right. Here, add group members under people with access.\n",
        "3) Moreover, under *Share > General access*, choose *Anyone with the link > Commenter*. Copy this link and paste it below.\n",
        "4) Work with the notebook and solve the tasks.\n",
        "5) Hand in the notebook by downloading it: File > Download > Download .ipynb. Next, on BS under *Project*, upload the file (I need that for the exam). Moreover, add the shared link of your Colab notebook as a link when you handin too.\n",
        "6) After hand-in do peer grading (see BS under *Project*)\n",
        "\n",
        "Sharing link: [add your link]\n",
        "\n",
        "**Deadlines**\n",
        "\n",
        "* Group signup 29/09/25\n",
        "* Hand-in solution 5/10/25\n",
        "* Peer grading 10/10/25\n"
      ],
      "metadata": {
        "id": "tHx3pQBrT8ob"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlrET7d3WOQ2"
      },
      "source": [
        "# Notebook pre steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0vEJAmyWKPb"
      },
      "outputs": [],
      "source": [
        "#@title Installations\n",
        "\n",
        "# ALWAYS SAVE YOUR OWN COPY OF THIS NOTEBOOK: File > Save a copy in Drive\n",
        "# IF DANISH MENU DO: HjÃ¦lp > Engelsk version\n",
        "\n",
        "# To clear output do: Edit > Clear all outputs\n",
        "\n",
        "## Useful shortscuts\n",
        "# Run current cell: Cmd+Enter\n",
        "# Run current cell and goto next: Shift+Enter\n",
        "# Run selection: Cmd+Shift+Enter\n",
        "\n",
        "# install missing packages\n",
        "!pip install pymdptoolbox\n",
        "!pip install dfply\n",
        "!pip install meteostat\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "from plotnine import *\n",
        "import joblib\n",
        "import gzip\n",
        "import gdown\n",
        "from meteostat import Hourly, Stations\n",
        "from datetime import datetime\n",
        "# from sklearn.ensemble import GradientBoostingRegressor\n",
        "import warnings\n",
        "from scipy.stats import norm\n",
        "from dfply import *\n",
        "# from typing import Tuple\n",
        "# from scipy.sparse import csr_matrix\n",
        "import itertools\n",
        "from IPython.display import Markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRfJ8WsMofFf"
      },
      "outputs": [],
      "source": [
        "#@title MDP class\n",
        "\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class MDP:\n",
        "    \"\"\"\n",
        "    A class representing a Markov Decision Process (MDP) using defaultdict structures.\n",
        "\n",
        "    This implementation includes state management, action specification, transition\n",
        "    probabilities, rewards, policies, and iterative algorithms for policy and value iteration.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes an empty MDP with model and state values.\n",
        "        \"\"\"\n",
        "        self.model = defaultdict(lambda: {\"pi\": None, \"actions\": defaultdict(dict)})\n",
        "        self.v = defaultdict(float)\n",
        "\n",
        "    def add_state_space(self, states):\n",
        "        \"\"\"\n",
        "        Adds states to the MDP.\n",
        "\n",
        "        Args:\n",
        "            states (list): List of state identifiers (strings or convertible to strings).\n",
        "        \"\"\"\n",
        "        for state in states:\n",
        "            _ = self.model[str(state)]\n",
        "        self.set_state_value()\n",
        "\n",
        "    def add_action_space(self, state_str, actions):\n",
        "        \"\"\"\n",
        "        Adds actions to a given state. Note you have to update the action\n",
        "        afterwards using `add_action`.\n",
        "\n",
        "        Args:\n",
        "            state_str (str): The state identifier.\n",
        "            actions (list): List of action identifiers.\n",
        "        \"\"\"\n",
        "        if not isinstance(state_str, str):\n",
        "            raise ValueError(\"State is not a sting!\")\n",
        "        if isinstance(actions, str):\n",
        "            # If it's a string, put it in a list to treat it as a single item\n",
        "            actions = [actions]\n",
        "        for action in actions:\n",
        "            # Initialize the action dictionary with 'pr' and 'r' keys\n",
        "            self.model[state_str][\"actions\"][str(action)] = {\"pr\": {}, \"r\": None}\n",
        "\n",
        "    def add_action(self, state_str, action_str, reward, pr):\n",
        "        \"\"\"\n",
        "        Adds a transition action with reward and transition probabilities.\n",
        "\n",
        "        Args:\n",
        "            state_str (str): State from which the action is taken.\n",
        "            action_str (str): Action identifier.\n",
        "            reward (float): Expected reward for taking the action.\n",
        "            pr (dict): Transition probabilities as {next_state: probability}.\n",
        "        \"\"\"\n",
        "        ## remove keys with zero trans pr\n",
        "        keys_to_remove = [key for key, value in pr.items() if value == 0]\n",
        "        for key in keys_to_remove:\n",
        "            del pr[key]\n",
        "        self.model[state_str][\"actions\"][action_str] = {\"r\": reward, \"pr\": pr}\n",
        "\n",
        "    def check(self, delta = 10*np.spacing(np.float64(1))):\n",
        "        \"\"\"\n",
        "        Performs checks on the built MDP model.\n",
        "\n",
        "        Verifies that transition probabilities sum to approximately 1.0 for each\n",
        "        state-action pair and checks for rewards less than the high_neg_reward.\n",
        "        Prints warnings if any issues are found.\n",
        "\n",
        "        Args:\n",
        "            delta (float, optional): Tolerance for the sum of transition probabilities. Defaults to 1e-10.\n",
        "        \"\"\"\n",
        "        ok = True\n",
        "        # Check if transition pr of an action sum to one\n",
        "        for state_label, state_content in self.model.items():\n",
        "            for action_label, action_content in state_content[\"actions\"].items():\n",
        "                pr = action_content[\"pr\"]\n",
        "                pr_sum = np.sum(list(pr.values()))\n",
        "                absdiff = np.abs(1-pr_sum)\n",
        "                if absdiff >= delta:\n",
        "                    print(f\"Warning: Transition probabilities for action '{action_label}' in state '{state_label}' do not sum to 1.0. Diff is: {absdiff}\")\n",
        "                    ok = False\n",
        "\n",
        "        # Check if there are states with no actions\n",
        "        for state_label, state_content in self.model.items():\n",
        "            if len(state_content[\"actions\"]) == 0:\n",
        "                print(f\"Warning: State '{state_label}' has no actions.\")\n",
        "                ok = False\n",
        "\n",
        "        # Check if all action transitions are to a state\n",
        "        states = list(self.model.keys())\n",
        "        for state_label, state_content in self.model.items():\n",
        "            for action_label, action_content in state_content[\"actions\"].items():\n",
        "                if not all(key in self.model for key in action_content['pr'].keys()):\n",
        "                    print(f\"Warning: Action '{action_label}' in state '{state_label}' has a transition to a non-existing state.\")\n",
        "                    ok = False\n",
        "        if ok:\n",
        "            print(\"All checks passed!\")\n",
        "\n",
        "\n",
        "    def normalize(self):\n",
        "        \"\"\"\n",
        "        Normalizes the transition probabilities for each state-action pair.\n",
        "        \"\"\"\n",
        "        for state_label, state_content in self.model.items():\n",
        "            for action_label, action_content in state_content[\"actions\"].items():\n",
        "                pr = action_content[\"pr\"]\n",
        "                pr_sum = np.sum(list(pr.values()))\n",
        "                for next_state_label, prob in pr.items():\n",
        "                    pr[next_state_label] = prob / pr_sum\n",
        "                action_content[\"pr\"] = pr\n",
        "\n",
        "    def set_state_value(self, states=None, value=0):\n",
        "        \"\"\"\n",
        "        Initializes or updates the value of states.\n",
        "\n",
        "        Args:\n",
        "            states (list, optional): List of state identifiers. Defaults to all states.\n",
        "            value (float, optional): Value to assign. Defaults to 0.\n",
        "        \"\"\"\n",
        "        states = states or list(self.model.keys())\n",
        "        for state in states:\n",
        "            self.v[state] = value\n",
        "\n",
        "    def set_random_deterministic_policy(self):\n",
        "        \"\"\"\n",
        "        Sets a random deterministic policy for each state.\n",
        "        \"\"\"\n",
        "        for state in self.model:\n",
        "            actions = list(self.model[state][\"actions\"].keys())\n",
        "            if actions:\n",
        "                chosen_action = random.choice(actions)\n",
        "                self.model[state][\"pi\"] = {chosen_action: 1}\n",
        "\n",
        "    def set_deterministic_policy(self, state_actions):\n",
        "        \"\"\"\n",
        "        Sets a deterministic policy from a state-action mapping.\n",
        "\n",
        "        Args:\n",
        "            state_actions (dict): Mapping {state: action}.\n",
        "        \"\"\"\n",
        "        for state, action in state_actions.items():\n",
        "            self.model[state][\"pi\"] = {action: 1}\n",
        "\n",
        "    def set_policy(self, states, pi):\n",
        "        \"\"\"\n",
        "        Sets a stochastic or deterministic policy for a list of states.\n",
        "\n",
        "        Args:\n",
        "            states (list): List of states to assign the policy.\n",
        "            pi (dict): Policy as {action: probability}.\n",
        "        \"\"\"\n",
        "        for state in states:\n",
        "            self.model[state][\"pi\"] = pi.copy()\n",
        "\n",
        "    def get_state_keys(self):\n",
        "        \"\"\"\n",
        "        Returns the list of state identifiers.\n",
        "\n",
        "        Returns:\n",
        "            list: List of state keys.\n",
        "        \"\"\"\n",
        "        return list(self.model.keys())\n",
        "\n",
        "    def get_action_keys(self, state):\n",
        "        \"\"\"\n",
        "        Returns the action identifiers for a given state.\n",
        "\n",
        "        Args:\n",
        "            state (str): State identifier.\n",
        "\n",
        "        Returns:\n",
        "            list: List of action keys.\n",
        "        \"\"\"\n",
        "        return list(self.model[state][\"actions\"].keys())\n",
        "\n",
        "    def get_action_info(self, state):\n",
        "        \"\"\"\n",
        "        Gets reward and transition probabilities for each action in a state.\n",
        "\n",
        "        Args:\n",
        "            state (str): State identifier.\n",
        "\n",
        "        Returns:\n",
        "            dict: Action information.\n",
        "        \"\"\"\n",
        "        return dict(self.model[state][\"actions\"])\n",
        "\n",
        "    def get_reward(self, state, action):\n",
        "        \"\"\"\n",
        "        Returns the reward for a given state-action pair.\n",
        "\n",
        "        Args:\n",
        "            state (str): State identifier.\n",
        "            action (str): Action identifier.\n",
        "\n",
        "        Returns:\n",
        "            float: Reward value.\n",
        "        \"\"\"\n",
        "        return self.model[state][\"actions\"][action][\"r\"]\n",
        "\n",
        "    def get_mdp_info(self):\n",
        "        \"\"\"\n",
        "        Returns information about the MDP.\n",
        "\n",
        "        Returns:\n",
        "            dict: The info.\n",
        "        \"\"\"\n",
        "        res = {}\n",
        "        res[\"states\"] = len(self.model)\n",
        "        res[\"actions\"] = sum([len(self.model[k][\"actions\"]) for k in self.model.keys()])\n",
        "        return res\n",
        "\n",
        "    def get_policy(self, add_state_values = False):\n",
        "        \"\"\"\n",
        "        Retrieves the current policy.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Table of state, action, and probability.\n",
        "        \"\"\"\n",
        "        policy = []\n",
        "        for state in self.get_state_keys():\n",
        "            for action, prob in self.model[state][\"pi\"].items():\n",
        "                if not add_state_values:\n",
        "                    policy.append({\"state\": state, \"action\": action, \"pr\": prob})\n",
        "                else:\n",
        "                    policy.append({\"state\": state, \"action\": action, \"pr\": prob, \"v\": self.v[state]})\n",
        "        df = pd.DataFrame(policy)\n",
        "        df.set_index(\"state\")\n",
        "        return df\n",
        "\n",
        "    def get_state_values(self, states=None):\n",
        "        \"\"\"\n",
        "        Returns the current value of each state.\n",
        "\n",
        "        Args:\n",
        "            states (list, optional): Subset of states. Defaults to all.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Table of state values.\n",
        "        \"\"\"\n",
        "        states = states or list(self.v.keys())\n",
        "        return pd.DataFrame([{\"state\": s, \"v\": self.v[s]} for s in states])\n",
        "\n",
        "    def get_mdp_matrices(self, high_neg_reward = -100000):\n",
        "        \"\"\"\n",
        "        Returns transition probability and reward matrices.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                p_mat (list): List of transition probability matrices.\n",
        "                r_mat (ndarray): Reward matrix.\n",
        "                states (list): List of state identifiers.\n",
        "                actions (list): List of action identifiers.\n",
        "        \"\"\"\n",
        "        states = self.get_state_keys()\n",
        "        actions = set(\n",
        "            action for state in states for action in self.get_action_keys(state)\n",
        "        )\n",
        "        actions = list(actions)\n",
        "        actions.sort()\n",
        "        p_mat = [pd.DataFrame(0.0, index=states, columns=states) for _ in actions]\n",
        "        for df in p_mat:\n",
        "            np.fill_diagonal(df.values, 1) # set default to transition to same state (so illigal actions work)\n",
        "        r_mat = pd.DataFrame(high_neg_reward, index=states, columns=actions)\n",
        "        for state in states:\n",
        "            for action in self.get_action_keys(state):\n",
        "                p_mat[actions.index(action)].at[state, state] = 0  # reset to 0 again (since action is not illigal)\n",
        "                pr = self.model[state][\"actions\"][action][\"pr\"]\n",
        "                r = self.model[state][\"actions\"][action][\"r\"]\n",
        "                r_mat.at[state, action] = r\n",
        "                for next_state, prob in pr.items():\n",
        "                    p_mat[actions.index(action)].at[state, next_state] = prob\n",
        "        p_mat = [m.to_numpy() for m in p_mat]  # convert to matrices\n",
        "        r_mat = r_mat.to_numpy()\n",
        "        return p_mat, r_mat, states, actions\n",
        "\n",
        "    def bellman_calc(self, gamma, state, action):\n",
        "        \"\"\"\n",
        "        Computes Bellman update for a given state-action pair.\n",
        "\n",
        "        Args:\n",
        "            gamma (float): Discount factor.\n",
        "            state (str): State identifier.\n",
        "            action (str): Action identifier.\n",
        "\n",
        "        Returns:\n",
        "            float: Updated value.\n",
        "        \"\"\"\n",
        "        pr = self.model[state][\"actions\"][action][\"pr\"]\n",
        "        reward = self.model[state][\"actions\"][action][\"r\"]\n",
        "        return reward + gamma * sum(pr[s] * self.v[s] for s in pr)\n",
        "\n",
        "    def policy_eval(self, gamma, theta=1e-5, max_iter=10000, reset=True):\n",
        "        \"\"\"\n",
        "        Iteratively evaluates the current policy.\n",
        "\n",
        "        Args:\n",
        "            gamma (float): Discount factor.\n",
        "            theta (float): Convergence threshold.\n",
        "            max_iter (int): Max iterations.\n",
        "            reset (bool): Whether to reset state values to 0.\n",
        "        \"\"\"\n",
        "        if reset:\n",
        "            self.set_state_value()\n",
        "        for _ in range(max_iter):\n",
        "            delta = 0\n",
        "            for state in self.model:\n",
        "                v_old = self.v[state]\n",
        "                pi = self.model[state][\"pi\"]\n",
        "                value = sum(pi[a] * self.bellman_calc(gamma, state, a) for a in pi)\n",
        "                self.v[state] = value\n",
        "                delta = max(delta, abs(v_old - value))\n",
        "            if delta < theta:\n",
        "                break\n",
        "        else:\n",
        "            print(f\"Policy evaluation stopped at max iterations: {max_iter}\")\n",
        "\n",
        "    def policy_iteration(self, gamma, theta=1e-5, max_iter_eval=10000, max_iter_policy=100):\n",
        "        \"\"\"\n",
        "        Performs policy iteration with evaluation and improvement steps.\n",
        "\n",
        "        Args:\n",
        "            gamma (float): Discount factor.\n",
        "            theta (float): Convergence threshold.\n",
        "            max_iter_eval (int): Max iterations during policy evaluation.\n",
        "            max_iter_policy (int): Max policy improvement steps.\n",
        "        \"\"\"\n",
        "        self.set_random_deterministic_policy()\n",
        "        for i in range(max_iter_policy):\n",
        "            self.policy_eval(gamma, theta, max_iter_eval, reset=False)\n",
        "            stable = True\n",
        "            for state in self.model:\n",
        "                old_action = next(iter(self.model[state][\"pi\"]))\n",
        "                best_val, best_action = float(\"-inf\"), None\n",
        "                for action in self.get_action_keys(state):\n",
        "                    val = self.bellman_calc(gamma, state, action)\n",
        "                    if val > best_val:\n",
        "                        best_val = val\n",
        "                        best_action = action\n",
        "                self.model[state][\"pi\"] = {best_action: 1}\n",
        "                if best_action != old_action:\n",
        "                    stable = False\n",
        "            if stable:\n",
        "                break\n",
        "        else:\n",
        "            print(f\"Policy iteration stopped at max iterations: {max_iter_policy}\")\n",
        "        print(f\"Policy iteration finished in {i + 1} iterations.\")\n",
        "\n",
        "    def value_iteration(self, gamma, theta=1e-5, max_iter=10000, reset=True):\n",
        "        \"\"\"\n",
        "        Performs value iteration algorithm.\n",
        "\n",
        "        Args:\n",
        "            gamma (float): Discount factor.\n",
        "            theta (float): Convergence threshold.\n",
        "            max_iter (int): Max number of iterations.\n",
        "            reset (bool): Whether to reinitialize state values.\n",
        "        \"\"\"\n",
        "        self.set_random_deterministic_policy()\n",
        "        if reset:\n",
        "            self.set_state_value()\n",
        "        for i in range(max_iter):\n",
        "            delta = 0\n",
        "            for state in self.model:\n",
        "                v_old = self.v[state]\n",
        "                best_val, best_action = float(\"-inf\"), None\n",
        "                for action in self.get_action_keys(state):\n",
        "                    val = self.bellman_calc(gamma, state, action)\n",
        "                    if val > best_val:\n",
        "                        best_val = val\n",
        "                        best_action = action\n",
        "                self.v[state] = best_val\n",
        "                self.model[state][\"pi\"] = {best_action: 1}\n",
        "                delta = max(delta, abs(v_old - best_val))\n",
        "            if delta < theta:\n",
        "                break\n",
        "        else:\n",
        "            print(f\"Value iteration stopped at max iterations: {max_iter}\")\n",
        "        print(f\"Value iteration finished in {i + 1} iterations.\")\n",
        "\n",
        "    def get_steady_state_pr(self, as_dataframe = True, tol=1e-8):\n",
        "        \"\"\"\n",
        "        Calculates the steady-state probabilities for the MDP under the optimal policy.\n",
        "\n",
        "        Args:\n",
        "            as_dataframe (bool): Whether to return the result as a DataFrame, or otherwise as an array.\n",
        "\n",
        "        Returns:\n",
        "            if as_dataframe:\n",
        "                pd.DataFrame: A DataFrame with states and their steady-state probabilities.\n",
        "            else:\n",
        "                ndarray: An array of steady-state probabilities.\n",
        "        \"\"\"\n",
        "        state_labels_to_index = {label: index for index, label in enumerate(self.get_state_keys())}\n",
        "        num_states = len(state_labels_to_index)\n",
        "        transition_matrix = np.zeros((num_states, num_states))\n",
        "        policy = self.get_policy()\n",
        "        policy['s_idx'] = policy['state'].map(state_labels_to_index)\n",
        "        policy = policy.set_index(['s_idx', 'action'])\n",
        "        # calc transition matrix\n",
        "        for s_label in self.get_state_keys():\n",
        "            s_idx = state_labels_to_index[s_label]\n",
        "            action_rows = policy.loc[s_idx]\n",
        "            for action, row in action_rows.iterrows():\n",
        "                pi = row['pr']\n",
        "                a = self.model[s_label]['actions'][action]\n",
        "                for s_next_label, prob in a['pr'].items():\n",
        "                    s_next_idx = state_labels_to_index[s_next_label]\n",
        "                    transition_matrix[s_idx, s_next_idx] += prob * pi\n",
        "\n",
        "        transition_matrix.sum(axis=1)\n",
        "\n",
        "        ## calc steady state pr\n",
        "        # # alternative 1\n",
        "        # eigenvalues, left_eigenvectors = np.linalg.eig(transition_matrix.T)\n",
        "        # # Find the eigenvalue closest to 1\n",
        "        # closest_eigenvalue_index = np.abs(eigenvalues - 1).argmin()\n",
        "        # # Extract the corresponding left eigenvector\n",
        "        # steady_state_vector = left_eigenvectors[:, closest_eigenvalue_index]\n",
        "        # # Ensure the eigenvector contains real values and take the real part\n",
        "        # steady_state_vector = np.real(steady_state_vector)\n",
        "        # # Normalize the vector to sum to 1\n",
        "        # steady_state_vector = steady_state_vector / np.sum(steady_state_vector)\n",
        "        # # Handle potential negative values due to numerical precision by taking absolute value\n",
        "        # steady_state_vector = np.abs(steady_state_vector)\n",
        "        # steady_state_vector = steady_state_vector / np.sum(steady_state_vector)\n",
        "        # # Verify that the sum of the steady-state probabilities is approximately 1\n",
        "        # print(\"Sum of steady-state probabilities:\", np.sum(steady_state_vector))\n",
        "        # # Verify that all probabilities are non-negative\n",
        "        # print(\"Minimum steady-state probability:\", np.min(steady_state_vector))\n",
        "        # steady = steady_state_vector\n",
        "\n",
        "        # Alternative 2\n",
        "        eigvals, eigvecs = np.linalg.eig(transition_matrix.T)\n",
        "        steady = np.real(eigvecs[:, np.isclose(eigvals, 1)])\n",
        "        steady = steady[:,0]\n",
        "        sum(steady)\n",
        "        steady = steady/steady.sum()\n",
        "\n",
        "        # # Alternative 3 (slow)\n",
        "        # # Solve (P^T - I) d^T = 0 with sum(d)=1 by replacing one equation with the normalization\n",
        "        # A = transition_matrix.T - np.eye(num_states)\n",
        "        # b = np.zeros(num_states)\n",
        "        # A[-1, :] = 1.0\n",
        "        # b[-1] = 1.0\n",
        "        # # Least-squares for robustness\n",
        "        # d, *_ = np.linalg.lstsq(A, b, rcond=None)\n",
        "        # # Clean numerical noise\n",
        "        # d = np.maximum(d, 0)\n",
        "        # d = d / d.sum()\n",
        "\n",
        "        # abs(steady - steady_state_vector) < 0.00000001\n",
        "        # abs(d - steady_state_vector) < 0.00000001\n",
        "        # abs(steady - d) < 0.00000001\n",
        "\n",
        "        if abs(sum(steady) - 1) > tol:\n",
        "            raise ValueError(\"Steady state probabilities do not sum to 1.\")\n",
        "\n",
        "        if as_dataframe:\n",
        "            policy.reset_index(inplace=True)\n",
        "            policy['steady_pr'] = [steady[s_idx] for s_idx in policy['s_idx']]\n",
        "            return policy\n",
        "        else:\n",
        "            return steady\n",
        "\n",
        "# self = mdp\n",
        "# mdp.get_mdp_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoUjhwzrUznn"
      },
      "source": [
        "# Hour-to-hour electricity market problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2appdU9ECTa"
      },
      "source": [
        "We consider an energy producer operating on an hour-to-hour electricity\n",
        "market (external power grid), which has a wind power plant and a battery available.\n",
        "\n",
        "The producer can every hour.\n",
        "\n",
        "- generate energy from the wind power plant or the battery\n",
        "- store energy by charging the battery.\n",
        "\n",
        "Given the current hour, the producer has to fulfil a given commitment on a production level to the external grid. Since the generated power from wind is uncertain the producer may not be able to meet the commitment by wind production. Thus, it may be a good idea to have some level of energy stored on the battery. If the producer does not manage to meet the commitment, it is faced\n",
        "with a penalty cost which may depend on the level of deviation to commitment. Deviation might occur if wind speed is less than expected or if there is a shortage of required energy in the battery.\n",
        "\n",
        "Although the circumstances on the market might make it favourable the deviate from the commitments, we will assume that this is not allowed. Consequently, the producer has to utilize its available energy sources to meet commitments as close as possible.\n",
        "\n",
        "The producer also has to choose a commitment level for the forthcomming hour. The producers choice of commitment level depends on the level of energy storaged on the battery, the level of possible wind energy production from the plant and the electricity spot prices.\n",
        "\n",
        "The sequential decision problem is to find a plan for choosing commitment levels that maximizes the expected profit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqz24EQuJ2Rn"
      },
      "source": [
        "## Problem parameters and relations\n",
        "\n",
        "Let us assume the following parameters:\n",
        "\n",
        "- $b^\\max$ Maximum capacity of the battery (MWh).\n",
        "- $p^{\\max}$ Maximum power of the wind turbine (MW).\n",
        "- $c^+$ Imbalance unit penalty cost when the electricity price is positive (EUR/MWh).\n",
        "- $c^-$ Imbalance unit penalty cost when the electricity price is negative (EUR/MWh).\n",
        "- Given wind speed $x$ the energy produced within a time period is $E(x)$  (MWh)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39Z7BysDjLBU"
      },
      "source": [
        "## State and action space\n",
        "\n",
        "We can model the sequential decision problem as an MDP. First, let us define the state space for a given period $t$. Let\n",
        "\n",
        "- $s^l_t$ be the level of energy storaged on the battery at period $t$ (MWh).\n",
        "- $s^c_t$ be the commitment level for period $t$ (MWh). Note if negative we buy energy from the grid.\n",
        "- $s^w_t$ is the expected wind speed (m/s),\n",
        "- $s^p_t$ is the electricity spot prices in period $t$ (EUR). Note may be negative.\n",
        "\n",
        "<!-- Then the state space of the MDP is $S = L \\times C \\times W \\times P$. -->\n",
        "Note that\n",
        "\n",
        "- $s^l_t$ and $s^c_t$ are endogenous variables. That is, the transition over time depends on the actions of the producer.\n",
        "- $s^w_t$ and $s^p_t$ are exogenous variables, i.e. they are governed by dynamics that do not depend on the endogenous variables.\n",
        "\n",
        "Next, consider the set of possible actions. Let\n",
        "\n",
        "- $a^c_t$ be the chosen commitment level for period $t+1$ (MWh).\n",
        "- $a^b_t$ is chosen level of energy discharged from the battery in period $t$ to the grid (MWh). Note if negative we choose to charge the battery.\n",
        "- $a^w_t$ is the chosen amount of generated wind energy from the wind power plant (MWh).\n",
        "\n",
        "<!-- Then $A=C \\times B \\times W$ is the action space. -->\n",
        "\n",
        "<!-- Assume that every variable and action is a real number taking only values on a bounded interval of the real line. -->\n",
        "\n",
        "### State transitions\n",
        "\n",
        "The dynamics of the stochastic processes are assumed to be known (the transitions probabilities). First, since $s^c_t$ is the current commitment level then for the next period, we have $$s^c_{t+1}=a^c_t.$$\n",
        "\n",
        "Next, the level of the battery is $$s^l_{t+1}=s^l_t - a^b_t.$$\n",
        "\n",
        "Finally, the exogenous variables are goverend by two stochastic functions\n",
        "\n",
        "$$s^w_{t+1}=W(s^w_t),$$\n",
        "\n",
        "$$s^p_{t+1}=G(s^p_t).$$\n",
        "\n",
        "<!-- That is the the level of wind in period $t+1$ only depends on the level of wind in period $t$, and the dynamics behave according to a Markovian transition kernel.  -->\n",
        "\n",
        "### Predetermined actions\n",
        "\n",
        "We assume that the producer has to utilise its available energy sources to meet commitments as closely as possible. Hence, the producer must follow the rules:\n",
        "\n",
        "1. First, fulfil commitment as much as possible from wind.\n",
        "2. Next, if any excess energy from wind charge the battery.\n",
        "3. Otherwise, try to filfil the remaining commitment by discharging the battery.\n",
        "\n",
        "For simplicity, let us assume no efficiency loss when energy is transfered.\n",
        "\n",
        "Hence, after observing the state variables $s^l_t,s^c_t,s^w_t$ and $s^p_t$, the actions $a^w_t$ and $a^b_t$ are given immediately from the state. That is, both actions are found given the state and should not be optimized. However, the optimal commitment level $a^c_t$ should be optimized. Below we give descriptions of $a^w_t$ and $a^b_t$.\n",
        "\n",
        "**If commitment is positive**\n",
        "\n",
        "Then we send energy to the grid and may use the battery.\n",
        "\n",
        "- If the commitment is greater than or equal to the possible energy from wind, then use all the wind\n",
        "\n",
        "  $$a^w_t = E(s^w_t) \\text{ if } s^c_t \\geq 0, s^c_t \\geq E(s^w_t).$$\n",
        "\n",
        "  Moreover, we discharge the battery, either fully or with the remaining energy needed ($s^c_t-E(s^w_t)$)\n",
        "\n",
        "$$a^b_t = \\min\\{s^l_t, s^c_t-E(s^w_t) \\} \\text{ if } s^c_t \\geq 0, s^c_t \\geq E(s^w_t).$$\n",
        "\n",
        "- If the commitment is less than the possible energy from wind. Then we charge the battery, which can receive either remaining energy $E(s^w_t)-s^c_t$, or the energy needed to fully charge the battery $b^\\max-s^l_t$. That is,\n",
        "\n",
        "  $$a^b_t = - \\min\\{b^\\max-s^l_t, E(s^w_t)-s^c_t\\}, \\text{ if } s^c_t \\geq 0, s^c_t < E(s^w_t),$$\n",
        "\n",
        "  and the amount of wind used is\n",
        "\n",
        "  $$a^w_t = s^c_t - a^b_t, \\text{ if } s^c_t \\geq 0, s^c_t < E(s^w_t).$$\n",
        "\n",
        "**If commitment is negative**\n",
        "\n",
        "Then we transfer from the grid and we charge the battery with energy from the grid (as much as possible) and use wind to add more energy to the battery if possible. That is, we assume that we first use energy from the grid (so do not to get a penalty) and then wind.\n",
        "\n",
        "$$a^b_t = - \\min\\{b^\\max-s^l_t, - s^c_t + E(s^w_t)\\} \\text{ if } s^c_t < 0$$\n",
        "\n",
        "$$a^w_t = \\max\\{0, - a^b_t + s^c_t\\} \\text{ if } s^c_t < 0$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "na6KGp1x21h1"
      },
      "source": [
        "## Reward\n",
        "\n",
        "The the amount of electricity delivered to the grid from the producer in period $t$ is $e_t = a^b_t + a^w_t$. The reward by being in state $s = (s^l_t,s^c_t,s^w_t,s^p_t)$ and chosing action $a = (a^c_t, a^b_t, a^w_t)$ is then\n",
        "\n",
        "\\begin{align}\n",
        "    R(s,a) &= R(s^l_t,s^c_t,s^w_t,s^p_t, a^b_t, a^b_t, a^w_t) \\\\\n",
        "    &= R(s^c_t, s^p_t, e_t) \\\\\n",
        "    &= \\begin{cases}\n",
        "        s^c_ts^p_t - c^+ |s^c_t-e_t| &\\text{if }s^p_t \\geq 0\\\\\n",
        "        s^c_ts^p_t - c^- |s^c_t-e_t| &\\text{if }s^p_t<0\n",
        "       \\end{cases}\n",
        "\\end{align}\n",
        "\n",
        "where $c^+$ and $c^-$ are imbalance prices, i.e. a penalty price if the producer does not meet its commitments to the market.\n",
        "\n",
        "For each period $t$, the producer first observes the state variables $(s^l_t,s^c_t,s^w_t,s^p_t)$ and then decides on a commitment level for the next period $a^c_t$ and an amount of energy actually to deliver to the market $e_t$. Finally, the reward $R(s^c_t,s^p_t,e_t)$ is received."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knuddBIKWIie"
      },
      "source": [
        "## Transition probabilities\n",
        "\n",
        "The transition probabilities can be split since the stochastic processes are independent:\n",
        "\n",
        "\\begin{align}\n",
        "    p(s_{t+1} | s_t, a_t) &= p( s^l_{t+1},s^c_{t+1},s^w_{t+1},s^p_{t+1} | s^l_t,s^c_t,s^w_t,s^p_t, a^c_t, a^b_t, a^w_t) \\\\\n",
        "        &= p(s^w_{t+1} | s^w_t)\n",
        "           p(s^p_{t+1} | s^p_t)\n",
        "           p( s^l_{t+1} | s^l_t, a^c_t, a^b_t, a^w_t)\n",
        "           p( s^c_{t+1} | s^c_t, a^c_t, a^b_t, a^w_t) \\\\\n",
        "        &= p(s^w_{t+1} | s^w_t)\n",
        "           p(s^p_{t+1} | s^p_t)\n",
        "           p( s^l_{t+1} | s^l_t, a^b_t)\n",
        "           p( s^c_{t+1} | a^c_t) \\\\\n",
        "\\end{align}\n",
        "\n",
        "Here $p(s^w_{t+1} | s^w_t)$ and $p(s^p_{t+1} | s^p_t)$ are calculated using the distributions for $W$ and $G$, respectively. Transitions for $s^l_{t}$ and $s^c_{t}$ are deterministic. That is, $p( s^l_{t+1} | s^l_t, a^b_t) = 1$ if $s^l_{t+1} = s^l_t - a^b_t$ and zero otherwise. Similar $p( s^c_{t+1} | a^c_t) = 1$ if $s^c_{t+1} = a^c_t$ and zero otherwise. Note that, no probabilities depend on $a^w_t$ and hence not needed to calculate the transition probabilities. Hence\n",
        "\n",
        "\\begin{align}\n",
        "    p(s_{t+1} | s_t, a_t) = p(s^w_{t+1} | s^w_t)\n",
        "           p(s^p_{t+1} | s^p_t)\n",
        "\\end{align}\n",
        "\n",
        "if $s^l_{t+1} = s^l_t - a^b_t$ and $s^c_{t+1} = a^c_t$ and zero otherwise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0us7vIXQ4Op"
      },
      "source": [
        "## Function $E(x)$\n",
        "\n",
        "To calculate the energy used during af tieme period, first recall that the unit megawatt (MW) is a measure of power, not energy. Power refers to the rate at which energy is produced or consumed, while energy is the total amount of power used or generated over time. Here we measure power in MW and energy in MWh.\n",
        "Given the expected wind speed (m/s), the expected maximal amount of power (MW) that the wind power plant can generate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhC-YVcnK0P6"
      },
      "outputs": [],
      "source": [
        "def power(wind_speed, p_max=10, w_cut_in=3, w_rated=12, w_cut_out=25):\n",
        "    \"\"\"\n",
        "    The power output of a wind turbine given a wind speed.\n",
        "\n",
        "    Parameters:\n",
        "    - wind_speed (float): The wind speed (in meters per second, m/s).\n",
        "    - p_max (float, optional): The rated power of the wind turbine (in megawatts, MW). Default is 10 MW.\n",
        "    - w_cut_in (float, optional): The cut-in wind speed (in meters per second, m/s). Default is 3 m/s.\n",
        "    - w_rated (float, optional): The rated wind speed (in meters per second, m/s). Default is 12 m/s.\n",
        "    - w_cut_out (float, optional): The cut-out wind speed (in meters per second, m/s). Default is 25 m/s.\n",
        "\n",
        "    Returns:\n",
        "    - float: The estimated power output (in megawatts, MW) based on the given wind speed.\n",
        "    \"\"\"\n",
        "    if wind_speed < w_cut_in:\n",
        "        return 0\n",
        "    elif w_cut_in <= wind_speed <= w_rated:\n",
        "        return p_max * ((wind_speed - w_cut_in) / (w_rated - w_cut_in)) ** 3\n",
        "    elif w_rated < wind_speed <= w_cut_out:\n",
        "        return p_max\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Create a range of wind speeds for plotting\n",
        "wind_speeds = np.linspace(0, 30, 500)\n",
        "\n",
        "# Calculate the corresponding power outputs for the offshore windmill\n",
        "power_outputs_offshore = [power(ws) for ws in wind_speeds]\n",
        "\n",
        "# Create a DataFrame for plotnine\n",
        "df = pd.DataFrame({\n",
        "    'wind_speed': wind_speeds,\n",
        "    'power_output': power_outputs_offshore\n",
        "})\n",
        "\n",
        "# Generate the plot using plotnine\n",
        "w_cut_in_offshore = 3\n",
        "w_rated_offshore = 12\n",
        "w_cut_out_offshore = 25\n",
        "p_max_offshore = 10\n",
        "plot = (ggplot(df, aes(x='wind_speed', y='power_output'))\n",
        "        + geom_line()\n",
        "        + geom_vline(xintercept=w_cut_in_offshore, color='red', linetype='dashed', size=1, show_legend=True)\n",
        "        + geom_vline(xintercept=w_rated_offshore, color='green', linetype='dashed', size=1, show_legend=True)\n",
        "        + geom_vline(xintercept=w_cut_out_offshore, color='orange', linetype='dashed', size=1, show_legend=True)\n",
        "        + labs(title='Offshore Windmill Power Output vs Wind Speed',\n",
        "               x='Wind Speed (m/s)',\n",
        "               y='Power Output (MW)')\n",
        "        + theme_minimal()\n",
        "        + scale_x_continuous(limits=(0, 30))\n",
        "        + scale_y_continuous(limits=(0, p_max_offshore))\n",
        ")\n",
        "plot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lraniuMwMnU8"
      },
      "source": [
        "The power curve of a wind turbine describes how its power output varies with wind speed. Here's how it's created:\n",
        "\n",
        "   * Cut-in Wind Speed: The minimum wind speed at which the turbine starts generating power.\n",
        "   * Rated Wind Speed: The wind speed at which the turbine generates its maximum rated power.\n",
        "   * Cut-out Wind Speed: The wind speed at which the turbine stops generating power for safety reasons (shut down).\n",
        "   * Rated Power: The maximum power the turbine can produce when operating at the rated wind speed.\n",
        "\n",
        "The calculation is as follows:\n",
        "\n",
        "   * Below Cut-in Speed: If the wind speed is below the cut-in speed, the turbine produces no power (0 kW).\n",
        "   * Between Cut-in and Rated Speed: As the wind speed increases, the turbine's power output increases non-linearly. This relationship is often modeled using a cubic function to simulate real-world turbine behavior.\n",
        "   * At Rated Speed: When the wind speed reaches the rated speed, the turbine produces its maximum power and stays at this constant value for higher wind speeds.\n",
        "   * Above Cut-out Speed: If the wind speed exceeds the cut-out speed, the turbine shuts down to prevent damage and produces no power.\n",
        "\n",
        "Note that if the period we consider is one hour, the energy produced is equal to the power times one, i.e. MW equals MWh.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzAtj2tNSaFv"
      },
      "outputs": [],
      "source": [
        "from scipy.optimize import root_scalar\n",
        "\n",
        "def energy(w, time_period_length = 1, p_max=10, w_cut_in=3, w_rated=12, w_cut_out=25):\n",
        "    \"\"\"\n",
        "    The energy output of a wind turbine over one hour given a wind speed (MWh).\n",
        "\n",
        "    Args:\n",
        "    - w (float): The wind speed (in meters per second, m/s).\n",
        "    - time_period_length (float, optional): The length of the time period (in hours). Default is 1 hour.\n",
        "\n",
        "    returns:\n",
        "    - (float) The energy produced.\n",
        "    \"\"\"\n",
        "    return power(w, p_max, w_cut_in, w_rated, w_cut_out) * time_period_length\n",
        "\n",
        "def energy_inverse(energy_target, time_period_length = 1, p_max=10, w_cut_in=3, w_rated=12, w_cut_out=25):\n",
        "    \"\"\"\n",
        "    Find the wind speed that produces a given amount of energy, assuming wind is between w_cut_in and w_rated.\n",
        "\n",
        "    Args:\n",
        "        energy_target (float): The amount of energy to produce.\n",
        "\n",
        "    Returns:\n",
        "        float: The wind speed that produces the specified amount of energy.\n",
        "    \"\"\"\n",
        "    if energy_target < 0 or energy_target > p_max:\n",
        "        return None\n",
        "    # if energy_target <= w_cut_in or energy_target >= w_cut_out:\n",
        "    #     return 0\n",
        "    # if energy_target >= w_rated:\n",
        "    #     return p_max\n",
        "    # Define a function representing the equation energy(w) - target_energy = 0\n",
        "    def find_wind_speed(w):\n",
        "        return energy(w, time_period_length, p_max, w_cut_in, w_rated, w_cut_out) - energy_target\n",
        "\n",
        "    # Use fsolve to find the root (the wind speed)\n",
        "    # Provide an initial guess for the wind speed (e.g., 10 m/s)\n",
        "\n",
        "    sol = root_scalar(find_wind_speed, bracket=[w_cut_in, w_rated], method='brentq')\n",
        "    return sol.root if sol.converged else None\n",
        "\n",
        "    # initial_guess = 5\n",
        "    # wind_speed_at_target_energy = fsolve(find_wind_speed, initial_guess)\n",
        "    # if wind_speed_at_target_energy[0] < w_cut_in:\n",
        "    #     return w_cut_in\n",
        "    # if wind_speed_at_target_energy[0] > w_rated:\n",
        "    #     return w_rated\n",
        "    # return wind_speed_at_target_energy[0]\n",
        "\n",
        "# Testing and verify the result\n",
        "target = 8 # MWh\n",
        "wind = energy_inverse(target)\n",
        "display(Markdown(f\"\"\"\n",
        "The wind speed required to produce {target} MWh of energy is approximately: {wind:.2f} m/s.\\n\n",
        "Energy produced at {wind:.2f} m/s: {energy(wind):.2f} MWh\n",
        "\"\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m2LaiLzHRDQ"
      },
      "source": [
        "## Functions G and W\n",
        "\n",
        "The functions for the endogeneous variables, are based on historical data and estimated using forecasting models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE9OoMAp5obf"
      },
      "source": [
        "### Price function $G(p)$\n",
        "\n",
        "The model is fitted using an AR(1) model and can be loaded using:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4D0VuffWWfp3"
      },
      "outputs": [],
      "source": [
        "# Google Drive direct download URL (using the 'uc?id=' format)\n",
        "url = 'https://drive.google.com/uc?id=1cGhte06iiWZnaRLPyj5D8ZzWX7gqPsIR'\n",
        "\n",
        "# Output filename for the downloaded file\n",
        "output_filename = 'prices_ar1.pkl.gz'\n",
        "\n",
        "# Download the file from Google Drive\n",
        "gdown.download(url, output_filename, quiet=True)\n",
        "\n",
        "# Load the model from the downloaded file\n",
        "with gzip.open(output_filename, \"rb\") as f:\n",
        "    model_price = joblib.load(f)\n",
        "\n",
        "display(Markdown(f\"Model loaded successfully from {output_filename}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyecBXE56GVX"
      },
      "source": [
        "We can now predict the price given hour, day and month:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Hl7kK-aXenE"
      },
      "outputs": [],
      "source": [
        "# Example\n",
        "price = 100\n",
        "phi = model_price.params.iloc[1]      # lag coefficient\n",
        "intercept = model_price.params.iloc[0]    # intercept\n",
        "price_next = intercept + phi * price\n",
        "display(Markdown(f\"\"\"\n",
        "Latest price: {price:.2f} EUR.\\n\n",
        "Predicted (mean) next hour price: {price_next:.2f} EUR\n",
        "\"\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl39AUUHKVzq"
      },
      "source": [
        " Moreover, the probability that the price falls within a given interval given current price is within an interval:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guNsiq92Q8QY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from scipy.integrate import quad\n",
        "\n",
        "def get_price_pr_inv(model, a, b, c, d):\n",
        "    \"\"\"\n",
        "    Compute the conditional probability that P_{t+1} â [a, b] given P_t â [c, d]\n",
        "    for a stationary AR(1) process with intercept.\n",
        "\n",
        "    AR(1) Model:\n",
        "        P_{t+1} = alpha + phi * P_t + epsilon_t\n",
        "        where epsilon_t ~ N(0, sigma^2)\n",
        "\n",
        "    Stationarity:\n",
        "        The AR(1) process is considered stationary if the autoregressive coefficient satisfies:\n",
        "            |phi| < 1\n",
        "\n",
        "        Under stationarity:\n",
        "            - The mean of W_t is constant over time: E[W_t] = mu = alpha / (1 - phi)\n",
        "            - The variance of W_t is constant: Var(W_t) = sigma^2 / (1 - phi^2)\n",
        "            - The process fluctuates around a stable long-term mean rather than trending or diverging\n",
        "\n",
        "    Parameters:\n",
        "        phi (float): AR(1) coefficient (must satisfy |phi| < 1 for stationarity)\n",
        "        alpha (float): Intercept term in the AR(1) model\n",
        "        sigma (float): Standard deviation of the noise term epsilon_t\n",
        "        a (float): Lower bound of target interval for W_{t+1}\n",
        "        b (float): Upper bound of target interval for W_{t+1}\n",
        "        c (float): Lower bound of conditioning interval for W_t\n",
        "        d (float): Upper bound of conditioning interval for W_t\n",
        "\n",
        "    Returns:\n",
        "        float: Conditional probability P(a â¤ W_{t+1} â¤ b | c â¤ W_t â¤ d)\n",
        "    \"\"\"\n",
        "    phi = model.params.iloc[1]      # lag coefficient\n",
        "    alpha = model.params.iloc[0]    # intercept\n",
        "    sigma = model.resid.std()   # residual standard deviation\n",
        "\n",
        "    # Long-run mean and stationary variance\n",
        "    mu = alpha / (1 - phi)\n",
        "    sigma_w = sigma / np.sqrt(1 - phi**2)\n",
        "\n",
        "    def marginal_pdf_w(w):\n",
        "        \"\"\"PDF of stationary W_t ~ N(mu, sigma_w^2).\"\"\"\n",
        "        return norm.pdf(w, loc=mu, scale=sigma_w)\n",
        "\n",
        "    def integrand(w):\n",
        "        \"\"\"Integrand: conditional probability * marginal density.\"\"\"\n",
        "        mean_t1 = alpha + phi * w\n",
        "        p = norm.cdf(b, loc=mean_t1, scale=sigma) - norm.cdf(a, loc=mean_t1, scale=sigma)\n",
        "        return p * marginal_pdf_w(w)\n",
        "\n",
        "    # Marginal probability P(c â¤ W_t â¤ d)\n",
        "    p_wt_interval = norm.cdf(d, loc=mu, scale=sigma_w) - norm.cdf(c, loc=mu, scale=sigma_w)\n",
        "\n",
        "    # Numerical integration over W_t in [c, d]\n",
        "    integral_result, _ = quad(integrand, c, d)\n",
        "\n",
        "    return integral_result / p_wt_interval\n",
        "\n",
        "display(Markdown(f\"\"\"\n",
        "Prob. of P_t+1 â [0, 8] given P_t â [3, 4]:\n",
        "{get_price_pr_inv(model_price, 0, 8, 3, 4):.4f}\n",
        "\"\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdVSnlSZNI7D"
      },
      "source": [
        "### Wind function $W(x)$\n",
        "\n",
        "The model is fitted using AR(1) on log(wind speed) (since cannot be negative) and can be loaded using:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-V3C8sqiNTUm"
      },
      "outputs": [],
      "source": [
        "# Google Drive direct download URL (using the 'uc?id=' format)\n",
        "url = 'https://drive.google.com/uc?id=1TJ1ACzev40QbeUlXBbDicYU3kEyiH1nB'\n",
        "\n",
        "# Output filename for the downloaded file\n",
        "output_filename = 'wind_log_ar1.pkl.gz'\n",
        "\n",
        "# Download the file from Google Drive\n",
        "gdown.download(url, output_filename, quiet=True)\n",
        "\n",
        "# Load the model from the downloaded file\n",
        "with gzip.open(output_filename, \"rb\") as f:\n",
        "    model_wind = joblib.load(f)\n",
        "\n",
        "print(f\"Model loaded successfully from {output_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Qk73f0-NTUm"
      },
      "source": [
        "We can now predict the price given hour, day and month:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rX4N-Tr7NTUm"
      },
      "outputs": [],
      "source": [
        "# Example input: current price and time features\n",
        "# Replace with actual values\n",
        "wind = 0.03\n",
        "log_wind = np.log(wind)\n",
        "phi = model_wind.params['wind.L1'] # Access params from wind_result\n",
        "intercept = model_wind.params['const'] # Access params from wind_result\n",
        "log_wind_next = intercept + phi * log_wind\n",
        "wind_next = np.exp(log_wind_next)\n",
        "\n",
        "display(Markdown(f\"\"\"\n",
        "Latest wind speed: {wind:.2f} m/s.\\n\n",
        "Predicted (mean) next hour wind speed: {wind_next:.2f} m/s\n",
        "\"\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2A4Rbv2NTUn"
      },
      "source": [
        " Moreover, the probability that the price falls within a given interval given current price is within an interval can be found:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qso3r6hJjoRA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm, lognorm\n",
        "from scipy.integrate import quad\n",
        "\n",
        "\n",
        "def get_wind_pr_inv(model, a, b, c, d):\n",
        "    \"\"\"\n",
        "    Compute the conditional probability:\n",
        "        P(a â¤ W_{t+1} â¤ b | c â¤ W_t â¤ d)\n",
        "    under a log-normal AR(1) model with intercept:\n",
        "        log(W_{t+1}) = alpha + phi * log(W_t) + epsilon_t,\n",
        "        where epsilon_t ~ N(0, sigma^2)\n",
        "\n",
        "    This transformation ensures W_t > 0 and W_{t+1} > 0.\n",
        "\n",
        "    Parameters:\n",
        "        model: Fitted AutoRegResults object (log-AR(1) model)\n",
        "        a (float): Lower bound of target interval for W_{t+1} (must be > 0)\n",
        "        b (float): Upper bound of target interval for W_{t+1} (must be > a)\n",
        "        c (float): Lower bound of conditioning interval for W_t (must be > 0)\n",
        "        d (float): Upper bound of conditioning interval for W_t\n",
        "\n",
        "    Returns:\n",
        "        float: Conditional probability P(a â¤ W_{t+1} â¤ b | c â¤ W_t â¤ d)\n",
        "    \"\"\"\n",
        "    # if a <= 0 or b <= 0 or c <= 0 or d <= 0:\n",
        "    #     raise ValueError(\"All bounds must be strictly positive for log-normal model.\")\n",
        "    eps = 0.0001\n",
        "    if a == 0: a = eps\n",
        "    if b == 0: b = eps\n",
        "    if c == 0: c = eps\n",
        "    if d == 0: d = eps\n",
        "    if a > b or c > d:\n",
        "        raise ValueError(\"Invalid interval bounds.\")\n",
        "\n",
        "    # Extract model parameters\n",
        "    phi = model.params.iloc[1]      # lag coefficient\n",
        "    alpha = model.params.iloc[0]    # intercept\n",
        "    sigma = model.resid.std()   # residual standard deviation\n",
        "\n",
        "    # PDF of W_t under stationary distribution (assumed log-normal)\n",
        "    # Approximate log(W_t) ~ N(mu, sigma_w^2) where mu = alpha / (1 - phi)\n",
        "    mu_log = alpha / (1 - phi)\n",
        "    sigma_log = sigma / np.sqrt(1 - phi**2)\n",
        "\n",
        "    def marginal_pdf_w(w):\n",
        "        \"\"\"PDF of W_t under stationary log-normal assumption.\"\"\"\n",
        "        return lognorm.pdf(w, s=sigma_log, scale=np.exp(mu_log))\n",
        "\n",
        "    def conditional_prob(w):\n",
        "        \"\"\"P(a â¤ W_{t+1} â¤ b | W_t = w) Ã f(W_t)\"\"\"\n",
        "        if w <= 0:\n",
        "            return 0.0\n",
        "        mu = alpha + phi * np.log(w)\n",
        "        p = lognorm.cdf(b, s=sigma, scale=np.exp(mu)) - lognorm.cdf(a, s=sigma, scale=np.exp(mu))\n",
        "        return p * marginal_pdf_w(w)\n",
        "\n",
        "    # Normalize over W_t â [c, d]\n",
        "    norm_const, _ = quad(marginal_pdf_w, c, d)\n",
        "    integral_val, _ = quad(conditional_prob, c, d)\n",
        "\n",
        "    return integral_val / norm_const\n",
        "\n",
        "display(Markdown(f\"\"\"\n",
        "Prob. of W_t+1 â [0, 8] given W_t â [3, 4]:\n",
        "{get_wind_pr_inv(model_wind, 0, 8, 3, 4):.4f}\n",
        "\"\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation\n",
        "\n",
        "To solve the problem we need the following steps:\n",
        "\n",
        "1) Discretizing the state and actions space\n",
        "2) A function that find all states\n",
        "3) A function that find all actions given a state\n",
        "4) A reward function\n",
        "5) A transition probability function\n",
        "6) Build the MDP using the above\n",
        "\n",
        "We will go through these steps one at a time.\n",
        "\n"
      ],
      "metadata": {
        "id": "22dLnjygiVIa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wLaHMpd3a9-"
      },
      "source": [
        "## Discretizing the MDP\n",
        "\n",
        "To solve the MDP we need to discreatize the states and actions. This can be done by assuming that:\n",
        "\n",
        "- The battery only can be charged/discharged in units of $\\delta$, e.g. $\\delta = 0.25$ MWh. That is, $a^b_t \\in \\{-b^{\\max}, \\ldots, -\\delta, 0, \\delta, ..., b^{\\max} \\}$\n",
        "\n",
        "- Hence the battery level is $s^l_t \\in \\{ 0, \\delta, 2\\delta, ..., b^{\\max} \\}$.\n",
        "\n",
        "- The same units are used for the wind energy production $a^w_t \\in \\{ 0, \\delta, 2\\delta, ..., p^{\\max} \\}$.\n",
        "\n",
        "- As a result, the commitment levels are $s^c_t \\in \\{ -b^\\max, \\ldots, -\\delta, 0, \\delta, \\ldots, b^{\\max} + p^{\\max}\\}$ since we can alt most buy energy for an empty battery and we can at most sell max wind production plus a full battery. The same intervals are used for $a^c_t$.\n",
        "\n",
        "- The wind speed is split into intervals $s^w_t \\in \\{ I^w_1, I^w_2, \\ldots, I^w_n \\}$ of length $\\gamma^w$ such that\n",
        "    - $I^w_1 = [0, w^\\text{cut_in}]$, i.e. from no wind to the cut-in wind speed.\n",
        "    - The speed from $w^\\text{cut_in}$ to $w^\\text{rated}$ is split into lenghts of $\\gamma$. That is $$I^w_j = ]a, b], \\gamma = b-a.$$\n",
        "    - A speed from $w^\\text{rated}$ to $w^\\text{cut_out}$ generates the same power, i.e $$I^w_{n-1} = ]w^\\text{rated}, w^\\text{cut_out}].$$\n",
        "    - Finally over cut out speed no power is generated, i.e. $$I^w_n = ]w^{\\text{cutout}}, \\infty[.$$\n",
        "\n",
        "- The price is split into intervals so $s^p_t \\in \\{ I^p_1, I^p_2, \\ldots, I^p_n \\}$ of length $\\gamma^p$ such that $I^p_j = ]b, c]$ and $\\gamma^p = c-b$. Special cases are $I^p_1 = (-\\infty, a]$ (price below $a$) and $I^p_n = [d, \\infty)$ (price above $d$).\n",
        "\n",
        "Note $\\delta$ must satisfy that $b^{\\max}/\\delta$ and $p^{\\max}/\\delta$ are integer. Moreover, when calculating $E(x)$, its value must be rounded down so contained in the set defined for $a^w_t$.\n",
        "\n",
        "Instead of storing intervals, we may represent e.g. $I^w_i = ]a,b]$ as a number $c = (b-a)/2$ (the middle). Given $c$ we can always find $a = c - (b-a)/2$ and $b = c + (b-a)/2$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbbgd7vjuhZp"
      },
      "source": [
        "Let us try to implement some functions for discreatization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0eVa-teueO_"
      },
      "outputs": [],
      "source": [
        "def generate_discrete_set(min_val, max_val, step):\n",
        "  \"\"\"\n",
        "  Generates a set of discrete values from min_val to max_val with step size.\n",
        "\n",
        "  Args:\n",
        "    min_val (float): The minimum value in the set.\n",
        "    max_val (float): The maximum value in the set.\n",
        "    step (float): The step size between consecutive values.\n",
        "\n",
        "  Returns:\n",
        "    set: A set of discrete values {min_val,â¦,âð¿,0,ð¿,...,max_val}.\n",
        "  \"\"\"\n",
        "  # Ensure step is a divisor of min_val\n",
        "  if not np.isclose(min_val % step, 0):\n",
        "      raise ValueError(\"min_val argument must be divisible by step.\")\n",
        "  if not np.isclose(max_val % step, 0):\n",
        "      raise ValueError(\"max_val argument must be divisible by step.\")\n",
        "\n",
        "  return np.arange(min_val, max_val + step, step)\n",
        "\n",
        "\n",
        "def generate_intervals(min_val, max_val, length, add_inf = False, add_neg_inf = False):\n",
        "    \"\"\"\n",
        "    Splits a given interval into n evenly spaced subintervals of type ]a,b] and returns midpoints and their corresponding bounds.\n",
        "\n",
        "    Args:\n",
        "        min_val (float): The minimum bound value of the first interval.\n",
        "        max_val (float): The maximum bound value of the last interval.\n",
        "        length (float): The interval length.\n",
        "        add_inf (bool): Add infinity interval too (midpoint is calculated based on the last interval).\n",
        "        add_neg_inf (bool): Add negative infinity interval too (midpoint is calculated based on the first interval).\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing:\n",
        "            - midpoints (list): Sorted list of midpoints in the intervals.\n",
        "            - intervals_dict (dict): Mapping of midpoints to a dictionary containing 'ab' (bounds) and 'length'.\n",
        "            - lower_inf (bool): Indicates if a negative infinity interval was added.\n",
        "            - upper_inf (bool): Indicates if an infinity interval was added.\n",
        "    \"\"\"\n",
        "    if not np.isclose((max_val - min_val) % length, 0):\n",
        "        raise ValueError(\"(max_val - min_val) must be divisible by length.\")\n",
        "\n",
        "    bounds = np.arange(min_val, max_val + length, length)\n",
        "    midpoints = np.arange(min_val + length/2, max_val + length/2, length)\n",
        "    midpoints = midpoints.tolist()\n",
        "    bounds = bounds.tolist()\n",
        "\n",
        "    intervals_dict = {}\n",
        "    for i in range(len(midpoints)):\n",
        "        intervals_dict[midpoints[i]] = {'ab': [bounds[i], bounds[i+1]], 'length': length}\n",
        "\n",
        "    if add_neg_inf:\n",
        "        neg_inf_midpoint = bounds[0] - 1  # Calculate midpoint for the negative infinity interval\n",
        "        intervals_dict[neg_inf_midpoint] = {'ab': [-np.inf, bounds[0]], 'length': np.inf}\n",
        "        midpoints.insert(0, neg_inf_midpoint)\n",
        "\n",
        "    if add_inf:\n",
        "        inf_midpoint = bounds[-1] + 1  # Calculate midpoint for the infinity interval\n",
        "        intervals_dict[inf_midpoint] = {'ab': [bounds[-1], np.inf], 'length': np.inf}\n",
        "        midpoints.append(inf_midpoint)\n",
        "\n",
        "    return {\"midpoints\": midpoints, 'intervals_dict': intervals_dict, \"lower_inf\": add_neg_inf, \"upper_inf\": add_inf}\n",
        "\n",
        "\n",
        "def get_interval(point, intervals, as_string = False):\n",
        "    \"\"\"\n",
        "    Return interval bounds given a point and interval.\n",
        "\n",
        "    Args:\n",
        "        midpoint (float): The midpoint of the interval.\n",
        "        intervals (dict): A dictionary\n",
        "        as_string (bool): If true return the interval as a string.\n",
        "    \"\"\"\n",
        "    midpoints = intervals['midpoints']\n",
        "    intervals_dict = intervals['intervals_dict']\n",
        "    lower_inf = intervals['lower_inf']\n",
        "    upper_inf = intervals['upper_inf']\n",
        "\n",
        "    # Check negative infinity interval\n",
        "    if lower_inf and point < intervals_dict[midpoints[0]]['ab'][1]:\n",
        "        if as_string:\n",
        "            return str(intervals_dict[midpoints[0]]['ab'])\n",
        "        else:\n",
        "            return intervals_dict[midpoints[0]]['ab']\n",
        "\n",
        "    # Check positive infinity interval\n",
        "    if upper_inf and point >= intervals_dict[midpoints[-1]]['ab'][0]:\n",
        "        if as_string:\n",
        "            return str(intervals_dict[midpoints[-1]]['ab'])\n",
        "        else:\n",
        "            return intervals_dict[midpoints[-1]]['ab']\n",
        "\n",
        "    # Check regular intervals\n",
        "    for midpoint in midpoints:\n",
        "        lower_bound, upper_bound = intervals_dict[midpoint]['ab']\n",
        "        if lower_bound <= point < upper_bound:\n",
        "            if as_string:\n",
        "                return str(intervals_dict[midpoint]['ab'])\n",
        "            else:\n",
        "                return intervals_dict[midpoint]['ab']\n",
        "\n",
        "    return None # Point is not in any defined interval\n",
        "\n",
        "\n",
        "def get_midpoint(point, intervals, interior = False):\n",
        "    \"\"\"\n",
        "    Return interval midpoint given a point and interval.\n",
        "\n",
        "    Args:\n",
        "        midpoint (float): The midpoint of the interval.\n",
        "        intervals (dict): A dictionary\n",
        "        interior (bool): If true only return the midpoint if the point is in the interior of an interval.\n",
        "    \"\"\"\n",
        "    midpoints = intervals['midpoints']\n",
        "    intervals_dict = intervals['intervals_dict']\n",
        "    lower_inf = intervals['lower_inf']\n",
        "    upper_inf = intervals['upper_inf']\n",
        "\n",
        "    # Check negative infinity interval\n",
        "    if lower_inf and point < intervals_dict[midpoints[0]]['ab'][1]:\n",
        "        return midpoints[0]\n",
        "\n",
        "    # Check positive infinity interval\n",
        "    if not interior:\n",
        "        if upper_inf and point >= intervals_dict[midpoints[-1]]['ab'][0]:\n",
        "             return midpoints[-1]\n",
        "    else:\n",
        "        if upper_inf and point > intervals_dict[midpoints[-1]]['ab'][0]:\n",
        "             return midpoints[-1]\n",
        "\n",
        "    # Check regular intervals\n",
        "    if not interior:\n",
        "        for midpoint in midpoints:\n",
        "            lower_bound, upper_bound = intervals_dict[midpoint]['ab']\n",
        "            if lower_bound <= point < upper_bound:\n",
        "                return midpoint\n",
        "    else:\n",
        "        for midpoint in midpoints:\n",
        "            lower_bound, upper_bound = intervals_dict[midpoint]['ab']\n",
        "            if lower_bound < point < upper_bound:\n",
        "                return midpoint\n",
        "\n",
        "    return None # Point is not in any defined interval\n",
        "\n",
        "\n",
        "def add_interval(a, b, intervals):\n",
        "    \"\"\"\n",
        "    Add an interval to a list of intervals.\n",
        "    This function assumes intervals sorted by midpoint.\n",
        "\n",
        "    Args:\n",
        "        a (float): The lower bound of the interval to add.\n",
        "        b (float): The upper bound of the interval to add.\n",
        "        intervals (dict): The existing dictionary of intervals.\n",
        "    \"\"\"\n",
        "    if a >= b:\n",
        "        raise ValueError(\"Lower bound must be smaller than upper bound.\")\n",
        "    if get_midpoint(a, intervals, interior = True) is not None or get_midpoint(b, intervals, interior = True) is not None:\n",
        "        raise ValueError(\"Upper or lower bound is already in an interval.\")\n",
        "    for midpoint in intervals['midpoints']:\n",
        "        if a <= midpoint <= b:\n",
        "            raise ValueError(\"Upper or lower bound is already in an interval.\")\n",
        "    if a == np.inf:\n",
        "        raise ValueError(\"Lower bound cannot be infinity.\")\n",
        "    if b == -np.inf:\n",
        "        raise ValueError(\"Upper bound cannot be -infinity.\")\n",
        "\n",
        "    if a == -np.inf:\n",
        "        intervals['lower_inf'] = True\n",
        "        midpoint = b - 1\n",
        "        length = np.inf\n",
        "    if b == np.inf:\n",
        "        intervals['upper_inf'] = True\n",
        "        midpoint = a + 1\n",
        "        length = np.inf\n",
        "    if not a == -np.inf and not b == np.inf:\n",
        "        midpoint = a + (b - a) / 2\n",
        "        length = b - a\n",
        "    new_interval_info = {'ab': [a, b], 'length': length}\n",
        "\n",
        "    # Find the correct insertion point for the midpoint\n",
        "    insert_index = 0\n",
        "    for i, existing_midpoint in enumerate(intervals['midpoints']):\n",
        "        if midpoint > existing_midpoint:\n",
        "            insert_index = i + 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    intervals['midpoints'].insert(insert_index, midpoint)\n",
        "    intervals['intervals_dict'][midpoint] = new_interval_info\n",
        "\n",
        "\n",
        "def remove_interval(point, intervals):\n",
        "    \"\"\"\n",
        "    Remove an interval from the dictionary based on a point.\n",
        "\n",
        "    Args:\n",
        "        point (float): A point of the interval to remove.\n",
        "        intervals (dict): The dictionary of intervals.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the interval was found and removed, False otherwise.\n",
        "    \"\"\"\n",
        "    midpoint = get_midpoint(point, intervals)\n",
        "    if not midpoint:\n",
        "        return False\n",
        "\n",
        "    if intervals['intervals_dict'][midpoint]['ab'][0] == -np.inf:\n",
        "        intervals['lower_inf'] = False\n",
        "    if intervals['intervals_dict'][midpoint]['ab'][1] == np.inf:\n",
        "        intervals['upper_inf'] = False\n",
        "\n",
        "    # Remove from midpoints list\n",
        "    intervals['midpoints'].remove(midpoint)\n",
        "\n",
        "    # Remove from intervals_dict\n",
        "    del intervals['intervals_dict'][midpoint]\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def join_intervals(a, b, intervals):\n",
        "    \"\"\"\n",
        "    Join two intervals if they are adjacent.\n",
        "\n",
        "    Args:\n",
        "        a (float): The lower bound of the first interval.\n",
        "        b (float): The upper bound of the second interval.\n",
        "        intervals (dict): The dictionary of intervals.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the intervals were joined, False otherwise.\n",
        "    \"\"\"\n",
        "    a = get_midpoint(a, intervals)\n",
        "    b = get_midpoint(b, intervals)\n",
        "    if not a or not b:\n",
        "        raise ValueError(\"At least one of the points is not in an interval.\")\n",
        "    if a == b:\n",
        "        raise ValueError(\"Points are in the same interval\")\n",
        "        return False\n",
        "    if (a > b):\n",
        "        a, b = b, a\n",
        "    a_idx = intervals['midpoints'].index(a)\n",
        "    b_idx = intervals['midpoints'].index(b)\n",
        "    if (a_idx != b_idx - 1):\n",
        "        return False\n",
        "    a_ab = intervals['intervals_dict'][a]['ab']\n",
        "    b_ab = intervals['intervals_dict'][b]['ab']\n",
        "    new_ab = [a_ab[0], b_ab[1]]\n",
        "    if a_ab[0] == -np.inf:\n",
        "        new_midpoint = b_ab[1] - 1\n",
        "        new_length = np.inf\n",
        "    if b_ab[1] == np.inf:\n",
        "        new_midpoint = a_ab[0] + 1\n",
        "        new_length = np.inf\n",
        "    if not a_ab[0] == -np.inf and not b_ab[1] == np.inf:\n",
        "        new_length = b_ab[1] - a_ab[0]\n",
        "        new_midpoint = a_ab[0] + new_length/2\n",
        "    new_interval = {'ab': new_ab, 'length': new_length}\n",
        "    intervals['intervals_dict'][new_midpoint] = new_interval\n",
        "    del intervals['intervals_dict'][a]\n",
        "    del intervals['intervals_dict'][b]\n",
        "    intervals['midpoints'][a_idx] = new_midpoint\n",
        "    intervals['midpoints'].remove(b)\n",
        "    return True\n",
        "\n",
        "\n",
        "def closest(lst, a):\n",
        "    \"\"\"\n",
        "    Given a number finds the closest number in a list (if ties choose the smallest).\n",
        "\n",
        "    Args:\n",
        "        lst (list): The list of numbers to search within.\n",
        "        a (float or int): The target number to find the closest value to.\n",
        "\n",
        "    Returns:\n",
        "        float or int: The number from the list that is closest to a.\n",
        "    \"\"\"\n",
        "    idx = min(range(len(lst)), key=lambda i: abs(lst[i]-a))\n",
        "    return lst[idx]\n",
        "\n",
        "\n",
        "def get_left_or_right(sorted_list, a, return_left = True):\n",
        "    \"\"\"\n",
        "    Given a sorted list and a number 'a', finds the left and right numbers\n",
        "    in the list that 'a' is between.\n",
        "\n",
        "    Args:\n",
        "        sorted_list (list): A list of numbers sorted in ascending order.\n",
        "        a (float or int): The number to find the bounds for.\n",
        "        return_left (bool): If True, returns the left bound. If False, returns the right bound.\n",
        "\n",
        "    Returns:\n",
        "        list: If 'a' is equal to a number in the list, returns a.\n",
        "              If 'a' is less than the smallest number, returns right.\n",
        "              If 'a' is greater than the largest number, returns left.\n",
        "              If 'a' is between two numbers in the list, returns left or right depending on `return_left`.\n",
        "    \"\"\"\n",
        "    # Handle edge cases: a is less than the smallest or greater than the largest\n",
        "    if a < sorted_list[0]:\n",
        "        return sorted_list[0]\n",
        "    if a > sorted_list[-1]:\n",
        "        return sorted_list[-1]\n",
        "\n",
        "    # Iterate through the sorted list to find the bounds\n",
        "    for i in range(len(sorted_list) - 1):\n",
        "        left = sorted_list[i]\n",
        "        right = sorted_list[i+1]\n",
        "\n",
        "        if left <= a <= right:\n",
        "            # If a is exactly in the list or between two numbers\n",
        "            if a == left or a == right:\n",
        "                return a\n",
        "            elif return_left:\n",
        "                return left\n",
        "            else:\n",
        "                return right\n",
        "\n",
        "    # This part should ideally not be reached if edge cases are handled and the list is sorted\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV6fMSguuz5t"
      },
      "source": [
        "Now let us see them in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTAU1fgku0Wg"
      },
      "outputs": [],
      "source": [
        "## Testing\n",
        "dis = generate_discrete_set(0, 10, 2)\n",
        "print(\"Point discreatization:\", dis)\n",
        "print(\"Find left point near 1.8:\", get_left_or_right(dis, 1.8))\n",
        "print(\"Find right point near 1.8:\", get_left_or_right(dis, 1.8, False))\n",
        "\n",
        "inv = generate_intervals(0, 2, 0.5, add_inf = True, add_neg_inf = False)\n",
        "print(\"Intervals (w inf):\")\n",
        "pprint(inv)\n",
        "print(\"Add interval:\")\n",
        "add_interval(-4, -3, inv)\n",
        "add_interval(-np.inf, -4, inv)\n",
        "pprint(inv)\n",
        "print(\"Join intervals:\")\n",
        "get_midpoint(1.25, inv)\n",
        "get_midpoint(1.75, inv)\n",
        "join_intervals(1.25, 1.75, inv)\n",
        "pprint(inv)\n",
        "print(\"Find midpoint closest to -2.5:\", closest(inv['midpoints'], -2.5))\n",
        "print(\"Get intervals:\")\n",
        "print(get_interval(0, inv))\n",
        "print(get_interval(0.5, inv))\n",
        "print(get_interval(1.51, inv))\n",
        "print(get_interval(2, inv))\n",
        "print(get_interval(-3, inv))\n",
        "print(get_interval(-4, inv))\n",
        "print(get_interval(-7, inv))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb7HkU5j0JTr"
      },
      "source": [
        "We want to discretize as stated in\n",
        "\n",
        "Assume the following parameter values are stored in a dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciro_ml2-75t"
      },
      "outputs": [],
      "source": [
        "settings = {\n",
        "    ## Problem parameters\n",
        "    'b_max': 10,  # battery capacity (MWh)\n",
        "    'p_max': 10,  # max power output wind (MW)\n",
        "    'c_max': 10 + 10,  # max commitment (MWh) # b_max + p_max\n",
        "    'w_cut_in': 3,  # cut in wind speed (m/s)\n",
        "    'w_rated': 12,  # rated wind speed (m/s)\n",
        "    'w_cut_out': 25,  # cut out wind speed (m/s)\n",
        "    'c_plus': 50,  # EUR/MWh\n",
        "    'c_minus': 50, # EUR/MWh\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isFeNuhqwrJN"
      },
      "source": [
        "### Q1\n",
        "\n",
        "Using the implemented functions and the settings dict, create sets and intervals that discretise the state and action variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmxPQ6Z60dNF"
      },
      "outputs": [],
      "source": [
        "#@title Solution\n",
        "\n",
        "## Input parameters\n",
        "step_mwh = 2  # step_mwh (float): Step size for energy resolution.\n",
        "length_ms = 3  # length_ms (float): Length of intervals for wind speed.\n",
        "length_p = 50  # length_p (float): Length of intervals for price.\n",
        "min_p = -50  # min_p (float): Minimum price.\n",
        "max_p = 250  # max_p (float): Maximum price.\n",
        "\n",
        "## extract settings\n",
        "b_max = settings['b_max']\n",
        "p_max = settings['p_max']\n",
        "c_max = settings['c_max']\n",
        "w_cut_in = settings['w_cut_in']\n",
        "w_rated = settings['w_rated']\n",
        "w_cut_out = settings['w_cut_out']\n",
        "\n",
        "## States variables\n",
        "s_l_set = generate_discrete_set(0, b_max, step_mwh)\n",
        "# your code\n",
        "\n",
        "# s_w\n",
        "s_w_inv = generate_intervals(w_cut_in, w_rated, length_ms)\n",
        "add_interval(0, w_cut_in, s_w_inv)\n",
        "# your code\n",
        "\n",
        "# s_p\n",
        "# your code\n",
        "\n",
        "## Action variables\n",
        "# your code\n",
        "\n",
        "# View results\n",
        "print(\"s_l:\", s_l_set)\n",
        "print(\"s_c:\", s_c_set)\n",
        "print(\"s_w:\")\n",
        "pprint(s_w_inv)\n",
        "print(\"s_p:\")\n",
        "pprint(s_p_inv)\n",
        "print(\"a_b:\", a_b_set)\n",
        "print(\"a_w:\", a_w_set)\n",
        "print(\"a_c:\", a_c_set)\n",
        "\n",
        "# Output:\n",
        "# s_l: [ 0  2  4  6  8 10]\n",
        "# s_c: [-10  -8  -6  -4  -2   0   2   4   6   8  10  12  14  16  18  20]\n",
        "# s_w:\n",
        "# {'intervals_dict': {1.5: {'ab': [0, 3], 'length': 3},\n",
        "#                     4.5: {'ab': [3, 6], 'length': 3},\n",
        "#                     7.5: {'ab': [6, 9], 'length': 3},\n",
        "#                     10.5: {'ab': [9, 12], 'length': 3},\n",
        "#                     18.5: {'ab': [12, 25], 'length': 13},\n",
        "#                     26: {'ab': [25, inf], 'length': inf}},\n",
        "#  'lower_inf': False,\n",
        "#  'midpoints': [1.5, 4.5, 7.5, 10.5, 18.5, 26],\n",
        "#  'upper_inf': True}\n",
        "# s_p:\n",
        "# {'intervals_dict': {-51: {'ab': [-inf, -50], 'length': inf},\n",
        "#                     -25.0: {'ab': [-50, 0], 'length': 50},\n",
        "#                     25.0: {'ab': [0, 50], 'length': 50},\n",
        "#                     75.0: {'ab': [50, 100], 'length': 50},\n",
        "#                     101: {'ab': [100, inf], 'length': inf}},\n",
        "#  'lower_inf': True,\n",
        "#  'midpoints': [-51, -25.0, 25.0, 75.0, 101],\n",
        "#  'upper_inf': True}\n",
        "# a_b: [-10  -8  -6  -4  -2   0   2   4   6   8  10]\n",
        "# a_w: [ 0  2  4  6  8 10]\n",
        "# a_c: [-10  -8  -6  -4  -2   0   2   4   6   8  10  12  14  16  18  20]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## States\n",
        "\n",
        "Note that a state in the MDP is a combination of the state variables, and the state space consists of combinations of the state variables. In the `MDP` class states are stored as strings. As we will see later, it is useful to store a state as a string that can be converted to a directory. Let us define all states as strings that can easily be converted to a dict and see how it works:"
      ],
      "metadata": {
        "id": "Y8N6EIKCnWrE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln3pfAQLpYLc"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "def state_variables_to_str(s_l, s_c, s_w, s_p):\n",
        "    \"\"\"\n",
        "    String representaion of a state.\n",
        "    \"\"\"\n",
        "    return \"{'s_l': \" + str(s_l) + \", 's_c': \" + str(s_c) + \", 's_w': \" + str(s_w) + \", 's_p': \" + str(s_p) + \"}\"\n",
        "\n",
        "def action_variables_to_str(a_b, a_w, a_c):\n",
        "    \"\"\"\n",
        "    String representaion of an action.\n",
        "    \"\"\"\n",
        "    return \"{'a_b': \" + str(a_b) + \", 'a_w': \" + str(a_w) + \", 'a_c': \" + str(a_c) + \"}\"\n",
        "\n",
        "def str_to_dict(str):\n",
        "    \"\"\"\n",
        "    Convert a string representation of a dictionary to a dictionary.\n",
        "\n",
        "    Args:\n",
        "        str (str): String representation of a dictionary.\n",
        "\n",
        "    Returns:\n",
        "        dict: Converted dictionary.\n",
        "    \"\"\"\n",
        "    return ast.literal_eval(str)\n",
        "\n",
        "# Testing\n",
        "s = state_variables_to_str(1, 2, 3, 4)\n",
        "print(\"State:\", s)\n",
        "s_dict = str_to_dict(s)\n",
        "print(\"State as dict:\", s_dict)\n",
        "print(\"Access s_c:\", s_dict['s_c'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNjtrzjb20Oq"
      },
      "source": [
        "### Q2\n",
        "\n",
        "Use the functions to store all states in a list. Hint: for state variables corresponding to intervals, use the midpoints. How many states are there?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "# Find all combinations\n",
        "state_labels = # your code [state_variables_to_str(...) for ...]\n",
        "display(Markdown(f\"There are {len(state_labels)} states.\"))"
      ],
      "metadata": {
        "id": "Rt0mAocRoCXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actions\n",
        "\n",
        "Given a state, a set of actions is possible"
      ],
      "metadata": {
        "id": "WdYFwJ5Gz7y2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3\n",
        "\n",
        "Create a function that takes a state (as a dict) and returns a list of possible actions (as dict)."
      ],
      "metadata": {
        "id": "Yplo7Vft5RU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "def get_actions(s: dict, settings) -> list:\n",
        "    \"\"\"\n",
        "    Get all possible actions for a given state.\n",
        "\n",
        "    Args:\n",
        "        s (dict): State containing 's_l', 's_c', 's_w', 's_p'.\n",
        "\n",
        "    Returns:\n",
        "        list: List of possible actions (as dict).\n",
        "    \"\"\"\n",
        "    b_max = settings['b_max']\n",
        "    # Determine a_b and a_w\n",
        "    e_max = get_left_or_right(a_w_set, energy(s['s_w'])) # max energy we can sell/produce (e.g. if energy(s['s_w']) = 1.25 but a_w_set is [0,1,2,...] then can only sell 1)\n",
        "    if s['s_c'] >= 0:\n",
        "        if s['s_c'] >= e_max:\n",
        "            # your code\n",
        "        if s['s_c'] < e_max:\n",
        "            # your code\n",
        "    if s['s_c'] < 0:\n",
        "        # your code\n",
        "\n",
        "    actions = [action_variables_to_str(a_b, a_w, a_c) for a_c in a_c_set]\n",
        "    # your code (actions to dict)\n",
        "    return actions\n",
        "\n",
        "# Test\n",
        "s_lab = state_labels[300]\n",
        "print(\"State:\", s_lab)\n",
        "print(\"Actions:\")\n",
        "pprint(get_actions(str_to_dict(s_lab), settings))"
      ],
      "metadata": {
        "id": "4YpxeuIA0sRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFqi2LvjFnuE"
      },
      "source": [
        "## Reward function\n",
        "\n",
        "The expected reward can be calculated given a state and action."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4\n",
        "\n",
        "Create a function that calculates the reward given the state and action"
      ],
      "metadata": {
        "id": "k_YJqAABqBBQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RC5He6J3FoqO"
      },
      "outputs": [],
      "source": [
        "#@title Solution\n",
        "\n",
        "def get_reward2(s_p, s_c, a_b, a_w, c_plus, c_minus):\n",
        "    \"\"\"\n",
        "    Calculate the reward for given state-action values.\n",
        "\n",
        "    Args:\n",
        "        s_p (float): Price.\n",
        "        s_c (float): Commitment.\n",
        "        a_b (float): Energy transferred to the battery.\n",
        "        a_w (float): Energy produced by the wind turbine.\n",
        "        c_plus (float): Penalty cost of energy above zero.\n",
        "        c_minus (float): Penalty cost of energy below zero.\n",
        "\n",
        "    Returns:\n",
        "        float: Reward value.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the reward cannot be calculated.\n",
        "    \"\"\"\n",
        "     # your code e =\n",
        "    if s_p >= 0:\n",
        "        # your code\n",
        "    if s_p < 0:\n",
        "        # your code\n",
        "    raise ValueError(\"Reward can not be calculated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that in the above reward function, the price is fixed. Since we know that the price state variable defines an interval, we need to calculate the expected reward over the interval:"
      ],
      "metadata": {
        "id": "dszARgMaNTgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import integrate\n",
        "\n",
        "def mean_value(f, a, b, *params):\n",
        "    \"\"\"\n",
        "    Compute the mean value of a function f(x, *params) over interval [a,b].\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    f : callable\n",
        "        Function f(x, *params) where x is the variable of integration.\n",
        "    a, b : float\n",
        "        Interval [a, b] over which to compute the mean.\n",
        "    *params : extra arguments\n",
        "        Constants passed to f.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        Mean value of f over [a, b].\n",
        "    \"\"\"\n",
        "    I, _ = integrate.quad(f, a, b, args=params)  # definite integral\n",
        "    return I / (b - a)\n",
        "\n",
        "\n",
        "def get_mean_reward(s_p, s_c, a_b, a_w, c_plus, c_minus, s_p_inv, lower_limit = -60, upper_limit = 260):\n",
        "    \"\"\"\n",
        "    Calculate the mean reward for given state-action values when s_p is within a given interval.\n",
        "\n",
        "    Args:\n",
        "        s_p (float): Price midpoint.\n",
        "        s_c (float): Commitment.\n",
        "        a_b (float): Energy transferred to the battery.\n",
        "        a_w (float): Energy produced by the wind turbine.\n",
        "        c_plus (float): Penalty cost of energy above zero.\n",
        "        c_minus (float): Penalty cost of energy below zero.\n",
        "        s_p_inv (dict): Price state intervals.\n",
        "        lower_limit (float, optional): Lower limit used if the lower limit of the interval is -inf. Defaults to -60.\n",
        "        upper_limit (float, optional): Upper limit used if the upper limit of the interval is inf. Defaults to 260.\n",
        "\n",
        "    Returns:\n",
        "        float: Mean reward\n",
        "    \"\"\"\n",
        "    ab = s_p_inv['intervals_dict'][s_p]['ab']\n",
        "    if np.isinf(ab[1]):\n",
        "        return mean_value(get_reward2, ab[0], upper_limit, s_c, a_b, a_w, c_plus, c_minus)\n",
        "\n",
        "    if np.isinf(ab[0]):\n",
        "        return mean_value(get_reward2, lower_limit, ab[1], s_c, a_b, a_w, c_plus, c_minus)\n",
        "\n",
        "    return mean_value(get_reward2, ab[0], ab[1], s_c, a_b, a_w, c_plus, c_minus)\n",
        "\n",
        "# np.isinf(-np.inf)\n",
        "# s_p_inv\n",
        "# get_reward2(25, 10, 4, 2, 500, 500)\n",
        "# get_mean_reward(25, 10, 4, 2, 500, 500, s_p_inv)\n",
        "# get_reward2(101, 10, 4, 2, 500, 500)\n",
        "# get_mean_reward(101, 10, 4, 2, 500, 500, s_p_inv)\n",
        "# get_reward2(-51, 10, 4, 2, 500, 500)\n",
        "# get_mean_reward(-51, 10, 4, 2, 500, 500, s_p_inv)\n",
        "# join_intervals(-51, -25, s_p_inv)\n",
        "# get_mean_reward(-1, 10, 4, 2, 500, 500, s_p_inv)"
      ],
      "metadata": {
        "id": "R40fH9t6NT5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To verify the reward function, we want to plot it. First, we generate a data frame with the reward:"
      ],
      "metadata": {
        "id": "G_dXNABCkeTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_rewards(c_plus = 500, c_minus = 500):\n",
        "    s_c = s_c_set\n",
        "    s_p = s_p_inv['midpoints']\n",
        "    a_b = a_b_set\n",
        "    a_w = a_w_set\n",
        "    all_combinations = list(itertools.product(s_c, s_p, a_b, a_w))  # get all combinations of the variables\n",
        "    df = pd.DataFrame(all_combinations, columns=['s_c', 's_p', 'a_b', 'a_w'])\n",
        "    df = (df\n",
        "        >> mutate(e = X.a_b + X.a_w)\n",
        "        >> mutate(imbalance = abs(X.s_c - X.e))\n",
        "        >> mutate(state = [state_variables_to_str(0, s_c, 0, s_p) for s_c, s_p in zip(df['s_c'], df['s_p'])])\n",
        "        >> mutate(action = [action_variables_to_str(a_b, a_w, 0) for a_b, a_w in zip(df['a_b'], df['a_w'])])\n",
        "    )\n",
        "    df['reward'] = df.apply(lambda x: get_reward2(x['s_p'], x['s_c'], x['a_b'], x['a_w'], c_plus, c_minus), axis = 1)\n",
        "    df['mean_reward'] = df.apply(lambda x: get_mean_reward(x['s_p'], x['s_c'], x['a_b'], x['a_w'], c_plus, c_minus, s_p_inv), axis = 1)\n",
        "    df = df >> mask(~((X.s_c > 0) & (X.e < 0))) >> mask(~((X.s_c < 0) & (X.e > 0)))  # remove rows which are not possible\n",
        "    return df\n",
        "\n",
        "df = generate_rewards(500, 0)\n",
        "display(df)"
      ],
      "metadata": {
        "id": "sdWfhYi6l6Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5\n",
        "\n",
        "Plot the reward using the data frame by:\n",
        "\n",
        "- Having `s_p` and `reward` on the x-axis and y-axis, respectively.\n",
        "- Use lines and points to plot the reward.\n",
        "- Use different colors for `s_c`.\n",
        "- Make a subplot (facet) for each `imbalance`.\n",
        "- Try different values of `[c_plus, c_minus]`, e.g. [50, 25], [25, 50] and [0, 0].\n",
        "\n",
        "Hint: You may use `theme(figure_size=(16,10))` to make the plot big. Make `s_c` categorical (discrete) using `pd.Categorical(df['s_c'])`.\n",
        "\n",
        "Verify that the rewards satisfy   \n",
        "\n",
        "- The reward increases for increasing prices if commitment is positive for price, either below or above zero.\n",
        "- The reward decreases for increasing prices if commitment is negative, for price, either below or above zero.\n",
        "- For a fixed positive price, the reward increases with the commitment.\n",
        "- For a fixed negative price, the reward decreases with the commitment.\n",
        "- For increasing imbalance, the reward decreases.\n",
        "\n"
      ],
      "metadata": {
        "id": "wXfEXNyjl-Ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "def plot_reward(c_plus, c_minus):\n",
        "    dat = generate_rewards(c_plus, c_minus)\n",
        "    dat['s_c'] = pd.Categorical(dat['s_c'])\n",
        "    pt = (\n",
        "        # ggplot(dat, aes(...))\n",
        "        # your code\n",
        "    )\n",
        "    pt.show()\n",
        "\n",
        "plot_reward(50, 25)\n",
        "plot_reward(25, 50)\n",
        "plot_reward(0, 0)\n",
        "plot_reward(500, 500)"
      ],
      "metadata": {
        "id": "KN5PDOyYkevB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPmZNhoS6qBk"
      },
      "source": [
        "## Transition probability funciton\n",
        "\n",
        "Given the discretization we need to be able to calculate probabilities:\n",
        "\n",
        "\\begin{align}\n",
        "    p(s^w_{t+1} | s^w_t) &= p(s^w_{t+1} \\in I_{t+1} | s^w_t \\in I_t) \\\\\n",
        "    p(s^p_{t+1} | s^p_t) &= p(s^p_{t+1} \\in I_{t+1} | s^p_t \\in I_t) \\\\\n",
        "\\end{align}\n",
        "\n",
        "Let us implement these functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tY-bsmCJ6yC3"
      },
      "outputs": [],
      "source": [
        "def get_trans_pr_w(s_w, s_w_next, s_w_inv):\n",
        "    \"\"\"\n",
        "    Calculate the transition probability for wind states.\n",
        "\n",
        "    Args:\n",
        "        s_w (float): Current wind speed.\n",
        "        s_w_next (float): Next wind speed.\n",
        "        s_w_inv (dict): wind state intervals.\n",
        "\n",
        "    Returns:\n",
        "        float: Transition probability between the current and next wind state interval.\n",
        "    \"\"\"\n",
        "    a, b = get_interval(s_w_next, s_w_inv)\n",
        "    c, d = get_interval(s_w, s_w_inv)\n",
        "    pr_w = get_wind_pr_inv(model_wind, a, b, c, d)\n",
        "    return pr_w\n",
        "\n",
        "def get_trans_pr_p(s_p, s_p_next, s_p_inv):\n",
        "    \"\"\"\n",
        "    Calculate the transition probability for price states.\n",
        "\n",
        "    Args:\n",
        "        s_p (float): Current price.\n",
        "        s_p_next (float): Next price.\n",
        "\n",
        "    Returns:\n",
        "        float: Transition probability between the current and next price state.\n",
        "    \"\"\"\n",
        "    a, b = get_interval(s_p_next, s_p_inv)\n",
        "    c, d = get_interval(s_p, s_p_inv)\n",
        "    pr_p = get_price_pr_inv(model_price, a, b, c, d)\n",
        "    return pr_p\n",
        "\n",
        "# Test\n",
        "print(\"Pr w:\", get_trans_pr_w(0, 0.5, s_w_inv))\n",
        "print(\"Pr p:\", get_trans_pr_p(0, 1.25, s_p_inv))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gteDBwC7L2N"
      },
      "source": [
        "Given the above functions we may store all values in two matrices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaAo2wVL7MZT"
      },
      "outputs": [],
      "source": [
        "# Save all combinations in a matrix pr_w_mat[s_w, s_w_next]\n",
        "pr_w_mat = np.zeros((len(s_w_inv['midpoints']), len(s_w_inv['midpoints'])))\n",
        "for r in range(pr_w_mat.shape[0]):\n",
        "    for c in range(pr_w_mat.shape[1]):\n",
        "        pr_w_mat[r, c] = get_trans_pr_w(\n",
        "            s_w_inv['midpoints'][r],\n",
        "            s_w_inv['midpoints'][c],\n",
        "            s_w_inv\n",
        "        )\n",
        "if any(abs(pr_w_mat.sum(axis = 1) - 1) > 0.000001):\n",
        "    raise ValueError(\"Error don't sum to one!\")\n",
        "# normalize small diffs\n",
        "pr_w_mat = pr_w_mat / pr_w_mat.sum(axis = 1, keepdims = True)\n",
        "\n",
        "# Save all combinations in a matrix pr_p_mat[s_p, s_p_next]\n",
        "pr_p_mat = np.zeros((len(s_p_inv['midpoints']), len(s_p_inv['midpoints'])))\n",
        "for r in range(pr_p_mat.shape[0]):\n",
        "    for c in range(pr_p_mat.shape[1]):\n",
        "        pr_p_mat[r, c] = get_trans_pr_p(\n",
        "            s_p_inv['midpoints'][r],\n",
        "            s_p_inv['midpoints'][c],\n",
        "            s_p_inv\n",
        "        )\n",
        "if any(abs(pr_p_mat.sum(axis = 1) - 1) > 0.000001):\n",
        "    raise ValueError(\"Error don't sum to one!\")\n",
        "# normalize small diffs\n",
        "pr_p_mat = pr_p_mat / pr_p_mat.sum(axis = 1, keepdims = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obiXR-iD9Qjy"
      },
      "source": [
        "### Q6\n",
        "\n",
        "These can be used to calculate the transition probabilities. Finish the function below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTsTeAWt9Xf4"
      },
      "outputs": [],
      "source": [
        "#@title Solution\n",
        "\n",
        "def get_trans_pr(s, a_c, a_b, pr_w_mat, pr_p_mat, s_w_inv, s_p_inv):\n",
        "    \"\"\"\n",
        "    Compute the transition probabilities for a given state and actions. Assume\n",
        "    the states and actions are discretised values.\n",
        "\n",
        "    Args:\n",
        "        s (dict): Current state, containing 's_l', 's_c', 's_w', 's_p'.\n",
        "        a_c (float): Commitment.\n",
        "        a_b (float): Energy transferred to the battery.\n",
        "        pr_w_mat (np.ndarray): Transition probability matrix for wind states.\n",
        "        pr_p_mat (np.ndarray): Transition probability matrix for price states.\n",
        "        s_w_inv (dict): wind state intervals.\n",
        "        s_p_inv (dict): price state intervals.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of transition probabilities to the next states.\n",
        "    \"\"\"\n",
        "    res = {}\n",
        "    # s_l_next = ...\n",
        "    # s_c_next = ...\n",
        "    s_w_idx = s_w_inv['midpoints'].index(s['s_w'])\n",
        "    s_p_idx = s_p_inv['midpoints'].index(s['s_p'])\n",
        "\n",
        "    for s_w_next_idx, s_w_next in enumerate(s_w_inv['midpoints']):\n",
        "        for s_p_next_idx, s_p_next in enumerate(s_p_inv['midpoints']):\n",
        "            # s_next = ...\n",
        "            # pr_w = ...\n",
        "            # pr_p = ...\n",
        "            # p = ...\n",
        "            # if p > 0:\n",
        "            #     res[s_next] = ...\n",
        "    return res\n",
        "\n",
        "# Test\n",
        "s = state_labels[4]\n",
        "s = str_to_dict(s)\n",
        "print(\"State:\", s)\n",
        "pr = get_trans_pr(s, 10, 4, pr_w_mat, pr_p_mat, s_w_inv, s_p_inv)\n",
        "print(\"Pr:\", pr)\n",
        "print(\"Sum:\",sum(pr.values()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHIAjChP-xIw"
      },
      "source": [
        "## Building the MDP\n",
        "\n",
        "We are now ready to build the MDP."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q7\n",
        "\n",
        "Create a function that, given the settings, builds the MDP using the MDP class and returns it."
      ],
      "metadata": {
        "id": "kcvBWtNrtkMX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Q4Oan02Hpjul"
      },
      "outputs": [],
      "source": [
        "#@title Solution\n",
        "\n",
        "def build_mdp(settings, s_l_set, s_c_set, s_w_inv, s_p_inv, a_b_set, a_w_set, a_c_set, pr_w_mat, pr_p_mat):\n",
        "    mdp = MDP()  # Create MDP\n",
        "\n",
        "    # Add states\n",
        "    # state_labels = ...\n",
        "    mdp.add_state_space(state_labels)\n",
        "\n",
        "    # Add actions\n",
        "    for s_label in mdp.get_state_keys():\n",
        "        s = str_to_dict(s_label)  # Convert state string to dict\n",
        "        for a in get_actions(s, settings):  # Add possible actions\n",
        "            action_label = str(a)\n",
        "            # your code\n",
        "            # mdp.add_action(...)\n",
        "\n",
        "    print(\"MDP info:\", mdp.get_mdp_info())\n",
        "    return mdp\n",
        "\n",
        "# mdp = build_mdp(...)\n",
        "mdp.check(delta=0.0001)  # check if diff below 0.0001 (assume ok)\n",
        "mdp.normalize()  # normalize pr so sum to one"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solving the MDP and reviewing results"
      ],
      "metadata": {
        "id": "vScy-to0yan0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wviC3Dw5-8Se"
      },
      "source": [
        "### Q8\n",
        "\n",
        "Find the optimal policy given a discount rate of 0.5.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "# <your code>"
      ],
      "metadata": {
        "id": "2ajRdFEm89hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To verify the model, we need to transform the policy into a dataframe:"
      ],
      "metadata": {
        "id": "XhJ8a08StBsh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V31kMVRN_L-V"
      },
      "outputs": [],
      "source": [
        "policy = mdp.get_policy(add_state_values = True)\n",
        "\n",
        "# Apply the parsing functions to create new columns with state variables\n",
        "df_result = policy\n",
        "# df_result.info()\n",
        "df_result = df_result.join(df_result['state'].apply(str_to_dict).apply(pd.Series))\n",
        "df_result = df_result.join(df_result['action'].apply(str_to_dict).apply(pd.Series))\n",
        "df_result = (\n",
        "    df_result\n",
        "        >> mutate(e = X.a_b + X.a_w)\n",
        "        >> mutate(imbalance = (X.e != X.s_c),\n",
        "                  E = [energy(s_w) for s_w in df_result['s_w']],\n",
        "                  reward = [mdp.get_reward(s, a) for s, a in zip(df_result['state'], df_result['action'])]\n",
        "        )\n",
        ")\n",
        "\n",
        "# Add interval index and labels\n",
        "df_result = df_result >> mutate(s_w_idx = [np.argmax(df_result['s_w'].unique() == v) for v in df_result['s_w']],\n",
        "                                s_w_str = [get_interval(s_w, s_w_inv, as_string=True) for s_w in df_result['s_w']])\n",
        "df_result = df_result >> mutate(s_p_idx = [np.argmax(df_result['s_p'].unique() == v) for v in df_result['s_p']],\n",
        "                                s_p_str = [get_interval(s_p, s_p_inv, as_string=True) for s_p in df_result['s_p']])\n",
        "\n",
        "# Get steady state pr and join\n",
        "pr = mdp.get_steady_state_pr(tol=0.00001).drop(columns = 'pr')\n",
        "pr.set_index(['state', 'action'], inplace=True)\n",
        "df_result.set_index(['state', 'action'], inplace=True)\n",
        "df_result = df_result.join(pr)\n",
        "\n",
        "display(df_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the we calculate the steady state probabilities. The steady state distribution is a probability distribution over states that remains unchanged as time goes to infinity.  \n",
        "\n",
        "If $\\pi$ is the optimal policy and $P^\\pi$ is its transition matrix, the steady-state distribution $\\mu$ satisfies:\n",
        "$$\n",
        "\\mu P^\\pi = \\mu, \\quad \\sum_{i} \\mu_i = 1.\n",
        "$$\n",
        "It tells you what fraction of time (in the long run) the system spends in each state under policy $\\pi$."
      ],
      "metadata": {
        "id": "GsS_AKx6LgpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q9\n",
        "\n",
        "Verify the model (is the model implemented correctly?) by checking\n",
        "\n",
        "1. If commit to the grid, then receiving energy should not be possible.\n",
        "2. If commit from the grid, then sending energy should not be possible.\n",
        "3. If commit from the grid, then discharging the battery should not be possible\n",
        "4. Using a negative amount of wind should not be possible.\n",
        "5. How many times is there an imbalance if commit from the grid? Is it reasonable?\n",
        "6. How many times is there an imbalance if commit to the grid? Is it reasonable?\n"
      ],
      "metadata": {
        "id": "1z37vKVftdlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "## Verification â âDid we build the model right?â\n",
        "print(\"1:\", len(df_result >> mask(X.e < 0, X.s_c > 0)), end = \", \")\n",
        "# print(\"2:\", ..., end = \", \")\n",
        "# your code"
      ],
      "metadata": {
        "id": "44cuVpY-toRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q10\n",
        "\n",
        "Validate the model (are the model's results reasonable?). Check the results by filtering the data frame.\n",
        "\n",
        "1. Do we choose to commit to the grid in the next period when prices (midpoints) are negative?\n",
        "2. Do we choose not to commit to the grid in the next period when prices (midpoints) are positive?\n",
        "3. How many different values of `a_c` is used?\n",
        "4. What actions are taken given the price and wind? Make a plot with\n",
        "   - `s_w_idx` and `s_p_idx` on the x-axis and y-axis, respectively.\n",
        "   - Consider low, mid and high values of `s_l` and low, mid and high values of `s_c`.\n",
        "   - Use `a_c` as label and color aesthetics.\n",
        "   - Use labels as geom.\n",
        "   - Make a subplot (facet) with rows `s_l` and cols `s_c`.\n",
        "   - Plot only states with a steady state probability larger than zero.\n",
        "\n",
        "   Make `s_c` categorical (discrete) using `pd.Categorical(df['s_c'])`.\n",
        "\n",
        "   - Is commitment increasing with prices?\n",
        "   - Do we have imbalances in the states we visit, given the optimal policy?\n",
        "   - Why are commitments the same no matter the value of `s_w` if `s_c` is negative?\n",
        "\n",
        "5. Make the same plot as above, but add reward as a label instead of `a_c`.\n",
        "   - Are the rewards as expected?\n"
      ],
      "metadata": {
        "id": "d1oNCO98yAoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "## Validation â âDid we build the right model?â\n",
        "# your code"
      ],
      "metadata": {
        "id": "1rHUcBC-wjPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us put it all together\n",
        "\n",
        "### Q11\n",
        "\n",
        "Join all the calculations into a function `build_and_solve_mdp`, that takes arguments: `settings, s_l_set, s_c_set, s_w_inv, s_p_inv, a_b_set, a_w_set, a_c_set, gamma`. The function build and solve the MDP. Next, the results are plotted.  \n"
      ],
      "metadata": {
        "id": "tCuInXkGgD1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "def build_and_solve_mdp(settings, s_l_set, s_c_set, s_w_inv, s_p_inv, a_b_set, a_w_set, a_c_set, gamma):\n",
        "    ## Calc trans pr matrices for s_w and s_p\n",
        "    # Save all combinations in a matrix pr_w_mat[s_w, s_w_next]\n",
        "    # your code\n",
        "\n",
        "    # Save all combinations in a matrix pr_p_mat[s_p, s_p_next]\n",
        "    # your code\n",
        "\n",
        "    ## Build the MDP\n",
        "    # your code\n",
        "\n",
        "    ## Optimize\n",
        "    # your code\n",
        "\n",
        "    ## Get results/policy dataframe\n",
        "    # your code\n",
        "\n",
        "    ## Plot results\n",
        "    dat = (\n",
        "        df_result\n",
        "            # >> mask((X.s_l == 0) | (X.s_l == 5) | (X.s_l == 10)) >> mask((X.s_c == -10) | (X.s_c == -2) | (X.s_c == 0) | (X.s_c == 2))   # mask(~X.imbalance)\n",
        "            >> mask(X.steady_pr > 0)\n",
        "    )\n",
        "    dat['a_c'] = pd.Categorical(dat['a_c'])  # Convert 'a_c' to a categorical variable for discrete scaling\n",
        "    # your code\n",
        "\n",
        "    return mdp, df_result"
      ],
      "metadata": {
        "id": "IKwNZFii5SXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test it using the same settings as before:"
      ],
      "metadata": {
        "id": "xET6cSO69BOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "settings = {\n",
        "    ## Problem parameters\n",
        "    'b_max': 10,  # battery capacity (MWh)\n",
        "    'p_max': 10,  # max power output wind (MW)\n",
        "    'c_max': 10 + 10,  # max commitment (MWh) # b_max + p_max\n",
        "    'w_cut_in': 3,  # cut in wind speed (m/s)\n",
        "    'w_rated': 12,  # rated wind speed (m/s)\n",
        "    'w_cut_out': 25,  # cut out wind speed (m/s)\n",
        "    'c_plus': 50,  # EUR/MWh\n",
        "    'c_minus': 50, # EUR/MWh\n",
        "}\n",
        "\n",
        "b_max = settings['b_max']\n",
        "p_max = settings['p_max']\n",
        "c_max = settings['c_max']\n",
        "w_cut_in = settings['w_cut_in']\n",
        "w_rated = settings['w_rated']\n",
        "w_cut_out = settings['w_cut_out']\n",
        "step_mwh = 2\n",
        "length_ms = 3\n",
        "length_p = 50\n",
        "min_p = -50\n",
        "max_p = 250\n",
        "\n",
        "## States variables\n",
        "s_l_set = generate_discrete_set(0, b_max, step_mwh)\n",
        "s_c_set = generate_discrete_set(-b_max, c_max, step_mwh)\n",
        "\n",
        "# s_w\n",
        "s_w_inv = generate_intervals(w_cut_in, w_rated, length_ms)\n",
        "add_interval(0, w_cut_in, s_w_inv)\n",
        "add_interval(w_rated, w_cut_out, s_w_inv)\n",
        "add_interval(w_cut_out, np.inf, s_w_inv)\n",
        "\n",
        "# s_p\n",
        "s_p_inv = generate_intervals(min_p, max_p - 150, length_p, add_inf = True, add_neg_inf = True)\n",
        "\n",
        "## Action variables\n",
        "a_b_set = generate_discrete_set(-b_max, b_max, step_mwh)\n",
        "a_w_set = generate_discrete_set(0, p_max, step_mwh)\n",
        "a_c_set = generate_discrete_set(-b_max, c_max, step_mwh)\n",
        "\n",
        "mdp, df_result = build_and_solve_mdp(settings, s_l_set, s_c_set, s_w_inv, s_p_inv, a_b_set, a_w_set, a_c_set, 0.9)"
      ],
      "metadata": {
        "id": "NhNOk_iL9fMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that it seems that the same action is taken for prices above/below zero. Hence, we may focus on small intervals here only. Moreover, not all possible commitments are used in states where the steady state probability is non-zero. This is due to the intervals chosen for wind speed not corresponding to all possible commitment values.\n",
        "\n",
        "Let us try to define different intervals for the price and wind:"
      ],
      "metadata": {
        "id": "7mHKTV1yNZq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make s_p_inv smaller\n",
        "print(\"Current prices:\")\n",
        "pprint(s_p_inv)\n",
        "s_p_inv = generate_intervals(-20, 5, 5, add_inf = True, add_neg_inf = True)\n",
        "print(\"New prices:\")\n",
        "pprint(s_p_inv)\n",
        "\n",
        "# Make new s_w_inv\n",
        "print(\"Commitment values:\", a_w_set)  # commitment values\n",
        "print(\"Commitment values used:\", dat['a_w'].unique())  # a_c values used\n",
        "e_prod = [energy(w) for w in s_w_inv['midpoints']] # possible energy production given wind intervals\n",
        "print(\"Possible energy production given wind intervals:\", e_prod)\n",
        "print(\"What we can sell:\", [get_left_or_right(a_w_set, e) for e in e_prod])  # what we can sell\n",
        "# pprint(s_w_inv)  # current wind speeds used\n",
        "val = [round(energy_inverse(e), 1) for e in a_w_set]  # wind speeds corresponding to a_w_set\n",
        "print(\"Wind speeds corresponding to a_w_set:\", val)\n",
        "# Modify s_w_inv\n",
        "s_w_inv = generate_intervals(0, w_cut_in, w_cut_in)\n",
        "for i, v in enumerate(val):\n",
        "    if i > 0:\n",
        "        add_interval(val[i-1], v, s_w_inv)\n",
        "# pprint(s_w_inv)\n",
        "add_interval(w_rated, w_cut_out, s_w_inv)\n",
        "add_interval(w_cut_out, np.inf, s_w_inv)\n",
        "print(\"New intervals:\")\n",
        "pprint(s_w_inv)"
      ],
      "metadata": {
        "id": "bIOu_AiuP3dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q12\n",
        "\n",
        "Solve the new model and comment on the results."
      ],
      "metadata": {
        "id": "YGPTkuzmP23w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "RG6lhIat9h1I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}