{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simon-mellergaard/RL/blob/main/RL/Exercises/10_td_pred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlrET7d3WOQ2"
      },
      "source": [
        "# Notebook pre steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0vEJAmyWKPb"
      },
      "outputs": [],
      "source": [
        "#@title Installations\n",
        "# install missing packages\n",
        "!pip install -q dfply\n",
        "\n",
        "from dfply import *\n",
        "from plotnine import *\n",
        "import numpy as np  # RNG and vector ops\n",
        "import pandas as pd  # tabular outputs\n",
        "from IPython.display import Markdown\n",
        "# import json\n",
        "# from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRfJ8WsMofFf",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title MDP class\n",
        "\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "class MDP:\n",
        "    \"\"\"\n",
        "    A class representing a Markov Decision Process (MDP) using defaultdict structures.\n",
        "\n",
        "    This implementation includes state management, action specification, transition\n",
        "    probabilities, rewards, policies, and iterative algorithms for policy and value iteration.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes an empty MDP with model and state values.\n",
        "        \"\"\"\n",
        "        self.model = defaultdict(lambda: {\"pi\": None, \"actions\": defaultdict(dict)})\n",
        "        self.v = defaultdict(float)\n",
        "\n",
        "    def add_state_space(self, states):\n",
        "        \"\"\"\n",
        "        Adds states to the MDP.\n",
        "\n",
        "        Args:\n",
        "            states (list): List of state identifiers (strings or convertible to strings).\n",
        "        \"\"\"\n",
        "        for state in states:\n",
        "            _ = self.model[str(state)]\n",
        "        self.set_state_value()\n",
        "\n",
        "    def add_action_space(self, state_str, actions):\n",
        "        \"\"\"\n",
        "        Adds actions to a given state. Note you have to update the action\n",
        "        afterwards using `add_action`.\n",
        "\n",
        "        Args:\n",
        "            state_str (str): The state identifier.\n",
        "            actions (list): List of action identifiers.\n",
        "        \"\"\"\n",
        "        if not isinstance(state_str, str):\n",
        "            raise ValueError(\"State is not a sting!\")\n",
        "        if isinstance(actions, str):\n",
        "            # If it's a string, put it in a list to treat it as a single item\n",
        "            actions = [actions]\n",
        "        for action in actions:\n",
        "            # Initialize the action dictionary with 'pr' and 'r' keys\n",
        "            self.model[state_str][\"actions\"][str(action)] = {\"pr\": {}, \"r\": None}\n",
        "\n",
        "    def add_action(self, state_str, action_str, reward, pr):\n",
        "        \"\"\"\n",
        "        Adds a transition action with reward and transition probabilities.\n",
        "\n",
        "        Args:\n",
        "            state_str (str): State from which the action is taken.\n",
        "            action_str (str): Action identifier.\n",
        "            reward (float): Expected reward for taking the action.\n",
        "            pr (dict): Transition probabilities as {next_state: probability}.\n",
        "        \"\"\"\n",
        "        ## remove keys with zero trans pr\n",
        "        keys_to_remove = [key for key, value in pr.items() if value == 0]\n",
        "        for key in keys_to_remove:\n",
        "            del pr[key]\n",
        "        self.model[state_str][\"actions\"][action_str] = {\"r\": reward, \"pr\": pr}\n",
        "\n",
        "    def check(self, delta = 10*np.spacing(np.float64(1))):\n",
        "        \"\"\"\n",
        "        Performs checks on the built MDP model.\n",
        "\n",
        "        Verifies that transition probabilities sum to approximately 1.0 for each\n",
        "        state-action pair and checks for rewards less than the high_neg_reward.\n",
        "        Prints warnings if any issues are found.\n",
        "\n",
        "        Args:\n",
        "            delta (float, optional): Tolerance for the sum of transition probabilities. Defaults to 1e-10.\n",
        "        \"\"\"\n",
        "        ok = True\n",
        "        # Check if transition pr of an action sum to one\n",
        "        for state_label, state_content in self.model.items():\n",
        "            for action_label, action_content in state_content[\"actions\"].items():\n",
        "                pr = action_content[\"pr\"]\n",
        "                pr_sum = np.sum(list(pr.values()))\n",
        "                absdiff = np.abs(1-pr_sum)\n",
        "                if absdiff >= delta:\n",
        "                    print(f\"Warning: Transition probabilities for action '{action_label}' in state '{state_label}' do not sum to 1.0. Diff is: {absdiff}\")\n",
        "                    ok = False\n",
        "\n",
        "        # Check if there are states with no actions\n",
        "        for state_label, state_content in self.model.items():\n",
        "            if len(state_content[\"actions\"]) == 0:\n",
        "                print(f\"Warning: State '{state_label}' has no actions.\")\n",
        "                ok = False\n",
        "\n",
        "        # Check if all action transitions are to a state\n",
        "        states = list(self.model.keys())\n",
        "        for state_label, state_content in self.model.items():\n",
        "            for action_label, action_content in state_content[\"actions\"].items():\n",
        "                if not all(key in self.model for key in action_content['pr'].keys()):\n",
        "                    print(f\"Warning: Action '{action_label}' in state '{state_label}' has a transition to a non-existing state.\")\n",
        "                    ok = False\n",
        "        if ok:\n",
        "            print(\"All checks passed!\")\n",
        "\n",
        "\n",
        "    def normalize(self):\n",
        "        \"\"\"\n",
        "        Normalizes the transition probabilities for each state-action pair.\n",
        "        \"\"\"\n",
        "        for state_label, state_content in self.model.items():\n",
        "            for action_label, action_content in state_content[\"actions\"].items():\n",
        "                pr = action_content[\"pr\"]\n",
        "                pr_sum = np.sum(list(pr.values()))\n",
        "                for next_state_label, prob in pr.items():\n",
        "                    pr[next_state_label] = prob / pr_sum\n",
        "                action_content[\"pr\"] = pr\n",
        "\n",
        "    def set_state_value(self, states=None, value=0):\n",
        "        \"\"\"\n",
        "        Initializes or updates the value of states.\n",
        "\n",
        "        Args:\n",
        "            states (list, optional): List of state identifiers. Defaults to all states.\n",
        "            value (float, optional): Value to assign. Defaults to 0.\n",
        "        \"\"\"\n",
        "        states = states or list(self.model.keys())\n",
        "        for state in states:\n",
        "            self.v[state] = value\n",
        "\n",
        "    def set_random_deterministic_policy(self):\n",
        "        \"\"\"\n",
        "        Sets a random deterministic policy for each state.\n",
        "        \"\"\"\n",
        "        for state in self.model:\n",
        "            actions = list(self.model[state][\"actions\"].keys())\n",
        "            if actions:\n",
        "                chosen_action = random.choice(actions)\n",
        "                self.model[state][\"pi\"] = {chosen_action: 1}\n",
        "\n",
        "    def set_deterministic_policy(self, state_actions):\n",
        "        \"\"\"\n",
        "        Sets a deterministic policy from a state-action mapping.\n",
        "\n",
        "        Args:\n",
        "            state_actions (dict): Mapping {state: action}.\n",
        "        \"\"\"\n",
        "        for state, action in state_actions.items():\n",
        "            self.model[state][\"pi\"] = {action: 1}\n",
        "\n",
        "    def set_policy(self, states, pi):\n",
        "        \"\"\"\n",
        "        Sets a stochastic or deterministic policy for a list of states.\n",
        "\n",
        "        Args:\n",
        "            states (list): List of states to assign the policy.\n",
        "            pi (dict): Policy as {action: probability}.\n",
        "        \"\"\"\n",
        "        for state in states:\n",
        "            self.model[state][\"pi\"] = pi.copy()\n",
        "\n",
        "    def get_state_keys(self):\n",
        "        \"\"\"\n",
        "        Returns the list of state identifiers.\n",
        "\n",
        "        Returns:\n",
        "            list: List of state keys.\n",
        "        \"\"\"\n",
        "        return list(self.model.keys())\n",
        "\n",
        "    def get_action_keys(self, state):\n",
        "        \"\"\"\n",
        "        Returns the action identifiers for a given state.\n",
        "\n",
        "        Args:\n",
        "            state (str): State identifier.\n",
        "\n",
        "        Returns:\n",
        "            list: List of action keys.\n",
        "        \"\"\"\n",
        "        return list(self.model[state][\"actions\"].keys())\n",
        "\n",
        "    def get_action_info(self, state):\n",
        "        \"\"\"\n",
        "        Gets reward and transition probabilities for each action in a state.\n",
        "\n",
        "        Args:\n",
        "            state (str): State identifier.\n",
        "\n",
        "        Returns:\n",
        "            dict: Action information.\n",
        "        \"\"\"\n",
        "        return dict(self.model[state][\"actions\"])\n",
        "\n",
        "    def get_reward(self, state, action):\n",
        "        \"\"\"\n",
        "        Returns the reward for a given state-action pair.\n",
        "\n",
        "        Args:\n",
        "            state (str): State identifier.\n",
        "            action (str): Action identifier.\n",
        "\n",
        "        Returns:\n",
        "            float: Reward value.\n",
        "        \"\"\"\n",
        "        return self.model[state][\"actions\"][action][\"r\"]\n",
        "\n",
        "    def get_mdp_info(self):\n",
        "        \"\"\"\n",
        "        Returns information about the MDP.\n",
        "\n",
        "        Returns:\n",
        "            dict: The info.\n",
        "        \"\"\"\n",
        "        res = {}\n",
        "        res[\"states\"] = len(self.model)\n",
        "        res[\"actions\"] = sum([len(self.model[k][\"actions\"]) for k in self.model.keys()])\n",
        "        return res\n",
        "\n",
        "    def get_policy(self, add_state_values = False):\n",
        "        \"\"\"\n",
        "        Retrieves the current policy.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Table of state, action, and probability.\n",
        "        \"\"\"\n",
        "        policy = []\n",
        "        for state in self.get_state_keys():\n",
        "            for action, prob in self.model[state][\"pi\"].items():\n",
        "                if not add_state_values:\n",
        "                    policy.append({\"state\": state, \"action\": action, \"pr\": prob})\n",
        "                else:\n",
        "                    policy.append({\"state\": state, \"action\": action, \"pr\": prob, \"v\": self.v[state]})\n",
        "        df = pd.DataFrame(policy)\n",
        "        df.set_index(\"state\")\n",
        "        return df\n",
        "\n",
        "    def get_state_values(self, states=None):\n",
        "        \"\"\"\n",
        "        Returns the current value of each state.\n",
        "\n",
        "        Args:\n",
        "            states (list, optional): Subset of states. Defaults to all.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Table of state values.\n",
        "        \"\"\"\n",
        "        states = states or list(self.v.keys())\n",
        "        return pd.DataFrame([{\"state\": s, \"v\": self.v[s]} for s in states])\n",
        "\n",
        "    def get_mdp_matrices(self, high_neg_reward = -100000):\n",
        "        \"\"\"\n",
        "        Returns transition probability and reward matrices.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                p_mat (list): List of transition probability matrices.\n",
        "                r_mat (ndarray): Reward matrix.\n",
        "                states (list): List of state identifiers.\n",
        "                actions (list): List of action identifiers.\n",
        "        \"\"\"\n",
        "        states = self.get_state_keys()\n",
        "        actions = set(\n",
        "            action for state in states for action in self.get_action_keys(state)\n",
        "        )\n",
        "        actions = list(actions)\n",
        "        actions.sort()\n",
        "        p_mat = [pd.DataFrame(0.0, index=states, columns=states) for _ in actions]\n",
        "        for df in p_mat:\n",
        "            np.fill_diagonal(df.values, 1) # set default to transition to same state (so illigal actions work)\n",
        "        r_mat = pd.DataFrame(high_neg_reward, index=states, columns=actions)\n",
        "        for state in states:\n",
        "            for action in self.get_action_keys(state):\n",
        "                p_mat[actions.index(action)].at[state, state] = 0  # reset to 0 again (since action is not illigal)\n",
        "                pr = self.model[state][\"actions\"][action][\"pr\"]\n",
        "                r = self.model[state][\"actions\"][action][\"r\"]\n",
        "                r_mat.at[state, action] = r\n",
        "                for next_state, prob in pr.items():\n",
        "                    p_mat[actions.index(action)].at[state, next_state] = prob\n",
        "        p_mat = [m.to_numpy() for m in p_mat]  # convert to matrices\n",
        "        r_mat = r_mat.to_numpy()\n",
        "        return p_mat, r_mat, states, actions\n",
        "\n",
        "    def save_mdp(self, path: str | Path):\n",
        "        \"\"\"\n",
        "        Saves the MDP to a JSON file.\n",
        "\n",
        "        Args:\n",
        "            path (str | Path): Path to the JSON file.\n",
        "        \"\"\"\n",
        "        path = Path(path)\n",
        "        with path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(self.model, f, indent=2, sort_keys=True, ensure_ascii=False)\n",
        "\n",
        "    def load_mdp(self, path: str | Path):\n",
        "        \"\"\"\n",
        "        Loads the MDP from a JSON file.\n",
        "\n",
        "        Args:\n",
        "            path (str | Path): Path to the JSON file.\n",
        "        \"\"\"\n",
        "        with Path(path).open(\"r\", encoding=\"utf-8\") as f:\n",
        "            self.model = json.load(f)\n",
        "        self.check()\n",
        "\n",
        "    def bellman_calc(self, gamma, state, action):\n",
        "        \"\"\"\n",
        "        Computes Bellman update for a given state-action pair.\n",
        "\n",
        "        Args:\n",
        "            gamma (float): Discount factor.\n",
        "            state (str): State identifier.\n",
        "            action (str): Action identifier.\n",
        "\n",
        "        Returns:\n",
        "            float: Updated value.\n",
        "        \"\"\"\n",
        "        pr = self.model[state][\"actions\"][action][\"pr\"]\n",
        "        reward = self.model[state][\"actions\"][action][\"r\"]\n",
        "        return reward + gamma * sum(pr[s] * self.v[s] for s in pr)\n",
        "\n",
        "    def policy_eval(self, gamma, theta=1e-5, max_iter=10000, reset=True):\n",
        "        \"\"\"\n",
        "        Iteratively evaluates the current policy.\n",
        "\n",
        "        Args:\n",
        "            gamma (float): Discount factor.\n",
        "            theta (float): Convergence threshold.\n",
        "            max_iter (int): Max iterations.\n",
        "            reset (bool): Whether to reset state values to 0.\n",
        "        \"\"\"\n",
        "        if reset:\n",
        "            self.set_state_value()\n",
        "        for _ in range(max_iter):\n",
        "            delta = 0\n",
        "            for state in self.model:\n",
        "                v_old = self.v[state]\n",
        "                pi = self.model[state][\"pi\"]\n",
        "                value = sum(pi[a] * self.bellman_calc(gamma, state, a) for a in pi)\n",
        "                self.v[state] = value\n",
        "                delta = max(delta, abs(v_old - value))\n",
        "            if delta < theta:\n",
        "                break\n",
        "        else:\n",
        "            print(f\"Policy evaluation stopped at max iterations: {max_iter}\")\n",
        "\n",
        "    def policy_iteration(self, gamma, theta=1e-5, max_iter_eval=10000, max_iter_policy=100):\n",
        "        \"\"\"\n",
        "        Performs policy iteration with evaluation and improvement steps.\n",
        "\n",
        "        Args:\n",
        "            gamma (float): Discount factor.\n",
        "            theta (float): Convergence threshold.\n",
        "            max_iter_eval (int): Max iterations during policy evaluation.\n",
        "            max_iter_policy (int): Max policy improvement steps.\n",
        "        \"\"\"\n",
        "        self.set_random_deterministic_policy()\n",
        "        for i in range(max_iter_policy):\n",
        "            self.policy_eval(gamma, theta, max_iter_eval, reset=False)\n",
        "            stable = True\n",
        "            for state in self.model:\n",
        "                old_action = next(iter(self.model[state][\"pi\"]))\n",
        "                best_val, best_action = float(\"-inf\"), None\n",
        "                for action in self.get_action_keys(state):\n",
        "                    val = self.bellman_calc(gamma, state, action)\n",
        "                    if val > best_val:\n",
        "                        best_val = val\n",
        "                        best_action = action\n",
        "                self.model[state][\"pi\"] = {best_action: 1}\n",
        "                if best_action != old_action:\n",
        "                    stable = False\n",
        "            if stable:\n",
        "                break\n",
        "        else:\n",
        "            print(f\"Policy iteration stopped at max iterations: {max_iter_policy}\")\n",
        "        print(f\"Policy iteration finished in {i + 1} iterations.\")\n",
        "\n",
        "    def value_iteration(self, gamma, theta=1e-5, max_iter=10000, reset=True):\n",
        "        \"\"\"\n",
        "        Performs value iteration algorithm.\n",
        "\n",
        "        Args:\n",
        "            gamma (float): Discount factor.\n",
        "            theta (float): Convergence threshold.\n",
        "            max_iter (int): Max number of iterations.\n",
        "            reset (bool): Whether to reinitialize state values.\n",
        "        \"\"\"\n",
        "        self.set_random_deterministic_policy()\n",
        "        if reset:\n",
        "            self.set_state_value()\n",
        "        for i in range(max_iter):\n",
        "            delta = 0\n",
        "            for state in self.model:\n",
        "                v_old = self.v[state]\n",
        "                best_val, best_action = float(\"-inf\"), None\n",
        "                for action in self.get_action_keys(state):\n",
        "                    val = self.bellman_calc(gamma, state, action)\n",
        "                    if val > best_val:\n",
        "                        best_val = val\n",
        "                        best_action = action\n",
        "                self.v[state] = best_val\n",
        "                self.model[state][\"pi\"] = {best_action: 1}\n",
        "                delta = max(delta, abs(v_old - best_val))\n",
        "            if delta < theta:\n",
        "                break\n",
        "        else:\n",
        "            print(f\"Value iteration stopped at max iterations: {max_iter}\")\n",
        "        print(f\"Value iteration finished in {i + 1} iterations.\")\n",
        "\n",
        "    def get_steady_state_pr(self, as_dataframe = True, tol=1e-8):\n",
        "        \"\"\"\n",
        "        Calculates the steady-state probabilities for the MDP under the optimal policy.\n",
        "\n",
        "        Args:\n",
        "            as_dataframe (bool): Whether to return the result as a DataFrame, or otherwise as an array.\n",
        "\n",
        "        Returns:\n",
        "            if as_dataframe:\n",
        "                pd.DataFrame: A DataFrame with states and their steady-state probabilities.\n",
        "            else:\n",
        "                ndarray: An array of steady-state probabilities.\n",
        "        \"\"\"\n",
        "        state_labels_to_index = {label: index for index, label in enumerate(self.get_state_keys())}\n",
        "        num_states = len(state_labels_to_index)\n",
        "        transition_matrix = np.zeros((num_states, num_states))\n",
        "        policy = self.get_policy()\n",
        "        policy['s_idx'] = policy['state'].map(state_labels_to_index)\n",
        "        policy = policy.set_index(['s_idx', 'action'])\n",
        "        # calc transition matrix\n",
        "        for s_label in self.get_state_keys():\n",
        "            s_idx = state_labels_to_index[s_label]\n",
        "            action_rows = policy.loc[s_idx]\n",
        "            for action, row in action_rows.iterrows():\n",
        "                pi = row['pr']\n",
        "                a = self.model[s_label]['actions'][action]\n",
        "                for s_next_label, prob in a['pr'].items():\n",
        "                    s_next_idx = state_labels_to_index[s_next_label]\n",
        "                    transition_matrix[s_idx, s_next_idx] += prob * pi\n",
        "\n",
        "        transition_matrix.sum(axis=1)\n",
        "\n",
        "        ## calc steady state pr\n",
        "        # # alternative 1\n",
        "        # eigenvalues, left_eigenvectors = np.linalg.eig(transition_matrix.T)\n",
        "        # # Find the eigenvalue closest to 1\n",
        "        # closest_eigenvalue_index = np.abs(eigenvalues - 1).argmin()\n",
        "        # # Extract the corresponding left eigenvector\n",
        "        # steady_state_vector = left_eigenvectors[:, closest_eigenvalue_index]\n",
        "        # # Ensure the eigenvector contains real values and take the real part\n",
        "        # steady_state_vector = np.real(steady_state_vector)\n",
        "        # # Normalize the vector to sum to 1\n",
        "        # steady_state_vector = steady_state_vector / np.sum(steady_state_vector)\n",
        "        # # Handle potential negative values due to numerical precision by taking absolute value\n",
        "        # steady_state_vector = np.abs(steady_state_vector)\n",
        "        # steady_state_vector = steady_state_vector / np.sum(steady_state_vector)\n",
        "        # # Verify that the sum of the steady-state probabilities is approximately 1\n",
        "        # print(\"Sum of steady-state probabilities:\", np.sum(steady_state_vector))\n",
        "        # # Verify that all probabilities are non-negative\n",
        "        # print(\"Minimum steady-state probability:\", np.min(steady_state_vector))\n",
        "        # steady = steady_state_vector\n",
        "\n",
        "        # Alternative 2\n",
        "        eigvals, eigvecs = np.linalg.eig(transition_matrix.T)\n",
        "        steady = np.real(eigvecs[:, np.isclose(eigvals, 1)])\n",
        "        steady = steady[:,0]\n",
        "        sum(steady)\n",
        "        steady = steady/steady.sum()\n",
        "\n",
        "        # # Alternative 3 (slow)\n",
        "        # # Solve (P^T - I) d^T = 0 with sum(d)=1 by replacing one equation with the normalization\n",
        "        # A = transition_matrix.T - np.eye(num_states)\n",
        "        # b = np.zeros(num_states)\n",
        "        # A[-1, :] = 1.0\n",
        "        # b[-1] = 1.0\n",
        "        # # Least-squares for robustness\n",
        "        # d, *_ = np.linalg.lstsq(A, b, rcond=None)\n",
        "        # # Clean numerical noise\n",
        "        # d = np.maximum(d, 0)\n",
        "        # d = d / d.sum()\n",
        "\n",
        "        # abs(steady - steady_state_vector) < 0.00000001\n",
        "        # abs(d - steady_state_vector) < 0.00000001\n",
        "        # abs(steady - d) < 0.00000001\n",
        "\n",
        "        if abs(sum(steady) - 1) > tol:\n",
        "            raise ValueError(\"Steady state probabilities do not sum to 1.\")\n",
        "\n",
        "        if as_dataframe:\n",
        "            policy.reset_index(inplace=True)\n",
        "            policy['steady_pr'] = [steady[s_idx] for s_idx in policy['s_idx']]\n",
        "            return policy\n",
        "        else:\n",
        "            return steady\n",
        "\n",
        "# self = mdp\n",
        "# mdp.get_mdp_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTjOLakAkCLV"
      },
      "source": [
        "# Temporal difference methods for prediction\n",
        "\n",
        "This notebook considers temporal difference (TD) learning one of the most fundamental concepts in reinforcement learning. TD learning is a combination of Monte Carlo (MC) and dynamic programming (DP) ideas. Like DP, TD update estimates based on other learned estimates, without waiting for a final outcome (bootstrap). That is, TD can learn on-line and do not need to wait until the whole sample-path is found. TD in general learn more efficiently than MC due to bootstrapping. In this module prediction using TD is considered.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqfijEcjk2bo"
      },
      "source": [
        "## A generic RL agent\n",
        "\n",
        "Let us now try to use RL and TD to estimate the value function for a policy. First, we define a generic RL agent that can be used for all environments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szh8HtKZl2n6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Generic RL agent\n",
        "\n",
        "import math  # math helpers\n",
        "import random  # tie-breaking choices\n",
        "from collections import defaultdict  # lazy nested dicts\n",
        "from typing import Optional, List, Dict, Any  # typing\n",
        "\n",
        "import numpy as np  # vector ops and RNG\n",
        "import pandas as pd  # tabular data\n",
        "\n",
        "class RLAgent:\n",
        "    \"\"\"\n",
        "    Tabular RL agent with:\n",
        "      - per-state action dictionaries {'q': value, 'n': visits}\n",
        "      - behavior policy pi (dict action->prob)\n",
        "      - state value v and state visit counter n\n",
        "\n",
        "    Uses defaultdict so states/actions can be created lazily.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        # model[state] = dict with keys:\n",
        "        #   'pi': policy dict(action->prob)\n",
        "        #   'v': state value\n",
        "        #   'n': state visit count\n",
        "        #   'actions': dict(action -> {'q': float, 'n': int})\n",
        "        self.model: Dict[str, Dict[str, Any]] = defaultdict(\n",
        "            lambda: { # dict for a state\n",
        "                \"pi\": None,     # policy probabilities\n",
        "                \"v\": float(\"nan\"),  # state value\n",
        "                \"n\": 0,         # state visits\n",
        "                \"actions\": defaultdict(lambda: {\"q\": 0.0, \"n\": 0}),  # actions\n",
        "            }\n",
        "        )\n",
        "\n",
        "    # ----------------------------- helpers ------------------------------------\n",
        "\n",
        "    def add_states(self, states: List[str]) -> None:\n",
        "        \"\"\"Force creation of states (defaultdict makes them auto-create).\"\"\"  # eager create\n",
        "        for s in states:\n",
        "            _ = self.model[str(s)]  # touch to ensure creation\n",
        "\n",
        "    def add_state_action(self, s: str, a: str) -> None:\n",
        "        \"\"\"Ensure a state and a specific action exist.\"\"\"  # lazy create\n",
        "        _ = self.model[str(s)][\"actions\"][str(a)]  # touch to ensure creation\n",
        "\n",
        "    def add_actions(self, s: str, actions: List[str]) -> None:\n",
        "        \"\"\"Force creation of actions in state s.\"\"\"  # batch add\n",
        "        for a in actions:\n",
        "            _ = self.model[str(s)][\"actions\"][str(a)]  # touch to ensure creation\n",
        "\n",
        "    def add_states_and_actions(self, df: pd.DataFrame) -> None:\n",
        "        \"\"\"Bulk add (state, action) pairs from DataFrame with columns 's' and 'a'.\"\"\"  # bulk\n",
        "        for s, a in zip(df[\"s\"].astype(str), df[\"a\"].astype(str)):\n",
        "            _ = self.model[s][\"actions\"][a]  # touch-create\n",
        "\n",
        "    # ----------------------------- setters ------------------------------------\n",
        "\n",
        "    def set_action_value(self, value: float = 0.0) -> None:\n",
        "        \"\"\"Set q(s,a) to constant for all actions.\"\"\"  # initializer/reset\n",
        "        for s in self.model:\n",
        "            for a in self.model[s][\"actions\"]:\n",
        "                self.model[s][\"actions\"][a][\"q\"] = float(value)  # assign\n",
        "\n",
        "    def set_state_value(self,\n",
        "                        states: Optional[List[str]] = None,\n",
        "                        value: float = 0.0) -> None:\n",
        "        \"\"\"Set v(s) for given states (all if None).\"\"\"  # V setter\n",
        "        states = states or list(self.model.keys())\n",
        "        for s in states:\n",
        "            self.model[s][\"v\"] = float(value)  # assign\n",
        "\n",
        "    def set_action_ctr_value(self, ctr_value: int = 0) -> None:\n",
        "        \"\"\"Set all action counters to ctr_value.\"\"\"  # reset N(s,a)\n",
        "        for s in self.model:\n",
        "            for a in self.model[s][\"actions\"]:\n",
        "                self.model[s][\"actions\"][a][\"n\"] = int(ctr_value)  # assign\n",
        "\n",
        "    def set_state_ctr_value(self, ctr_value: int = 0) -> None:\n",
        "        \"\"\"Set all state visit counters to ctr_value.\"\"\"  # reset N(s)\n",
        "        for s in self.model:\n",
        "            self.model[s][\"n\"] = int(ctr_value)  # assign\n",
        "\n",
        "    def set_action_value_single(self,\n",
        "                                s: str,\n",
        "                                a: str,\n",
        "                                value: float = 0.0,\n",
        "                                ctr_value: int = 0) -> None:\n",
        "        \"\"\"Set q(s,a) and n(s,a) for a single state-action.\"\"\"  # direct set\n",
        "        _ = self.model[str(s)][\"actions\"][str(a)]  # ensure exists\n",
        "        self.model[s][\"actions\"][a][\"q\"] = float(value)  # set q\n",
        "        self.model[s][\"actions\"][a][\"n\"] = int(ctr_value)  # set n\n",
        "\n",
        "    def set_random_eps_greedy_policy(self, eps: float) -> None:\n",
        "        \"\"\"Set π(s) to random ε-greedy (random greedy action per state).\"\"\"  # init π\n",
        "        for s in self.model:\n",
        "            actions = list(self.model[s][\"actions\"].keys())  # available actions\n",
        "            if not actions:\n",
        "                self.model[s][\"pi\"] = None  # no policy if no actions\n",
        "                continue  # skip\n",
        "            base = eps / len(actions)  # exploration mass\n",
        "            pi = {a: base for a in actions}  # flat\n",
        "            a_star = random.choice(actions)  # random greedy pick\n",
        "            pi[a_star] += 1.0 - eps  # exploitation bump\n",
        "            self.model[s][\"pi\"] = pi  # store\n",
        "\n",
        "    def set_eps_greedy_policy(self, eps: float, states: List[str] | str) -> None:\n",
        "        \"\"\"Make policy epsilon-greedy w.r.t current q-values.\"\"\"  # improve π\n",
        "        states_list = [states] if isinstance(states, str) else list(states)\n",
        "        for s in states_list:\n",
        "            actions = list(self.model[s][\"actions\"].keys())\n",
        "            if not actions:\n",
        "                self.model[s][\"pi\"] = None  # no policy\n",
        "                continue  # skip\n",
        "            q_vals = np.array([self.model[s][\"actions\"][a][\"q\"] for a in actions])  # q-vector\n",
        "            max_mask = q_vals == q_vals.max()  # tie mask\n",
        "            idx = int(np.random.choice(np.flatnonzero(max_mask)))  # random tie\n",
        "            base = eps / len(actions)  # exploration mass\n",
        "            pi = {a: base for a in actions}  # flat init\n",
        "            pi[actions[idx]] += 1.0 - eps  # greedy bump\n",
        "            self.model[s][\"pi\"] = pi  # assign\n",
        "\n",
        "    def set_greedy_policy(self, states: Optional[List[str]] = None) -> None:\n",
        "        \"\"\"Set greedy deterministic policy from q-values.\"\"\"  # greedy π\n",
        "        states = states or list(self.model.keys())\n",
        "        for s in states:\n",
        "            actions = list(self.model[s][\"actions\"].keys())\n",
        "            if not actions:\n",
        "                self.model[s][\"pi\"] = None  # no policy\n",
        "                continue  # skip\n",
        "            q_vals = [self.model[s][\"actions\"][a][\"q\"] for a in actions]  # q list\n",
        "            best = actions[int(np.argmax(q_vals))]  # greedy idx\n",
        "            self.model[s][\"pi\"] = {best: 1.0}  # point mass\n",
        "\n",
        "    def set_policy(self, states: List[str], pi: Dict[str, float]) -> None:\n",
        "        \"\"\"Set π(s) explicitly for each s in states (probabilities need not be normalized).\"\"\"  # explicit π\n",
        "        total = sum(pi.values())  # sum\n",
        "        norm = {a: (p / total) for a, p in pi.items()} if total > 0 else {a: 0.0 for a in pi}  # normalize\n",
        "        for s in states:\n",
        "            self.model[s][\"pi\"] = dict(norm)  # copy in\n",
        "\n",
        "    # ----------------------------- getters ------------------------------------\n",
        "\n",
        "    def get_info(self):\n",
        "        \"\"\"\n",
        "        Returns information about the agent.\n",
        "\n",
        "        Returns:\n",
        "            dict: The info.\n",
        "        \"\"\"\n",
        "        res = {}\n",
        "        res[\"states\"] = len(self.model)\n",
        "        res[\"actions\"] = sum([len(self.model[k][\"actions\"]) for k in self.model.keys()])\n",
        "        return res\n",
        "\n",
        "    def get_state_keys(self) -> List[str]:\n",
        "        return list(self.model.keys())  # all states\n",
        "\n",
        "    def get_action_keys(self, s: str) -> List[str]:\n",
        "        return list(self.model[s][\"actions\"].keys())  # actions in s\n",
        "\n",
        "    def get_action_info(self, s: str) -> Dict[str, Dict[str, Any]]:\n",
        "        \"\"\"Return shallow copy of the actions dict for state s.\"\"\"  # inspection\n",
        "        return dict(self.model[s][\"actions\"])  # shallow copy\n",
        "\n",
        "    def get_state_value_q(self, s: str) -> float:\n",
        "        \"\"\"Compute v_pi(s) = sum_a pi(a|s) q(s,a).\"\"\"  # V from Q & π\n",
        "        pi = self.model[s][\"pi\"] or {}  # distribution\n",
        "        return float(sum(p * self.model[s][\"actions\"][a][\"q\"]\n",
        "                         for a, p in pi.items()))  # dot product\n",
        "\n",
        "    def get_state_values(self,\n",
        "                         states: Optional[List[str]] = None) -> pd.DataFrame:\n",
        "        \"\"\"Return DataFrame of (state, v). Uses dfply::mutate if available.\"\"\"  # tidy\n",
        "        states = states or list(self.model.keys())\n",
        "        df = pd.DataFrame({\"state\": states})  # seed\n",
        "        return pd.DataFrame({\n",
        "            \"state\": states,\n",
        "            \"v\": [self.model[s][\"v\"] for s in states],\n",
        "        })  # basic\n",
        "\n",
        "    def get_policy(self, states: Optional[List[str]] = None) -> pd.DataFrame:\n",
        "        \"\"\"Return DataFrame (state, action, pr) for current π.\"\"\"  # tidy π\n",
        "        states = states or list(self.model.keys())\n",
        "        rows = []  # collect\n",
        "        for s in states:\n",
        "            pi = self.model[s][\"pi\"] or {}  # distribution\n",
        "            for a, p in pi.items():\n",
        "                rows.append({\"state\": s, \"action\": a, \"pr\": float(p)})  # row\n",
        "        return pd.DataFrame(rows)  # frame\n",
        "\n",
        "    def get_state_action_q_mat(self) -> pd.DataFrame:\n",
        "        \"\"\"Return wide Q-matrix DataFrame (rows=states, cols=actions).\"\"\"  # matrix\n",
        "        states = list(self.model.keys())  # rows\n",
        "        actions = sorted({a for s in states for a in self.model[s][\"actions\"].keys()})  # unique cols\n",
        "        mat = pd.DataFrame(np.nan, index=states, columns=actions)  # init\n",
        "        for s in states:\n",
        "            for a, rec in self.model[s][\"actions\"].items():\n",
        "                mat.loc[s, a] = rec[\"q\"]  # fill\n",
        "        return mat  # matrix\n",
        "\n",
        "    def get_action_values(self,\n",
        "                          states: Optional[List[str]] = None) -> pd.DataFrame:\n",
        "        \"\"\"Return long-form DataFrame of q-values and counts.\"\"\"  # tidy Q\n",
        "        states = [states] if isinstance(states, str) else states\n",
        "        states = states or list(self.model.keys())\n",
        "        rows = []\n",
        "        for s in states:\n",
        "            for a, info in self.model[s][\"actions\"].items():\n",
        "                rows.append({\n",
        "                    \"state\": s,\n",
        "                    \"action\": a,\n",
        "                    \"q\": info[\"q\"],\n",
        "                    \"n\": info[\"n\"],\n",
        "                })\n",
        "        return pd.DataFrame(rows)  # frame\n",
        "\n",
        "    # ----------------------------- action selection ---------------------------\n",
        "\n",
        "    def get_action_ucb(self, s: str, coeff: float = 1.0) -> Optional[str]:\n",
        "        \"\"\"UCB1-like selection; updates n(s) and n(s,a).\"\"\"  # UCB\n",
        "        actions = list(self.model[s][\"actions\"].keys())  # available\n",
        "        if not actions:\n",
        "            return None  # no action\n",
        "        self.model[s][\"n\"] += 1  # visit state\n",
        "        qv = np.array([self.model[s][\"actions\"][a][\"q\"] for a in actions], dtype=float)  # q\n",
        "        na = np.array([max(1, self.model[s][\"actions\"][a][\"n\"]) for a in actions], dtype=float)  # counts\n",
        "        ns = float(self.model[s][\"n\"])  # state count\n",
        "        bonus = coeff * np.sqrt(np.log(ns + 1e-4) / na)  # exploration term\n",
        "        idx = int(np.argmax(qv + bonus))  # argmax\n",
        "        a = actions[idx]  # pick\n",
        "        self.model[s][\"actions\"][a][\"n\"] += 1  # visit action\n",
        "        return a  # chosen\n",
        "\n",
        "    def get_action_eg(self, s: str, eps: float) -> str:\n",
        "        \"\"\"Epsilon-greedy action selection (increments counters).\"\"\"  # ε-greedy\n",
        "        self.model[s][\"n\"] += 1  # visit state\n",
        "        actions = list(self.model[s][\"actions\"].keys())  # list\n",
        "        q = np.array([self.model[s][\"actions\"][a][\"q\"] for a in actions])  # q-values\n",
        "        max_mask = q == q.max()  # ties\n",
        "        idx_greedy = int(np.random.choice(np.flatnonzero(max_mask)))  # random tie\n",
        "        probs = np.full(len(actions), eps / len(actions), dtype=float)  # base mass\n",
        "        probs[idx_greedy] += 1.0 - eps  # greedy bump\n",
        "        idx = int(np.random.choice(np.arange(len(actions)), p=probs))  # sample\n",
        "        a = actions[idx]  # chosen action\n",
        "        self.model[s][\"actions\"][a][\"n\"] += 1  # visit action\n",
        "        return a  # return\n",
        "\n",
        "    def get_action_pi(self, s: str) -> Optional[str]:\n",
        "        \"\"\"Sample an action from stored pi(a|s) (increments counters).\"\"\"  # sample π\n",
        "        self.model[s][\"n\"] += 1  # visit state\n",
        "        pi = self.model[s][\"pi\"] or {}  # distribution\n",
        "        if not pi:\n",
        "            return None  # no policy\n",
        "        actions, probs = zip(*pi.items())  # unpack\n",
        "        probs = np.array(probs, dtype=float)  # array\n",
        "        probs /= probs.sum() if probs.sum() > 0 else 1.0  # normalize\n",
        "        a = str(np.random.choice(list(actions), p=probs))  # draw\n",
        "        self.model[s][\"actions\"][a][\"n\"] += 1  # visit action\n",
        "        return a  # chosen\n",
        "\n",
        "    def get_max_action_value(self, s: str) -> float:\n",
        "        \"\"\"Return max_a Q(s,a).\"\"\"  # convenience\n",
        "        q = [rec[\"q\"] for rec in self.model[s][\"actions\"].values()]  # list\n",
        "        return float(max(q)) if q else float(\"nan\")  # handle empty\n",
        "\n",
        "    def get_exp_action_value(self, s: str) -> float:\n",
        "        \"\"\"Return E_{a~π}[Q(s,a)] under current π(s).\"\"\"  # expectation\n",
        "        pi = self.model[s][\"pi\"] or {}  # distribution\n",
        "        return float(sum(p * self.model[s][\"actions\"][a][\"q\"] for a, p in pi.items()))  # dot\n",
        "\n",
        "    # ----------------------------- learning -----------------------------------\n",
        "\n",
        "    def policy_eval_td0(self,\n",
        "                        env: Any,\n",
        "                        gamma: float = 1.0,\n",
        "                        alpha: float = 0.1,\n",
        "                        max_e: int = 1000,\n",
        "                        max_el: int = 10000,\n",
        "                        reset: bool = True,\n",
        "                        states: Optional[List[str]] = None) -> None:\n",
        "        \"\"\"\n",
        "        TD(0) policy evaluation of V(s). The environment used must implement:\n",
        "        get_time_step_data(s, a) -> {'r': float, 'sN': next_state or None (terminal)}.\n",
        "\n",
        "        Args:\n",
        "            env: Environment with get_time_step_data method.\n",
        "            gamma: The discount factor.\n",
        "            alpha: Step-size parameter\n",
        "            max_e: Maximum number of iterations (episodes)\n",
        "            max_el: Maximum episode length.\n",
        "            reset: Reset action-values, state and action counters to 0.\n",
        "            states: Starting states. For each iteration, generate\n",
        "                an episode for each state. If `None` uses all states.\n",
        "        \"\"\"\n",
        "        if reset:\n",
        "            self.set_state_value(value=0.0)  # V=0\n",
        "        starts = states or self.get_state_keys()  # candidate starts\n",
        "        for _ in range(max_e):  # episodes\n",
        "            s = random.choice(starts)  # start\n",
        "            for i in range(max_el):  # steps\n",
        "                a = self.get_action_pi(s)  # act under π\n",
        "                if a is None:  # no policy\n",
        "                    break  # abort\n",
        "                dat = env.get_time_step_data(s, a)  # env step\n",
        "                r, sN = dat[\"r\"], dat[\"sN\"]  # unpack\n",
        "                if sN is None:  # terminal\n",
        "                    break  # end\n",
        "                old_v = self.model[s][\"v\"]  # current V\n",
        "                td_target = r + gamma * self.model[sN][\"v\"]  # target\n",
        "                self.model[s][\"v\"] = old_v + alpha * (td_target - old_v)  # update\n",
        "                s = sN  # advance\n",
        "            if i + 1 == max_el:  # cap hit\n",
        "                break  # stop\n",
        "\n",
        "    def policy_eval_mc(self,\n",
        "                       env: Any,\n",
        "                       gamma: float = 1.0,\n",
        "                       theta: float = 0.1,\n",
        "                       min_ite: int = 100,\n",
        "                       max_ite: int = 2000,\n",
        "                       reset: bool = True,\n",
        "                       states: Optional[List[str]] = None,\n",
        "                       verbose: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        Every-visit Monte Carlo evaluation of V(s).\n",
        "        Env must implement: get_episode_pi(agent, s0) -> DataFrame with columns ['s','a','r'].  # contract\n",
        "        \"\"\"  # doc\n",
        "        if reset:\n",
        "            self.set_state_value(value=0.0)  # V=0\n",
        "            self.set_action_ctr_value(0)  # reset N(s,a)\n",
        "            self.set_state_ctr_value(0)  # reset N(s)\n",
        "        starts = states or self.get_state_keys()  # start set\n",
        "        for ite in range(1, max_ite + 1):  # iterations\n",
        "            delta = 0.0  # max change\n",
        "            for s0 in starts:  # episode per start\n",
        "                df = env.get_episode_pi(self, s0)  # generate under π\n",
        "                if df is None or len(df) == 0:\n",
        "                    continue  # skip\n",
        "                if verbose:\n",
        "                    df['g'] = np.nan\n",
        "                    df['n_s'] = np.nan\n",
        "                    df['old_v'] = np.nan\n",
        "                    df['new_v'] = np.nan\n",
        "                g = 0.0  # return accumulator\n",
        "                for i in range(len(df) - 1, -1, -1):  # reverse pass\n",
        "                    s = str(df.iloc[i][\"s\"])  # state\n",
        "                    r = float(df.iloc[i][\"r\"])  # reward\n",
        "                    g = r + gamma * g  # return update\n",
        "                    n_s = max(1, self.model[s][\"n\"])  # denom\n",
        "                    old_v = self.model[s][\"v\"]  # prev\n",
        "                    step = 1.0 / n_s  # 1/N schedule\n",
        "                    self.model[s][\"v\"] = old_v + step * (g - old_v)  # update\n",
        "                    delta = max(delta, abs(old_v - self.model[s][\"v\"]))  # track\n",
        "                    if verbose:\n",
        "                        df.at[i,\"g\"] = g\n",
        "                        df.at[i,\"n_s\"] = n_s\n",
        "                        df.at[i,\"old_v\"] = old_v\n",
        "                        df.at[i,\"new_v\"] = self.model[s][\"v\"]\n",
        "                if verbose:\n",
        "                    print(\"Episode:\")\n",
        "                    print(df)  # trace\n",
        "            if delta < theta and ite >= min_ite:  # convergence\n",
        "                break  # stop\n",
        "        if ite == max_ite:\n",
        "            print(f\"Policy eval algorithm stopped at max iterations allowed: {max_ite}\")  # warn\n",
        "        print(f\"Policy eval algorithm finished in {ite} iterations.\")  # info\n",
        "\n",
        "    def gpi_on_policy_mc(self,\n",
        "                         env: Any,\n",
        "                         gamma: float = 1.0,\n",
        "                         theta: float = 0.1,\n",
        "                         min_ite: int = 100,\n",
        "                         max_ite: int = 1000,\n",
        "                         reset: bool = True,\n",
        "                         states: Optional[List[str]] = None,\n",
        "                         eps: float = 0.1,\n",
        "                         verbose: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        On-policy GPI via Every-Visit MC control on Q(s,a).\n",
        "        Env must implement: get_episode(agent, s0, eps) -> DataFrame ['s','a','r']\n",
        "        and update visit counters.\n",
        "\n",
        "        Args:\n",
        "            env (Any): Environment with get_episode method.\n",
        "            gamma (float, optional): Discount factor. Defaults to 1.0.\n",
        "            theta (float, optional): Convergence threshold. Defaults to 0.1.\n",
        "            min_ite (int, optional): Minimum number of iterations. Defaults to 100.\n",
        "            max_ite (int, optional): Maximum number of iterations. Defaults to 1000.\n",
        "            reset (bool, optional): Reset action-values, state and action counters to 0.\n",
        "            states (list, optional): Starting states. For each iteration, generate\n",
        "                an episiode for each state. If `None uses all states.\n",
        "            eps (float, optional): Epsilon for eps-greedy policy. Defaults to 0.1.\n",
        "            verbose (bool, optional): Print episode info. Defaults to False.\n",
        "        \"\"\"\n",
        "        if reset:\n",
        "            self.set_action_value(0.0)  # Q=0\n",
        "            self.set_action_ctr_value(0)  # reset N(s,a)\n",
        "            self.set_state_ctr_value(0)  # reset N(s)\n",
        "        self.set_eps_greedy_policy(eps, self.get_state_keys())  # init π_ε\n",
        "        starts = states or self.get_state_keys()  # start pool\n",
        "        for ite in range(1, max_ite + 1):  # iterations\n",
        "            delta = 0.0  # track |ΔV|\n",
        "            for s0 in starts:  # episode per start\n",
        "                df = env.get_episode(self, s0, eps)  # behavior inside env\n",
        "                if verbose:\n",
        "                    df['g'] = np.nan\n",
        "                    df['n_sa'] = np.nan\n",
        "                    df['old_q'] = np.nan\n",
        "                    df['step'] = np.nan\n",
        "                    df['new_q'] = np.nan\n",
        "                    df['old_v'] = np.nan\n",
        "                    df['new_v'] = np.nan\n",
        "                if df is None or len(df) == 0:\n",
        "                    continue  # skip\n",
        "                g = 0.0  # return\n",
        "                for i in range(len(df) - 1, -1, -1):  # reverse sweep\n",
        "                    s = str(df.iloc[i][\"s\"])  # state\n",
        "                    a = str(df.iloc[i][\"a\"])  # action\n",
        "                    r = float(df.iloc[i][\"r\"])  # reward\n",
        "                    g = r + gamma * g  # return update\n",
        "                    # step size: (1 / n_sa) ** 0.5 as in R  # schedule\n",
        "                    n_sa = max(1, self.model[s][\"actions\"][a][\"n\"])  # visits\n",
        "                    old_q = self.model[s][\"actions\"][a][\"q\"]  # prev Q\n",
        "                    old_v = self.get_state_value_q(s)  # V before update\n",
        "                    step = (1.0 / n_sa) ** 0.5  # step-size\n",
        "                    new_q = old_q + step * (g - old_q)  # MC update\n",
        "                    self.model[s][\"actions\"][a][\"q\"] = new_q  # MC update\n",
        "                    self.set_eps_greedy_policy(eps, [s])  # improve π(s)\n",
        "                    new_v = self.get_state_value_q(s)  # V after\n",
        "                    delta = max(delta, abs(old_v - new_v))  # track\n",
        "                    if verbose:\n",
        "                        df.at[i,\"g\"] = g\n",
        "                        df.at[i,\"n_sa\"] = n_sa\n",
        "                        df.at[i,\"old_q\"] = old_q\n",
        "                        df.at[i,\"step\"] = step\n",
        "                        df.at[i,\"new_q\"] = new_q\n",
        "                        df.at[i,\"old_v\"] = old_v\n",
        "                        df.at[i,\"new_v\"] = new_v\n",
        "                if verbose:\n",
        "                    print(\"Episode:\")\n",
        "                    print(df)\n",
        "\n",
        "            if delta < theta and ite >= min_ite:\n",
        "                break  # stop\n",
        "        if ite == max_ite:\n",
        "            print(f\"GPI algorithm stopped at max iterations allowed: {max_ite}\")\n",
        "        print(f\"GPI algorithm finished in {ite} iterations.\")  # info\n",
        "\n",
        "    def gpi_on_policy_sarsa(self,\n",
        "                            env: Any,\n",
        "                            gamma: float = 1.0,\n",
        "                            max_e: int = 1000,\n",
        "                            max_el: int = 10000,\n",
        "                            reset: bool = True,\n",
        "                            states: Optional[List[str]] = None,\n",
        "                            eps: float = 0.1,\n",
        "                            alpha: float = 0.1,\n",
        "                            verbose: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        On-policy SARSA with fixed α.\n",
        "        Env must implement: get_time_step_data(s, a) -> {'r': float, 'sN': next_state or None}.  # contract\n",
        "        \"\"\"  # doc\n",
        "        if reset:\n",
        "            self.set_action_value(0.0)  # Q=0\n",
        "            self.set_action_ctr_value(0)  # counts\n",
        "        self.set_eps_greedy_policy(eps, self.get_state_keys())  # init π_ε\n",
        "        starts = states or self.get_state_keys()  # start pool\n",
        "        for _ in range(max_e):  # episodes\n",
        "            s = random.choice(starts)  # pick start\n",
        "            a = self.get_action_pi(s)  # first action under π\n",
        "            for i in range(max_el):  # steps\n",
        "                if a is None:\n",
        "                    break  # no action available\n",
        "                dat = env.get_time_step_data(s, a)  # env step\n",
        "                r, sN = dat[\"r\"], dat[\"sN\"]  # unpack\n",
        "                if sN is None:\n",
        "                    break  # terminal\n",
        "                aN = self.get_action_pi(sN)  # next action\n",
        "                old_q = self.model[s][\"actions\"][a][\"q\"]  # current\n",
        "                target = r + gamma * (self.model[sN][\"actions\"][aN][\"q\"] if aN is not None else 0.0)  # SARSA target\n",
        "                self.model[s][\"actions\"][a][\"q\"] = old_q + alpha * (target - old_q)  # update\n",
        "                if verbose:\n",
        "                    print(f\"(s,a,r,sN,aN)=({s},{a},{r},{sN},{aN}) oldQ={old_q} newQ={self.model[s]['actions'][a]['q']}\")  # trace\n",
        "                self.set_eps_greedy_policy(eps, [s])  # local improve\n",
        "                s, a = sN, aN  # advance\n",
        "            if i + 1 == max_el:\n",
        "                break  # cap hit\n",
        "        print(\"GPI/SARSA finished.\")  # info\n",
        "\n",
        "    def gpi_off_policy_q_learning(self,\n",
        "                                  env: Any,\n",
        "                                  gamma: float = 1.0,\n",
        "                                  max_e: int = 1000,\n",
        "                                  max_el: int = 10000,\n",
        "                                  reset: bool = True,\n",
        "                                  states: Optional[List[str]] = None,\n",
        "                                  eps: float = 0.1,\n",
        "                                  alpha: float = 0.1,\n",
        "                                  verbose: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        Off-policy Q-learning with behavior π_ε and greedy target.  # control\n",
        "        \"\"\"  # doc\n",
        "        if reset:\n",
        "            self.set_action_value(0.0)  # Q=0\n",
        "            self.set_action_ctr_value(0)  # counts\n",
        "        self.set_eps_greedy_policy(eps, self.get_state_keys())  # behavior π_ε\n",
        "        starts = states or self.get_state_keys()  # start pool\n",
        "        for _ in range(max_e):  # episodes\n",
        "            s = random.choice(starts)  # start\n",
        "            for i in range(max_el):  # steps\n",
        "                a = self.get_action_pi(s)  # behavior action\n",
        "                if a is None:\n",
        "                    break  # no action\n",
        "                dat = env.get_time_step_data(s, a)  # env step\n",
        "                r, sN = dat[\"r\"], dat[\"sN\"]  # unpack\n",
        "                if sN is None:\n",
        "                    break  # terminal\n",
        "                q_next = [rec[\"q\"] for rec in self.model[sN][\"actions\"].values()]  # next Qs\n",
        "                max_q = max(q_next) if q_next else 0.0  # greedy target\n",
        "                old_q = self.model[s][\"actions\"][a][\"q\"]  # current\n",
        "                self.model[s][\"actions\"][a][\"q\"] = old_q + alpha * (r + gamma * max_q - old_q)  # update\n",
        "                if verbose:\n",
        "                    print(f\"(s,a,r,sN)=({s},{a},{r},{sN}) oldQ={old_q} maxQ={max_q} newQ={self.model[s]['actions'][a]['q']}\")  # trace\n",
        "                self.set_eps_greedy_policy(eps, [s])  # refresh behavior at s\n",
        "                s = sN  # advance\n",
        "            if i + 1 == max_el:\n",
        "                break  # cap\n",
        "        self.set_greedy_policy()  # finalize with greedy π\n",
        "        print(\"GPI/Q-learning finished.\")  # info\n",
        "\n",
        "    def gpi_on_policy_exp_sarsa(self,\n",
        "                                env: Any,\n",
        "                                gamma: float = 1.0,\n",
        "                                max_e: int = 1000,\n",
        "                                max_el: int = 10000,\n",
        "                                reset: bool = True,\n",
        "                                states: Optional[List[str]] = None,\n",
        "                                eps: float = 0.1,\n",
        "                                alpha: float = 0.1,\n",
        "                                verbose: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        On-policy Expected SARSA with fixed α.  # control\n",
        "        \"\"\"  # doc\n",
        "        if reset:\n",
        "            self.set_action_value(0.0)  # Q=0\n",
        "            self.set_action_ctr_value(0)  # counts\n",
        "        self.set_eps_greedy_policy(eps, self.get_state_keys())  # init π_ε\n",
        "        starts = states or self.get_state_keys()  # start pool\n",
        "        for _ in range(max_e):  # episodes\n",
        "            s = random.choice(starts)  # start\n",
        "            for i in range(max_el):  # steps\n",
        "                a = self.get_action_pi(s)  # act under π\n",
        "                if a is None:\n",
        "                    break  # no action\n",
        "                dat = env.get_time_step_data(s, a)  # env step\n",
        "                r, sN = dat[\"r\"], dat[\"sN\"]  # unpack\n",
        "                if sN is None:\n",
        "                    break  # terminal\n",
        "                exp_q = self.get_exp_action_value(sN)  # expectation under π(sN)\n",
        "                old_q = self.model[s][\"actions\"][a][\"q\"]  # current\n",
        "                self.model[s][\"actions\"][a][\"q\"] = old_q + alpha * (r + gamma * exp_q - old_q)  # update\n",
        "                if verbose:\n",
        "                    print(f\"(s,a,r,sN)=({s},{a},{r},{sN}) oldQ={old_q} expQ={exp_q} newQ={self.model[s]['actions'][a]['q']}\")  # trace\n",
        "                self.set_eps_greedy_policy(eps, [s])  # improve behavior at s\n",
        "                s = sN  # advance\n",
        "            if i + 1 == max_el:\n",
        "                break  # cap\n",
        "        print(\"GPI/Expected-SARSA finished.\")  # info"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we need to pay attention to `policy_eval_td0` that is an implementation of the [TD(0) prediction algorithm](https://bss-osca.github.io/rl/10_td-pred.html#fig-td0-pred-alg)."
      ],
      "metadata": {
        "id": "HeFrNKhq9ewk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elsnCKZDH9ep"
      },
      "source": [
        "## Exercises\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Exercise - A random walk\n",
        "\n",
        "Consider [Exercise 10.6.1](https://bss-osca.github.io/rl/10_td-pred.html#sec-ex-td-pred-random).\n",
        "\n"
      ],
      "metadata": {
        "id": "1BzUCPQxstvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q1\n",
        "\n",
        "Formulate the MDP model and calculate the state-value $v_\\pi$ for each state using the [Bellman equations](https://bss-osca.github.io/rl/08_dp.html#eq-bm-pol-eval). Hint: there is no need to code this. Just solve the Bellman equations for state 2-6.\n",
        "\n",
        "<details>\n",
        "   <summary>Solution (don't look too early)</summary>\n",
        "   The state space is $S = \\{ 1, 2, \\ldots, 6, 7 \\}$ with $A(s) = \\{ \\text{left}, \\text{right}\\}$ (transition to the neighbour states) except for terminating states (1 and 7) which only have an action with transition to itself. Rewards are deterministic $R = \\{0, 1\\}$ which also holds for the transition probabilities. The state-value can be found using the Bellman equations\n",
        "   \n",
        "   $$v_\\pi(s) = \\sum_{a \\in A}\\pi(a | s)\\left( r(s,a) + \\gamma\\sum_{s' \\in S} p(s' | s, a) v_\\pi(s')\\right),$$\n",
        "   \n",
        "   which becomes (with the state-values equal to 0 for the terminating states):\n",
        "   \n",
        "   $$\n",
        "    \\begin{align}\n",
        "    v_\\pi(2) &= 0.5v_\\pi(1) + 0.5v_\\pi(3) = 0.5v_\\pi(3) \\\\\n",
        "    v_\\pi(3) &= 0.5v_\\pi(2) + 0.5v_\\pi( 4 ) \\\\\n",
        "    v_\\pi( 4 ) &= 0.5v_\\pi(3) + 0.5v_\\pi(5) \\\\\n",
        "    v_\\pi(5) &= 0.5v_\\pi( 4 ) + 0.5v_\\pi(6) \\\\\n",
        "    v_\\pi(6) &= 0.5v_\\pi(5) + 0.5(1 + v_\\pi(7)) = 0.5v_\\pi(5) + 0.5\\\\\n",
        "    \\end{align}\n",
        "   $$\n",
        "   \n",
        "   Solving the equations gives state-values $\\frac{1}{6}, \\frac{2}{6}, \\frac{3}{6}, \\frac{4}{6}$ and $\\frac{5}{6}$ for 2-6, respectively.\n",
        "</details>"
      ],
      "metadata": {
        "id": "0rh9O3_0tgAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q2\n",
        "\n",
        "Consider the code below, which builds and sets the policy."
      ],
      "metadata": {
        "id": "bbOfHZYjGc0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Build the MDP ---\n",
        "mdp = MDP()\n",
        "\n",
        "# add state keys \"1\"..\"7\"\n",
        "mdp.add_state_space([str(i) for i in range(1, 8)])\n",
        "\n",
        "# add action spaces\n",
        "states = [str(i) for i in range(2, 7)]  # \"2\"..\"6\"\n",
        "for s in states:\n",
        "    mdp.add_action_space(s, [\"left\", \"right\"])\n",
        "mdp.add_action_space(\"1\", [\"dummy\"])\n",
        "mdp.add_action_space(\"7\", [\"dummy\"])\n",
        "\n",
        "# transitions & rewards\n",
        "# right moves\n",
        "for s in range(2, 6):  # 2..5\n",
        "    mdp.add_action(str(s), \"right\", reward=0, pr={str(s + 1): 1.0})\n",
        "mdp.add_action(\"6\", \"right\", reward=1, pr={\"7\": 1.0})  # terminal reward on reaching 7\n",
        "\n",
        "# left moves\n",
        "for s in range(2, 7):  # 2..6\n",
        "    mdp.add_action(str(s), \"left\", reward=0, pr={str(s - 1): 1.0})\n",
        "\n",
        "# dummy self-loops at the ends\n",
        "mdp.add_action(\"1\", \"dummy\", reward=0, pr={\"1\": 1.0})\n",
        "mdp.add_action(\"7\", \"dummy\", reward=0, pr={\"7\": 1.0})\n",
        "\n",
        "# inspect a few states\n",
        "print(\"Actions in 1:\", mdp.get_action_info(\"1\"))\n",
        "print(\"Actions in 2:\", mdp.get_action_info(\"2\"))\n",
        "print(\"Actions in 6:\", mdp.get_action_info(\"6\"))\n",
        "print(\"Actions in 7:\", mdp.get_action_info(\"7\"))\n",
        "\n",
        "# --- Set the policy ---\n",
        "# middle states: 50/50 left/right\n",
        "pi_mid = {\"left\": 0.5, \"right\": 0.5}\n",
        "mdp.set_policy(states, pi_mid)\n",
        "\n",
        "# terminal ends: always dummy\n",
        "pi_ends = {\"dummy\": 1.0}\n",
        "mdp.set_policy([\"1\", \"7\"], pi_ends)\n",
        "\n",
        "print(\"\\nPolicy:\")\n",
        "display(mdp.get_policy())\n",
        "\n"
      ],
      "metadata": {
        "id": "e3tTzSoYG169"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to understand what happens, evaluate the policy and check if the results are the same as found in Question 1."
      ],
      "metadata": {
        "id": "bcxOGaz7EAtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "# --- Evaluate the policy ---\n",
        "mdp.policy_eval(gamma=1.0)  # undiscounted\n",
        "res = mdp.get_state_values()\n",
        "# Add values from Q1\n",
        "res['v_mdp'] = 0\n",
        "res.set_index('state', inplace=True)\n",
        "res.loc['2', 'v_mdp'] = 1/6\n",
        "res.loc['3', 'v_mdp'] = 2/6\n",
        "res.loc['4', 'v_mdp'] = 3/6\n",
        "res.loc['5', 'v_mdp'] = 4/6\n",
        "res.loc['6', 'v_mdp'] = 5/6\n",
        "print(\"\\nState values:\")\n",
        "display(res)"
      ],
      "metadata": {
        "id": "YSvnFa4fJPCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q3\n",
        "\n",
        "Consider an episode with sequence $S_0, R_1, S_1, R_2, S_2, R_3, S_3 = 4, 0, 3, 0, 2, 0, 1$. Let the initial state-value estimates of states 2-6 be 0.5 and update the state-values using TD(0) with $\\alpha = 0.1$. It appears that only $V(2)$ changed. Why was only the estimate for this one state changed? By exactly how much was it changed? Hint: There is no need to code this. Just solve equations.\n",
        "\n",
        "*Add your solution*"
      ],
      "metadata": {
        "id": "GbbnH5L6GhLj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q4\n",
        "\n",
        "Consider the TD(0) prediction algorithm with $\\alpha = 0.1$. Make a plot of the state-value estimate (y-axis) given state 2-6 (x-axis) for TD(0) running for 1, 10 and 100 episodes. You may use the code below as a starting point.\n",
        "\n",
        "First, we need an environment representing the problem\n",
        "\n"
      ],
      "metadata": {
        "id": "m1h5wq7EGhlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title RLEnvRandom\n",
        "\n",
        "from typing import Optional, List, Dict, Any\n",
        "import pandas as pd\n",
        "\n",
        "class RLEnvRandom:\n",
        "    \"\"\"\n",
        "    RL environment for the 1–7 chain:\n",
        "      - States: \"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"\n",
        "      - Actions in 2–6: \"left\",\"right\"\n",
        "      - Actions in 1 and 7: \"dummy\"\n",
        "      - Reward 1 only for (s=\"6\", a=\"right\") transitioning to \"7\"; otherwise 0\n",
        "      - Terminal after taking \"dummy\" in 1 or 7 (episode ends)\n",
        "    \"\"\"\n",
        "\n",
        "    # --- API mirroring the R version / your agent expectations ---\n",
        "\n",
        "    def get_states(self) -> List[str]:\n",
        "        \"\"\"Return all state keys as strings.\"\"\"\n",
        "        return [str(i) for i in range(1, 8)]\n",
        "\n",
        "    def get_actions(self, s: str) -> List[str] | str:\n",
        "        \"\"\"Return available actions (keys) for a state.\"\"\"\n",
        "        if s in (\"1\", \"7\"):\n",
        "            return \"dummy\"  # to mirror the R return type exactly\n",
        "        return [\"left\", \"right\"]\n",
        "\n",
        "    def get_episode_pi(self, agent: Any, start_state: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Generate an episode under the agent's current policy π.\n",
        "        Returns a DataFrame with columns ['s','a','r'].\n",
        "        If start_state is terminal (\"1\" or \"7\"), returns an empty frame.\n",
        "        \"\"\"\n",
        "        # Terminal start → empty episode with correct columns\n",
        "        if start_state in (\"1\", \"7\"):\n",
        "            return pd.DataFrame(columns=[\"s\", \"a\", \"r\"])\n",
        "\n",
        "        s = int(start_state)\n",
        "        rows: List[Dict[str, Any]] = []\n",
        "        max_len = 1000\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            if s in (1, 7):\n",
        "                break  # reached terminal state\n",
        "\n",
        "            a = agent.get_action_pi(str(s))  # sample from π\n",
        "            if a is None:\n",
        "                break  # no available action\n",
        "\n",
        "            # reward only when (s==6 and a==\"right\")\n",
        "            r = 1.0 if (s == 6 and a == \"right\") else 0.0\n",
        "\n",
        "            rows.append({\"s\": str(s), \"a\": a, \"r\": float(r)})\n",
        "\n",
        "            # state transition\n",
        "            if a == \"right\":\n",
        "                s = s + 1\n",
        "            elif a == \"left\":\n",
        "                s = s - 1\n",
        "            elif a == \"dummy\":\n",
        "                # taking dummy at ends would end, but we only offer dummy at 1/7\n",
        "                break\n",
        "\n",
        "        return pd.DataFrame(rows, columns=[\"s\", \"a\", \"r\"])\n",
        "\n",
        "    def get_time_step_data(self, s: str, a: str) -> Dict[str, Optional[str]]:\n",
        "        \"\"\"\n",
        "        One-step transition: return dict with keys:\n",
        "          - 'r': float reward\n",
        "          - 'sN': next state (str) or None if terminal\n",
        "        \"\"\"\n",
        "        s_num = int(s)\n",
        "\n",
        "        # in-chain moves\n",
        "        if a == \"left\" and 1 < s_num < 7:\n",
        "            return {\"r\": 0.0, \"sN\": str(s_num - 1)}\n",
        "\n",
        "        if a == \"right\" and 1 < s_num < 7:\n",
        "            if s_num == 6:\n",
        "                return {\"r\": 1.0, \"sN\": str(s_num + 1)}  # 6 -> 7 with reward 1\n",
        "            return {\"r\": 0.0, \"sN\": str(s_num + 1)}\n",
        "\n",
        "        # terminal dummy action at ends\n",
        "        if s_num in (1, 7) and a == \"dummy\":\n",
        "            return {\"r\": 0.0, \"sN\": None}\n",
        "\n",
        "        raise ValueError(\"Error finding next state and reward!\")\n",
        "\n",
        "# --- quick parity checks\n",
        "env = RLEnvRandom()\n",
        "print(env.get_time_step_data(\"3\", \"right\"))  # {'r': 0.0, 'sN': '4'}\n",
        "print(env.get_time_step_data(\"1\", \"dummy\"))  # {'r': 0.0, 'sN': None}\n",
        "print(env.get_time_step_data(\"6\", \"right\"))  # {'r': 1.0, 'sN': '7'}\n"
      ],
      "metadata": {
        "id": "OZp0gBcqM6dX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note, we define a method `get_time_step_data` that takes a state and action and returns the reward and next state. This method is used by the RL agent class in method `policy_eval_td0`.\n",
        "\n",
        "We can now define the RL agent and set the policy, which must be done before evaluating a policy:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Je_eStftNULL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = RLAgent()\n",
        "env = RLEnvRandom()\n",
        "\n",
        "# add states\n",
        "agent.add_states(env.get_states())\n",
        "\n",
        "# add actions for each state (normalize string -> list)\n",
        "for s in agent.get_state_keys():\n",
        "    acts = env.get_actions(s)\n",
        "    if isinstance(acts, str):\n",
        "        acts = [acts]\n",
        "    agent.add_actions(s, acts)\n",
        "\n",
        "# --- Set the policy ---\n",
        "# middle states: \"2\"..\"6\" with 50/50 left/right\n",
        "states_mid = [str(i) for i in range(2, 7)]\n",
        "pi_mid = {\"left\": 0.5, \"right\": 0.5}\n",
        "agent.set_policy(states_mid, pi_mid)\n",
        "\n",
        "# ends: always dummy\n",
        "pi_ends = {\"dummy\": 1.0}\n",
        "agent.set_policy([\"1\", \"7\"], pi_ends)\n",
        "\n",
        "# show policy\n",
        "print(agent.get_policy())\n"
      ],
      "metadata": {
        "id": "4mMSl2uGNz8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Note a policy must be defined for each possible state that may be generated in an episode. We can now run TD(0) for one episode:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3EmFt_VHOMAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set random seed\n",
        "random.seed(875)\n",
        "np.random.seed(875)\n",
        "\n",
        "# --- initialize state values ---\n",
        "states_mid = [str(i) for i in range(2, 7)]\n",
        "agent.set_state_value(states_mid, 0.5)\n",
        "agent.set_state_value([\"1\", \"7\"], 0.0)\n",
        "\n",
        "print(agent.get_state_values())\n",
        "\n",
        "# --- run one TD(0) evaluation episode starting from state \"4\" ---\n",
        "agent.policy_eval_td0(env, gamma=1.0, states=[\"4\"], max_e=1, reset=False)\n",
        "\n",
        "# --- collect results using dfply ---\n",
        "resTD0 = agent.get_state_values() >> mutate(episodes=1)\n",
        "print(resTD0)\n"
      ],
      "metadata": {
        "id": "H9gRe-xVOij-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note, we first have to set the state-values to the default specified. Next, we run TD(0) prediction with one episode `max_e = 1` and starting state 4. You now have to run similar code for 10 and 100 episodes, store the results and plot a line for each result. You may also add the state-values for the MDP for comparison. Finally, comment on your results."
      ],
      "metadata": {
        "id": "OtTDP6dNOyT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution (finish the code)\n",
        "\n",
        "# --- 10 episodes ---\n",
        "\n",
        "\n",
        "resTD0 = resTD0 >> bind_rows(agent.get_state_values() >> mutate(episodes=10))\n",
        "\n",
        "# --- 100 episodes ---\n",
        "\n",
        "\n",
        "# --- add MDP values for comparison ---\n",
        "mdp_part = mdp.get_state_values() >> mutate(episodes=\"mdp\")\n",
        "\n",
        "\n",
        "# --- plot (plotnine) ---\n",
        "pt = (ggplot(resTD0, aes(x=\"state\", y=\"v\", color=\"episodes\"))\n",
        "     + geom_line()\n",
        "     + geom_point())\n",
        "pt.show()\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "Your comments.\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "Z2J67hrzP7-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q5\n",
        "\n",
        "Run an MC prediction algorithm with 1, 10 and 100 episodes. Hint: you have to call `policy_eval_mc` instead of `policy_eval_td0`. Comment on your results.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b_Y3YBkXDvC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution (finish code)\n",
        "\n",
        "np.random.seed(2947)\n",
        "\n",
        "# convenience\n",
        "states_mid = [str(i) for i in range(2, 7)]  # \"2\"..\"6\"\n",
        "\n",
        "# --- 1 episode ---\n",
        "\n",
        "\n",
        "# --- 10 episodes ---\n",
        "\n",
        "\n",
        "# --- 100 episodes ---\n",
        "\n",
        "\n",
        "# add MDP baseline and keep interior states 2..6\n",
        "\n",
        "# plot\n",
        "pt = (ggplot(resMC, aes(x=\"state\", y=\"v\", color=\"episodes\"))\n",
        "     + geom_line()\n",
        "     + geom_point())\n",
        "pt.show()\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "Your comments.\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "CCQuRhF6h7N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q6\n",
        "\n",
        "Let us join the results for TD(0) and MC and calculate the root mean square (RMS) error $$\\sqrt{ \\frac{1}{5}\\sum_{s=2,\\ldots 6}(V(s)-v_\\pi(s))^2}$$.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S9xT7bKJGh2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resTD0 = resTD0 >> mutate(alg=\"TD0\")\n",
        "resMC = resMC >> mutate(alg=\"MC\")\n",
        "resMDP = (mdp.get_state_values() >> mutate(state=X.state.astype(int)))\n",
        "resMDP = resMDP.rename(columns={\"v\": \"vMDP\"})\n",
        "print(resMDP)\n",
        "\n",
        "# dfply pipeline with bind_rows\n",
        "res = (resTD0\n",
        "    >> bind_rows(resMC)\n",
        "    >> filter_by(X.episodes != \"mdp\")\n",
        "    >> left_join(resMDP, by='state')\n",
        "    >> group_by(X.episodes, X.alg)\n",
        "    >> summarize(rms=0.2 * ((X.v - X.vMDP) ** 2).sum())\n",
        ")\n",
        "res['rms'] = [np.sqrt(x) for x in res['rms']]\n",
        "display(res)"
      ],
      "metadata": {
        "id": "HE-d8K66nkRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which algorithm is best in estimating the state-values? Explain.\n",
        "\n",
        "*Add you comments*"
      ],
      "metadata": {
        "id": "OB4N_2ZKvAYW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q7\n",
        "\n",
        "The results are dependent on the value of the step-size parameter. Let us estimate the state-values using TD(0) for $\\alpha = 0.1, 0.2$ and 0.5 and plot the root mean square (RMS) error given the number of episodes:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8oMWF-OaGh9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(98)\n",
        "np.random.seed(98)\n",
        "\n",
        "states = [str(i) for i in range(2, 7)]  # \"2\"..\"6\"\n",
        "res = None\n",
        "\n",
        "for run in range(1, 21):  # 1..20\n",
        "    for alpha in (0.1, 0.2, 0.5):\n",
        "        # reset values\n",
        "        agent.set_state_value(states, 0.5)\n",
        "        agent.set_state_value([\"1\", \"7\"], 0.0)\n",
        "\n",
        "        e_old = 0  # number of episiodes\n",
        "        for e in [1] + list(range(5, 101, 5)):  # 1, 5, 10, ...\n",
        "            # run only the additional episodes each step\n",
        "            agent.policy_eval_td0(env, gamma=1.0, states=[\"4\"], max_e=e - e_old, alpha=alpha, reset=False)\n",
        "            e_old = e\n",
        "\n",
        "            step_df = agent.get_state_values() >> mutate(episodes=e, alpha=alpha, run=run)\n",
        "            if res is None:\n",
        "                res = step_df.copy()\n",
        "            else:\n",
        "                res = res >> bind_rows(step_df)\n",
        "res = res >> mutate(state=X.state.astype(int))\n",
        "\n",
        "res_plot = (res\n",
        "            >> left_join(resMDP, by=\"state\")\n",
        "            >> group_by(X.episodes, X.alpha, X.run)\n",
        "            >> summarize(rms=0.2 * ((X.v - X.vMDP) ** 2).sum())\n",
        ")\n",
        "res_plot['rms'] = [np.sqrt(x) for x in res_plot['rms']]\n",
        "res_plot = (res_plot\n",
        "            >> group_by(X.episodes, X.alpha)\n",
        "            >> summarize(rms=X.rms.mean())\n",
        "            >> mutate(alpha_cat=X.alpha.astype(str)))\n",
        "\n",
        "pt = (ggplot(res_plot, aes(x=\"episodes\", y=\"rms\", color=\"alpha_cat\"))\n",
        "     + geom_line())\n",
        "pt.show()\n"
      ],
      "metadata": {
        "id": "6c3-IzD6vwBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain the plot.\n",
        "\n",
        "*Add your comments.*"
      ],
      "metadata": {
        "id": "Of2H2w-VGiEP"
      }
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}