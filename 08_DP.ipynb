{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simon-mellergaard/RL/blob/main/08_DP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Notebook pre steps\n",
        "\n",
        "# ALWAYS SAVE YOUR OWN COPY OF THIS NOTEBOOK: File > Save a copy in Drive\n",
        "# IF DANISH MENU DO: Hjælp > Engelsk version\n",
        "\n",
        "# To clear output do: Edit > Clear all outputs\n",
        "\n",
        "## Useful shortscuts\n",
        "# Run current cell: Cmd+Enter\n",
        "# Run current cell and goto next: Shift+Enter\n",
        "# Run selection (or line if no selection): Cmd+Shift+Enter\n",
        "\n",
        "# install missing packages\n",
        "!pip install pymdptoolbox\n",
        "!pip install dfply\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "o0vEJAmyWKPb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic programming\n",
        "\n",
        "This notebook considers dynamic programming algorithms. The term *Dynamic Programming* (*DP*) refers to a collection of algorithms that can be used to compute optimal policies of a model with full information about the dynamics, e.g. a Markov Decision Process (MDP).\n",
        "\n",
        "There are many ways to represent an MDP. First, we implement the algorithm by having\n",
        "\n",
        "*  A list `p_mat` which contains a matrix for each action, e.g. `p_mat[a]` is a the transition probabilities for doing action with index a, i.e. `p_mat[a][s, s'] is the transition probability $p(s'|a,s)$.\n",
        "* A matrix `r_mat` such that `r_mat[s, a]` equals $r(s,a)$.\n",
        "\n",
        "Note this imply that we assume that all states can take all actions. If a state has illigal actions, then set the transition probability to the same state to 1. Moreover, set the reward $r(s,a)$ to a large negative number, so the action not will be choosen as optimal."
      ],
      "metadata": {
        "id": "5jz1VNdl3cUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example - Factory Storage\n",
        "\n",
        "Let us consider [Exercise 6.6.4](https://bss-osca.github.io/rl/06_mdp-1.html#sec-mdp-1-storage) where a factory has a storage tank with a capacity of 4 $\\mathrm{m}^{3}$ for temporarily storing waste produced by the factory. Each week the factory produces $0,1$, 2 or 3 $\\mathrm{m}^{3}$ waste with respective probabilities\n",
        "$$p_{0}=\\displaystyle \\frac{1}{8},\\ p_{1}=\\displaystyle \\frac{1}{2},\\ p_{2}=\\displaystyle \\frac{1}{4} \\text{ and } p_{3}=\\displaystyle \\frac{1}{8}.$$\n",
        "If the amount of waste produced in one week exceeds the remaining capacity of the tank, the excess is specially removed at a cost of \\$30 per cubic metre. At the end of each week there is a regular opportunity to remove all waste from the storage tank at a fixed cost of \\$25 and a variable cost of \\$5 per cubic metre.\n",
        "\n",
        "The problem can be modelled as a finite MDP where a state denote the amount of waste in the tank at the end of week $n$ just before the regular removal opportunity $$S = \\{ 0,1,2,3,4 \\}.$$\n",
        "The action space is $$A(s) = \\{ empty (e), keep (k) \\}.$$\n",
        "The expected cost of a given state and action is the cost of empting the container and the expected cost of a special removal during the next week. Hence the expected reward is $$r(s, e) = -(25 + 5s)$$\n",
        "and $$r(s,k) = -30\\sum_{i>4-s} (s+i-4)p_i.$$\n",
        "Finally, the transition probabilities are:\n",
        "$$\n",
        "\\begin{align}\n",
        "   p(s'|s,k) &= p_{s'-s}\\text{ if } s\\leq s' \\leq 3, \\\\\n",
        "   p(4|s,k)  &= \\sum_{i\\geq 4-s} p_i, \\\\\n",
        "   p(s'|s,e) &= p_{s'}\\text{ if }  0\\leq s' \\leq 4, \\\\\n",
        "   p(s'|s,a) &=  0 \\text{ otherwise.}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "First we add transition probabilities and reward to a list and matrix so in the format needed for the DP algorithms."
      ],
      "metadata": {
        "id": "UR2qQXyNdVC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "NUM_STATES = 5       # Tank levels: 0 to 4\n",
        "NUM_ACTIONS = 2      # Actions: 0 = keep, 1 = empty\n",
        "\n",
        "# Waste generation probabilities for 0–3 m³\n",
        "waste_probs = [1/8, 1/2, 1/4, 1/8]\n",
        "MAX_WASTE = 3        # Max production in one week\n",
        "\n",
        "# Initialize transition probability matrices for each action\n",
        "p_mat = [np.zeros((NUM_STATES, NUM_STATES)) for _ in range(NUM_ACTIONS)]\n",
        "\n",
        "# Initialize reward matrix: rows = states, columns = actions\n",
        "r_mat = np.zeros((NUM_STATES, NUM_ACTIONS))\n",
        "\n",
        "# Action 0: keep\n",
        "# Accumulate overflow costs based on possible next waste productions\n",
        "for s in range(NUM_STATES):\n",
        "    expected_overflow_cost = 0\n",
        "    for i in range(len(waste_probs)):  # i: waste produced (0–3 m³)\n",
        "        next_state = s + i\n",
        "        if next_state <= 4:\n",
        "            p_mat[0][s, next_state] += waste_probs[i]\n",
        "        else:\n",
        "            # Clamp overflowed states to 4, calculate overflow cost\n",
        "            p_mat[0][s, 4] += waste_probs[i]\n",
        "            overflow = next_state - 4\n",
        "            expected_overflow_cost += overflow * 30 * waste_probs[i]\n",
        "    r_mat[s, 0] = -expected_overflow_cost  # Reward is negative cost\n",
        "\n",
        "# Action 1: empty\n",
        "# Transition probabilities are based only on new waste produced\n",
        "# Tank is reset to empty before production\n",
        "for s in range(NUM_STATES):\n",
        "    for i in range(len(waste_probs)):\n",
        "        p_mat[1][s, i] = waste_probs[i]  # Transition to state i\n",
        "    r_mat[s, 1] = -(25 + 5 * s)  # Cost of emptying tank\n",
        "\n",
        "# Output results using data frames\n",
        "state_labels = [f\"s{i}\" for i in range(NUM_STATES)]\n",
        "action_labels = [\"keep\", \"empty\"]\n",
        "\n",
        "# Display reward matrix\n",
        "reward_df = pd.DataFrame(r_mat, index=state_labels, columns=action_labels)\n",
        "print(\"Reward matrix with labels:\")\n",
        "print(reward_df)\n",
        "\n",
        "# Display transition matrix for 'keep' action\n",
        "transition_df = pd.DataFrame(p_mat[0], index=state_labels, columns=state_labels)\n",
        "print(\"\\nTransition matrix for action 'keep':\")\n",
        "print(transition_df)\n",
        "\n",
        "# Display transition matrix for 'empty' action\n",
        "transition_df = pd.DataFrame(p_mat[1], index=state_labels, columns=state_labels)\n",
        "print(\"\\nTransition matrix for action 'empty':\")\n",
        "print(transition_df)"
      ],
      "metadata": {
        "id": "KBWP6X1rdVC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy evaluation\n",
        "\n",
        "There are many ways to represent an MDP. Here we implement the algorithm by having\n",
        "\n",
        "*  A list `p_mat` which contains a matrix for each action, e.g. `p_mat[a]` is a the transition probabilities for doing action with index a, i.e. `p_mat[a][s, s'] is the transition probability $p(s'|a,s)$.\n",
        "* A matrix `r_mat` such that `r_mat[s, a]` equals $r(s,a)$.\n",
        "\n",
        "Note this imply that we assume that all states can take all actions. If a state has illigal actions, then set the transition probability to the same state to 1. Moreover, set the reward $r(s,a)$ to a large negative number, so the action not will be choosen as optimal."
      ],
      "metadata": {
        "id": "P63ryp28Y1Mq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def policy_evaluation(policy, p_mat, r_mat, gamma=0.5, threshold=1e-5):\n",
        "    \"\"\"\n",
        "    Performs policy evaluation for a given MDP. Note here we identify states and\n",
        "    actions by their indices (starting from 0).\n",
        "\n",
        "    Args:\n",
        "        policy (ndarray): The policy to evaluate.\n",
        "        p_mat (list): Transition probability matrices for each action.\n",
        "        r_mat (ndarray): Reward matrix of shape (states, actions).\n",
        "        gamma (float): Discount factor.\n",
        "        threshold (float): Convergence threshold.\n",
        "\n",
        "    Returns:\n",
        "        V (ndarray): Value function for the given policy.\n",
        "        iteration (int): Number of iterations until convergence.\n",
        "    \"\"\"\n",
        "    NUM_STATES = r_mat.shape[0]\n",
        "    NUM_ACTIONS = r_mat.shape[1]\n",
        "    iteration = 0\n",
        "    V = np.zeros(NUM_STATES) # current state values\n",
        "\n",
        "    # Policy evaluation\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in range(NUM_STATES):\n",
        "            v = V[s]\n",
        "            action = policy[s]\n",
        "            V[s] = r_mat[s, action] + gamma * np.dot(p_mat[action][s], V)\n",
        "            delta = max(delta, abs(V[s]-v))\n",
        "        iteration += 1\n",
        "        if delta < threshold:\n",
        "            break\n",
        "\n",
        "    return V, iteration"
      ],
      "metadata": {
        "id": "jzygR6_wY1Mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your turn - Factory Storage\n",
        "\n",
        "1. Consider the policy never to empty except if the tank contains $1 m^3$.\n",
        "   - Calculate the total expected discounted cost with a discount rate of 0.99.\n",
        "   - What is the expected total discounted cost assuming starting with $3m^3$ in the tank?\n",
        "2. Consider the policy to never empty the tank.\n",
        "   - Calculate the total expected discounted cost with a discount rate of 0.99.\n",
        "   - What is the expected total discounted cost assuming starting with $2m^3$ in the tank?\n",
        "3. Which policy is best if I start with an empty tank? Which is best if start with $1m^3$ in the tank?"
      ],
      "metadata": {
        "id": "nTLnAHbmaWiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "policy1 = (___, ___, ___, ___, ___)\n",
        "V_pe1, ite1 = policy_evaluation(___)\n",
        "policy2 = (___, ___, ___, ___, ___)\n",
        "V_pe2, ite2 = ___\n",
        "\n",
        "print(\"\\nQ3: Compare policies using a data frame:\")\n",
        "df_result = pd.DataFrame({\n",
        "    \"State\": state_labels,\n",
        "    \"V (pe1)\": V_pe1,\n",
        "    \"Policy (pe1)\": [action_labels[i] for i in policy1],\n",
        "    \"V (pe2)\": V_pe2,\n",
        "    \"Policy (pe2)\": [action_labels[i] for i in policy2],\n",
        "})\n",
        "display(df_result)"
      ],
      "metadata": {
        "id": "dJ8g5EBWatAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy iteration\n",
        "\n",
        "We now extend the policy evaluation algorithm."
      ],
      "metadata": {
        "id": "6FWgh_k68Mob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def policy_iteration(p_mat, r_mat, gamma=0.5, threshold=1e-5):\n",
        "    \"\"\"\n",
        "    Performs policy iteration for a given MDP. Note here we identify states and\n",
        "    actions by their indices (starting from 0).\n",
        "\n",
        "    Args:\n",
        "        p_mat (list): Transition probability matrices for each action.\n",
        "        r_mat (ndarray): Reward matrix of shape (states, actions).\n",
        "        gamma (float): Discount factor.\n",
        "        threshold (float): Convergence threshold.\n",
        "\n",
        "    Returns:\n",
        "        V (ndarray): Optimal value function.\n",
        "        policy (ndarray): Optimal policy.\n",
        "        iteration (int): Number of iterations until convergence.\n",
        "    \"\"\"\n",
        "    NUM_STATES = r_mat.shape[0]\n",
        "    NUM_ACTIONS = r_mat.shape[1]\n",
        "    policy = np.zeros(NUM_STATES, dtype=int)\n",
        "    is_policy_stable = False\n",
        "    iteration = 0\n",
        "    V = np.zeros(NUM_STATES) # current state values\n",
        "\n",
        "    while not is_policy_stable:\n",
        "        # Policy evaluation\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for s in range(NUM_STATES):\n",
        "                v = V[s]\n",
        "                action = policy[s]\n",
        "                V[s] = r_mat[s, action] + gamma * np.dot(p_mat[action][s], V)\n",
        "                delta = max(delta, abs(V[s]-v))\n",
        "            iteration += 1\n",
        "            if delta < threshold:\n",
        "                break\n",
        "\n",
        "        # Policy improvement\n",
        "        is_policy_stable = True\n",
        "        for s in range(NUM_STATES):\n",
        "            old_action = policy[s]\n",
        "            action_values = [r_mat[s, a] + gamma * np.dot(p_mat[a][s], V) for a in range(NUM_ACTIONS)]\n",
        "            policy[s] = int(np.argmax(action_values))\n",
        "            if old_action != policy[s]:\n",
        "                is_policy_stable = False\n",
        "        iteration += 1\n",
        "    return V, policy, iteration"
      ],
      "metadata": {
        "id": "2yeYd7lu_-42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your turn - Factory Storage\n",
        "\n",
        "1. Run policy iteration using a discount rate of 0.5.\n",
        "2. What is the optimal policy?"
      ],
      "metadata": {
        "id": "zOyc_Ks3exeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "# Run Policy Iteration\n",
        "___\n",
        "\n",
        "print(\"\\nPolicy Iteration Results:\")\n",
        "print(\"Optimal policy (0 = keep, 1 = empty):\", policy_pi)\n",
        "print(\"Value function:\", V_pi)\n",
        "print(\"Converged in iterations:\", num_iters_pi, \"\\n\")\n",
        "\n",
        "# The results show that ..."
      ],
      "metadata": {
        "id": "iCTWanBBexep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Value iteration\n",
        "\n",
        "We may implement this in Python by having a set of matrices for the transition probabilities and rewards as when implemented policy iteration (see previous section)."
      ],
      "metadata": {
        "id": "ZzRg8Dq9Ac3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(p_mat, r_mat, gamma=0.5, threshold=1e-5):\n",
        "    \"\"\"\n",
        "    Performs value iteration for a given MDP.\n",
        "\n",
        "    Args:\n",
        "        p_mat (list): Transition probability matrices for each action.\n",
        "        r_mat (ndarray): Reward matrix of shape (states, actions).\n",
        "        gamma (float): Discount factor.\n",
        "        threshold (float): Convergence threshold.\n",
        "\n",
        "    Returns:\n",
        "        V (ndarray): Optimal value function.\n",
        "        policy (ndarray): Optimal policy.\n",
        "        iteration (int): Number of iterations until convergence.\n",
        "    \"\"\"\n",
        "    NUM_STATES = r_mat.shape[0]\n",
        "    NUM_ACTIONS = r_mat.shape[1]\n",
        "    V = np.zeros(NUM_STATES)\n",
        "    policy = np.zeros(NUM_STATES, dtype=int)\n",
        "    iteration = 0\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in range(NUM_STATES):\n",
        "            action_values = []\n",
        "            for a in range(NUM_ACTIONS):\n",
        "                expected_value = r_mat[s, a] + gamma * np.dot(p_mat[a][s], V)\n",
        "                action_values.append(expected_value)\n",
        "            best_value = max(action_values)\n",
        "            delta = max(delta, abs(best_value - V[s]))\n",
        "            V[s] = best_value\n",
        "            policy[s] = int(np.argmax(action_values))\n",
        "        iteration += 1\n",
        "        if delta < threshold:\n",
        "            break\n",
        "    return V, policy, iteration"
      ],
      "metadata": {
        "id": "fvei53eyAk_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your turn - Factory Storage\n",
        "\n",
        "1. Run value iteration using a discount rate of 0.5.\n",
        "2. Compare the results from value iteration (they should be the same)."
      ],
      "metadata": {
        "id": "nY6zWiv_3ikg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "# Run Value Iteration\n",
        "___\n",
        "\n",
        "print(\"\\nValue Iteration Results:\")\n",
        "print(\"Optimal policy (0 = keep, 1 = empty):\", policy_vi)\n",
        "print(\"Value function:\", V_vi)\n",
        "print(\"Converged in iterations:\", num_iters_vi)\n",
        "\n",
        "print(\"Compare results using a data frame:\")\n",
        "df_result = pd.DataFrame({\n",
        "    \"State\": state_labels,\n",
        "    \"V (vi)\": V_vi.round(4),\n",
        "    \"Policy (vi)\": [action_labels[i] for i in policy_vi],\n",
        "    \"V (pi)\": V_pi.round(4),\n",
        "    \"Policy (pi)\": [action_labels[i] for i in policy_pi],\n",
        "})\n",
        "display(df_result)"
      ],
      "metadata": {
        "id": "gWyNFU9J-olS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using `mdptoolbox` algorithms\n",
        "\n",
        "There is also a library that already implements policy and value iteration. It must be installed using `pip` (see the pre-steps section)."
      ],
      "metadata": {
        "id": "KDMOxlNLMIwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mdptoolbox as mdpbox\n",
        "\n",
        "# Solve using Value Iteration\n",
        "vi = mdpbox.mdp.ValueIteration(p_mat, r_mat, 0.5)\n",
        "vi.run()\n",
        "\n",
        "# Display optimal policy and value function\n",
        "print(\"\\nOptimal policy (0 = keep, 1 = empty):\", vi.policy)\n",
        "print(\"Value function:\", vi.V)"
      ],
      "metadata": {
        "id": "mwaTUULz-37M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Your turn\n",
        "\n",
        "1. Solve the problem using policy iteration with a discount rate of 0.5.\n",
        "2. Solve the problem with a discount rate of 0.99.\n",
        "  - Is the policy different than when used a discount rate of 0.5?\n",
        "  - What is the total expected cost if start with an empty tank?\n",
        "  - Why is it different than when used a discount rate of 0.5?"
      ],
      "metadata": {
        "id": "ftBjLD7eMfXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "# Q1\n",
        "pi = mdpbox.mdp.PolicyIteration(___)\n",
        "pi.run()\n",
        "\n",
        "# Display optimal policy and value function\n",
        "print(\"Q1)\\nOptimal policy (gamma = 0.5):\", pi.policy)\n",
        "print(\"Value function:\", pi.V)\n",
        "\n",
        "# Q2\n",
        "___\n",
        "\n"
      ],
      "metadata": {
        "id": "qHLakXz7_AaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A generic MDP builder\n",
        "\n",
        "Often, a state in an MDP consists of a combination of several state variables. For instance, a state $s$ could be defined by $s = (w, p)$ where $w$ is the weather and $p$ is the price. Here, we need a way to identify a state with a given state index. As a result, a generic way of building the MDP is desired. Below, we implement a class for building an MDP where states, actions, and transition probabilities are stored using a directory."
      ],
      "metadata": {
        "id": "LoK1X5OCpxOv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwFSJoCSdvmX"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class MDP:\n",
        "    \"\"\"\n",
        "    A class representing a Markov Decision Process (MDP) using defaultdict structures.\n",
        "\n",
        "    This implementation includes state management, action specification, transition\n",
        "    probabilities, rewards, policies, and iterative algorithms for policy and value iteration.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes an empty MDP with model and state values.\n",
        "        \"\"\"\n",
        "        self.model = defaultdict(lambda: {\"pi\": None, \"actions\": defaultdict(dict)})\n",
        "        self.v = defaultdict(float)\n",
        "\n",
        "    def add_state_space(self, states):\n",
        "        \"\"\"\n",
        "        Adds states to the MDP.\n",
        "\n",
        "        Args:\n",
        "            states (list): List of state identifiers (strings or convertible to strings).\n",
        "        \"\"\"\n",
        "        for state in states:\n",
        "            _ = self.model[str(state)]\n",
        "        self.set_state_value()\n",
        "\n",
        "    def add_action_space(self, state_str, actions):\n",
        "        \"\"\"\n",
        "        Adds actions to a given state. Note you have to update the action\n",
        "        afterwards using `add_action`.\n",
        "\n",
        "        Args:\n",
        "            state_str (str): The state identifier.\n",
        "            actions (list): List of action identifiers.\n",
        "        \"\"\"\n",
        "        if not isinstance(state_str, str):\n",
        "            raise ValueError(\"State is not a sting!\")\n",
        "        if isinstance(actions, str):\n",
        "            # If it's a string, put it in a list to treat it as a single item\n",
        "            actions = [actions]\n",
        "        for action in actions:\n",
        "            # Initialize the action dictionary with 'pr' and 'r' keys\n",
        "            self.model[state_str][\"actions\"][str(action)] = {\"pr\": {}, \"r\": None}\n",
        "\n",
        "    def add_action(self, state_str, action_str, reward, pr):\n",
        "        \"\"\"\n",
        "        Adds a transition action with reward and transition probabilities.\n",
        "\n",
        "        Args:\n",
        "            state_str (str): State from which the action is taken.\n",
        "            action_str (str): Action identifier.\n",
        "            reward (float): Expected reward for taking the action.\n",
        "            pr (dict): Transition probabilities as {next_state: probability}.\n",
        "        \"\"\"\n",
        "        ## remove keys with zero trans pr\n",
        "        keys_to_remove = [key for key, value in pr.items() if value == 0]\n",
        "        for key in keys_to_remove:\n",
        "            del pr[key]\n",
        "        self.model[state_str][\"actions\"][action_str] = {\"r\": reward, \"pr\": pr}\n",
        "\n",
        "    def check(self, delta = 10*np.spacing(np.float64(1))):\n",
        "        \"\"\"\n",
        "        Performs checks on the built MDP model.\n",
        "\n",
        "        Verifies that transition probabilities sum to approximately 1.0 for each\n",
        "        state-action pair and checks for rewards less than the high_neg_reward.\n",
        "        Prints warnings if any issues are found.\n",
        "\n",
        "        Args:\n",
        "            delta (float, optional): Tolerance for the sum of transition probabilities. Defaults to 1e-10.\n",
        "        \"\"\"\n",
        "        ok = True\n",
        "        # Check if transition pr of an action sum to one\n",
        "        for state_label, state_content in self.model.items():\n",
        "            for action_label, action_content in state_content[\"actions\"].items():\n",
        "                pr = action_content[\"pr\"]\n",
        "                pr_sum = np.sum(list(pr.values()))\n",
        "                absdiff = np.abs(1-pr_sum)\n",
        "                if absdiff >= delta:\n",
        "                    print(f\"Warning: Transition probabilities for action '{action_label}' in state '{state_label}' do not sum to 1.0. Diff is: {absdiff}\")\n",
        "                    ok = False\n",
        "\n",
        "        # Check if there are states with no actions\n",
        "        for state_label, state_content in self.model.items():\n",
        "            if len(state_content[\"actions\"]) == 0:\n",
        "                print(f\"Warning: State '{state_label}' has no actions.\")\n",
        "                ok = False\n",
        "\n",
        "        # Check if all action transitions are to a state\n",
        "        states = list(self.model.keys())\n",
        "        for state_label, state_content in self.model.items():\n",
        "            for action_label, action_content in state_content[\"actions\"].items():\n",
        "                if not all(key in self.model for key in action_content['pr'].keys()):\n",
        "                    print(f\"Warning: Action '{action_label}' in state '{state_label}' has a transition to a non-existing state.\")\n",
        "                    ok = False\n",
        "        if ok:\n",
        "            print(\"All checks passed!\")\n",
        "\n",
        "\n",
        "    def normalize(self):\n",
        "        \"\"\"\n",
        "        Normalizes the transition probabilities for each state-action pair.\n",
        "        \"\"\"\n",
        "        for state_label, state_content in self.model.items():\n",
        "            for action_label, action_content in state_content[\"actions\"].items():\n",
        "                pr = action_content[\"pr\"]\n",
        "                pr_sum = np.sum(list(pr.values()))\n",
        "                for next_state_label, prob in pr.items():\n",
        "                    pr[next_state_label] = prob / pr_sum\n",
        "                action_content[\"pr\"] = pr\n",
        "\n",
        "    def set_state_value(self, states=None, value=0):\n",
        "        \"\"\"\n",
        "        Initializes or updates the value of states.\n",
        "\n",
        "        Args:\n",
        "            states (list, optional): List of state identifiers. Defaults to all states.\n",
        "            value (float, optional): Value to assign. Defaults to 0.\n",
        "        \"\"\"\n",
        "        states = states or list(self.model.keys())\n",
        "        for state in states:\n",
        "            self.v[state] = value\n",
        "\n",
        "    def set_random_deterministic_policy(self):\n",
        "        \"\"\"\n",
        "        Sets a random deterministic policy for each state.\n",
        "        \"\"\"\n",
        "        for state in self.model:\n",
        "            actions = list(self.model[state][\"actions\"].keys())\n",
        "            if actions:\n",
        "                chosen_action = random.choice(actions)\n",
        "                self.model[state][\"pi\"] = {chosen_action: 1}\n",
        "\n",
        "    def set_deterministic_policy(self, state_actions):\n",
        "        \"\"\"\n",
        "        Sets a deterministic policy from a state-action mapping.\n",
        "\n",
        "        Args:\n",
        "            state_actions (dict): Mapping {state: action}.\n",
        "        \"\"\"\n",
        "        for state, action in state_actions.items():\n",
        "            self.model[state][\"pi\"] = {action: 1}\n",
        "\n",
        "    def set_policy(self, states, pi):\n",
        "        \"\"\"\n",
        "        Sets a stochastic or deterministic policy for a list of states.\n",
        "\n",
        "        Args:\n",
        "            states (list): List of states to assign the policy.\n",
        "            pi (dict): Policy as {action: probability}.\n",
        "        \"\"\"\n",
        "        for state in states:\n",
        "            self.model[state][\"pi\"] = pi.copy()\n",
        "\n",
        "    def get_state_keys(self):\n",
        "        \"\"\"\n",
        "        Returns the list of state identifiers.\n",
        "\n",
        "        Returns:\n",
        "            list: List of state keys.\n",
        "        \"\"\"\n",
        "        return list(self.model.keys())\n",
        "\n",
        "    def get_action_keys(self, state):\n",
        "        \"\"\"\n",
        "        Returns the action identifiers for a given state.\n",
        "\n",
        "        Args:\n",
        "            state (str): State identifier.\n",
        "\n",
        "        Returns:\n",
        "            list: List of action keys.\n",
        "        \"\"\"\n",
        "        return list(self.model[state][\"actions\"].keys())\n",
        "\n",
        "    def get_action_info(self, state):\n",
        "        \"\"\"\n",
        "        Gets reward and transition probabilities for each action in a state.\n",
        "\n",
        "        Args:\n",
        "            state (str): State identifier.\n",
        "\n",
        "        Returns:\n",
        "            dict: Action information.\n",
        "        \"\"\"\n",
        "        return dict(self.model[state][\"actions\"])\n",
        "\n",
        "    def get_mdp_info(self):\n",
        "        \"\"\"\n",
        "        Returns information about the MDP.\n",
        "\n",
        "        Returns:\n",
        "            dict: The info.\n",
        "        \"\"\"\n",
        "        res = {}\n",
        "        res[\"states\"] = len(self.model)\n",
        "        res[\"actions\"] = sum([len(self.model[k][\"actions\"]) for k in self.model.keys()])\n",
        "        return res\n",
        "\n",
        "    def get_policy(self, add_state_values = False):\n",
        "        \"\"\"\n",
        "        Retrieves the current policy.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Table of state, action, and probability.\n",
        "        \"\"\"\n",
        "        policy = []\n",
        "        for state in self.get_state_keys():\n",
        "            for action, prob in self.model[state][\"pi\"].items():\n",
        "                if not add_state_values:\n",
        "                    policy.append({\"state\": state, \"action\": action, \"pr\": prob})\n",
        "                else:\n",
        "                    policy.append({\"state\": state, \"action\": action, \"pr\": prob, \"v\": self.v[state]})\n",
        "        df = pd.DataFrame(policy)\n",
        "        df.set_index(\"state\")\n",
        "        return df\n",
        "\n",
        "    def get_state_values(self, states=None):\n",
        "        \"\"\"\n",
        "        Returns the current value of each state.\n",
        "\n",
        "        Args:\n",
        "            states (list, optional): Subset of states. Defaults to all.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Table of state values.\n",
        "        \"\"\"\n",
        "        states = states or list(self.v.keys())\n",
        "        return pd.DataFrame([{\"state\": s, \"v\": self.v[s]} for s in states])\n",
        "\n",
        "    def get_mdp_matrices(self, high_neg_reward = -100000):\n",
        "        \"\"\"\n",
        "        Returns transition probability and reward matrices.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                p_mat (list): List of transition probability matrices.\n",
        "                r_mat (ndarray): Reward matrix.\n",
        "                states (list): List of state identifiers.\n",
        "                actions (list): List of action identifiers.\n",
        "        \"\"\"\n",
        "        states = self.get_state_keys()\n",
        "        actions = set(\n",
        "            action for state in states for action in self.get_action_keys(state)\n",
        "        )\n",
        "        actions = list(actions)\n",
        "        actions.sort()\n",
        "        p_mat = [pd.DataFrame(0.0, index=states, columns=states) for _ in actions]\n",
        "        for df in p_mat:\n",
        "            np.fill_diagonal(df.values, 1) # set default to transition to same state (so illigal actions work)\n",
        "        r_mat = pd.DataFrame(high_neg_reward, index=states, columns=actions)\n",
        "        for state in states:\n",
        "            for action in self.get_action_keys(state):\n",
        "                p_mat[actions.index(action)].at[state, state] = 0  # reset to 0 again (since action is not illigal)\n",
        "                pr = self.model[state][\"actions\"][action][\"pr\"]\n",
        "                r = self.model[state][\"actions\"][action][\"r\"]\n",
        "                r_mat.at[state, action] = r\n",
        "                for next_state, prob in pr.items():\n",
        "                    p_mat[actions.index(action)].at[state, next_state] = prob\n",
        "        p_mat = [m.to_numpy() for m in p_mat]  # convert to matrices\n",
        "        r_mat = r_mat.to_numpy()\n",
        "        return p_mat, r_mat, states, actions\n",
        "\n",
        "    def bellman_calc(self, gamma, state, action):\n",
        "        \"\"\"\n",
        "        Computes Bellman update for a given state-action pair.\n",
        "\n",
        "        Args:\n",
        "            gamma (float): Discount factor.\n",
        "            state (str): State identifier.\n",
        "            action (str): Action identifier.\n",
        "\n",
        "        Returns:\n",
        "            float: Updated value.\n",
        "        \"\"\"\n",
        "        pr = self.model[state][\"actions\"][action][\"pr\"]\n",
        "        reward = self.model[state][\"actions\"][action][\"r\"]\n",
        "        return reward + gamma * sum(pr[s] * self.v[s] for s in pr)\n",
        "\n",
        "    def policy_eval(self, gamma, theta=1e-5, max_iter=10000, reset=True):\n",
        "        \"\"\"\n",
        "        Iteratively evaluates the current policy.\n",
        "\n",
        "        Args:\n",
        "            gamma (float): Discount factor.\n",
        "            theta (float): Convergence threshold.\n",
        "            max_iter (int): Max iterations.\n",
        "            reset (bool): Whether to reset state values to 0.\n",
        "        \"\"\"\n",
        "        if reset:\n",
        "            self.set_state_value()\n",
        "        for _ in range(max_iter):\n",
        "            delta = 0\n",
        "            for state in self.model:\n",
        "                v_old = self.v[state]\n",
        "                pi = self.model[state][\"pi\"]\n",
        "                value = sum(pi[a] * self.bellman_calc(gamma, state, a) for a in pi)\n",
        "                self.v[state] = value\n",
        "                delta = max(delta, abs(v_old - value))\n",
        "            if delta < theta:\n",
        "                break\n",
        "        else:\n",
        "            print(f\"Policy evaluation stopped at max iterations: {max_iter}\")\n",
        "\n",
        "    def policy_iteration(self, gamma, theta=1e-5, max_iter_eval=10000, max_iter_policy=100):\n",
        "        \"\"\"\n",
        "        Performs policy iteration with evaluation and improvement steps.\n",
        "\n",
        "        Args:\n",
        "            gamma (float): Discount factor.\n",
        "            theta (float): Convergence threshold.\n",
        "            max_iter_eval (int): Max iterations during policy evaluation.\n",
        "            max_iter_policy (int): Max policy improvement steps.\n",
        "        \"\"\"\n",
        "        self.set_random_deterministic_policy()\n",
        "        for i in range(max_iter_policy):\n",
        "            self.policy_eval(gamma, theta, max_iter_eval, reset=False)\n",
        "            stable = True\n",
        "            for state in self.model:\n",
        "                old_action = next(iter(self.model[state][\"pi\"]))\n",
        "                best_val, best_action = float(\"-inf\"), None\n",
        "                for action in self.get_action_keys(state):\n",
        "                    val = self.bellman_calc(gamma, state, action)\n",
        "                    if val > best_val:\n",
        "                        best_val = val\n",
        "                        best_action = action\n",
        "                self.model[state][\"pi\"] = {best_action: 1}\n",
        "                if best_action != old_action:\n",
        "                    stable = False\n",
        "            if stable:\n",
        "                break\n",
        "        else:\n",
        "            print(f\"Policy iteration stopped at max iterations: {max_iter_policy}\")\n",
        "        print(f\"Policy iteration finished in {i + 1} iterations.\")\n",
        "\n",
        "    def value_iteration(self, gamma, theta=1e-5, max_iter=10000, reset=True):\n",
        "        \"\"\"\n",
        "        Performs value iteration algorithm.\n",
        "\n",
        "        Args:\n",
        "            gamma (float): Discount factor.\n",
        "            theta (float): Convergence threshold.\n",
        "            max_iter (int): Max number of iterations.\n",
        "            reset (bool): Whether to reinitialize state values.\n",
        "        \"\"\"\n",
        "        self.set_random_deterministic_policy()\n",
        "        if reset:\n",
        "            self.set_state_value()\n",
        "        for i in range(max_iter):\n",
        "            delta = 0\n",
        "            for state in self.model:\n",
        "                v_old = self.v[state]\n",
        "                best_val, best_action = float(\"-inf\"), None\n",
        "                for action in self.get_action_keys(state):\n",
        "                    val = self.bellman_calc(gamma, state, action)\n",
        "                    if val > best_val:\n",
        "                        best_val = val\n",
        "                        best_action = action\n",
        "                self.v[state] = best_val\n",
        "                self.model[state][\"pi\"] = {best_action: 1}\n",
        "                delta = max(delta, abs(v_old - best_val))\n",
        "            if delta < theta:\n",
        "                break\n",
        "        else:\n",
        "            print(f\"Value iteration stopped at max iterations: {max_iter}\")\n",
        "        print(f\"Value iteration finished in {i + 1} iterations.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us test the class:"
      ],
      "metadata": {
        "id": "-F9Grt9eP_vQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint as pp\n",
        "\n",
        "## Testing\n",
        "state_labels = [\"a\", \"b\", \"c\"]\n",
        "mdp = MDP()\n",
        "mdp.add_state_space(state_labels)\n",
        "mdp.get_state_keys()\n",
        "mdp.get_action_keys(\"b\")\n",
        "mdp.add_action(\"b\", \"keep\", 5, {\"a\": 0.5, \"c\": 0.5})\n",
        "mdp.add_action(\"b\", \"empty\", 2, {\"a\": 1})\n",
        "# mdp.add_action(\"a\", \"empty\", 2, {\"d\": 1})\n",
        "print(mdp.get_action_info(\"b\"))\n",
        "print(mdp.get_mdp_info())\n",
        "mdp.check()\n",
        "pp.pprint(mdp.model)  # internal representation of the dasd MDP\n",
        "print(\"Matrices:\")\n",
        "mdp.get_mdp_matrices() # if want to get matrices for mdptoolbox"
      ],
      "metadata": {
        "id": "cs1FY5SRP_vR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example - Factory Storage (using `MDP` class)\n",
        "\n",
        "Lets us try to build and solve the model:"
      ],
      "metadata": {
        "id": "M1hheJe1e2Qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_labels = [f'{l}' for l in range(0,5)]\n",
        "action_labels = [\"keep\", \"empty\"]\n",
        "mdp = MDP()\n",
        "mdp.add_state_space(state_labels)\n",
        "\n",
        "def reward(s_l, a_l):\n",
        "    \"\"\"\n",
        "    Calculates the expected reward for a given state and action in the Factory Storage problem.\n",
        "\n",
        "    Args:\n",
        "        s_l (str): The label of the current state (tank level).\n",
        "        a_l (str): The label of the action taken (\"keep\" or \"empty\").\n",
        "\n",
        "    Returns:\n",
        "        float: The calculated expected reward.\n",
        "    \"\"\"\n",
        "    s = int(s_l)\n",
        "    p = np.array([1/8, 1/2, 1/4, 1/8])\n",
        "\n",
        "    if a_l == \"keep\":\n",
        "        if s < 2:\n",
        "            return 0  # no excess waste\n",
        "        k = np.arange(4 - s + 1, 4)  # equivalent to (4 - i + 1):3 in R\n",
        "        return -30 * np.sum((s + k - 4) * p[k])\n",
        "    elif a_l == \"empty\":\n",
        "        return -1 * (25 + 5 * s)\n",
        "    return None\n",
        "\n",
        "def transPr(s_l, a_l):\n",
        "    \"\"\"\n",
        "    Calculates the transition probabilities to the next states for a given state and action.\n",
        "\n",
        "    Args:\n",
        "        s_l (str): The label of the current state (tank level).\n",
        "        a_l (str): The label of the action taken (\"keep\" or \"empty\").\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are the next state labels (strings) and values are the corresponding probabilities.\n",
        "    \"\"\"\n",
        "    s = int(s_l)\n",
        "    p = np.array([1/8, 1/2, 1/4, 1/8])\n",
        "    pr = []\n",
        "    ids = []\n",
        "    if a_l == \"keep\":\n",
        "        if s < 4:\n",
        "            for j in range(s, 4):\n",
        "                pr.append(p[j - s])  # equivalent to p[j - i + 1] in R\n",
        "                ids.append(str(j)) # Convert to string\n",
        "\n",
        "        if s <= 4:\n",
        "            pr.append(np.sum(p[4 - s:4]))  # equivalent to sum(p[(4 - i):3 + 1])\n",
        "            ids.append('4') # Convert to string\n",
        "    elif a_l == \"empty\":\n",
        "        for j in range(4):\n",
        "            pr.append(p[j])\n",
        "            ids.append(str(j)) # Convert to string\n",
        "        pr.append(0.0) # Add probability for state 4\n",
        "        ids.append('4') # Add state label '4'\n",
        "    return dict(zip(ids, pr))  # Return the transition probabilities as a dictionary\n",
        "\n",
        "# Add actions\n",
        "for s_l in mdp.get_state_keys():\n",
        "    for a_l in action_labels:\n",
        "        # print(f\"\\ns: {s_l} a: {a_l} reward: {reward(s_l, a_l)} trans pr: {transPr(s_l, a_l)}\")\n",
        "        tpr = transPr(s_l, a_l)\n",
        "        # row_sum = sum(tpr.values())\n",
        "        # if not np.isclose(row_sum, 1.0):\n",
        "        #     print(f\"Warning: Transition probabilities for action '{a_l}' (index {a_index}) in state {s_l} (index {s_index}) do not sum to 1.0. Row sum is: {row_sum}\")\n",
        "        mdp.add_action(s_l, a_l, reward(s_l, a_l), transPr(s_l, a_l))\n",
        "mdp.get_mdp_info()\n",
        "mdp.check()\n",
        "\n",
        "# Solve using Value Iteration and mdptoolbox\n",
        "p_mat, r_mat, states, actions = mdp.get_mdp_matrices()\n",
        "vi = mdpbox.mdp.ValueIteration(p_mat, r_mat, 0.5)\n",
        "vi.run()\n",
        "\n",
        "# Display optimal policy and value function\n",
        "df_result = pd.DataFrame({\n",
        "    \"State\": states,\n",
        "    \"Policy\": [actions[p] for p in vi.policy],\n",
        "    \"Value\": vi.V\n",
        "})\n",
        "print(\"Optimal policy using mdptollbox\")\n",
        "display(df_result)\n",
        "\n",
        "print(\"Optimal policy using MDP class algorithm\")\n",
        "mdp.value_iteration(0.5)\n",
        "mdp.get_policy(add_state_values = True)"
      ],
      "metadata": {
        "id": "d2e7Y9tne2Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises"
      ],
      "metadata": {
        "id": "EYQMDeKmcpwQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise - Gambler's problem\n",
        "\n",
        "Consider the [gambler's problem](https://bss-osca.github.io/rl/06_mdp-1.html#sec-mdp-1-gambler)."
      ],
      "metadata": {
        "id": "GweToDSPd5gj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q1\n",
        "\n",
        "Build the model. Assume that a reward of one only is given if reach the terminal state 100. Hint: This can be modelled by adding a new terminal state 101 representing the game is over. A transition to this state is then done if reach loose or win state. Moreover, the reward is set to 1 if transition from the win state to the terminal state."
      ],
      "metadata": {
        "id": "nXAY5Innm4SI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution (Q1)\n",
        "\n",
        "def build_gambler_model(ph = 0.4, goal = 100, obj_eq_pr = True):\n",
        "    terminate = goal + 1  # terminal state\n",
        "    fail = 0  # Loose all money\n",
        "\n",
        "    mdp = MDP()\n",
        "    mdp.add_state_space(range(goal + 2))\n",
        "\n",
        "    ## Add action labels\n",
        "    for s_l in mdp.get_state_keys():\n",
        "        s = int(s_l)\n",
        "        if s > fail and s < goal: mdp.add_action_space(s_l, range(1, min(s, goal-s) + 1))\n",
        "        if s == fail: ___\n",
        "        if s == goal: ___\n",
        "        if s == terminate: ___\n",
        "\n",
        "    ## Add trans pr and rewards to actions\n",
        "    mdp.add_action(str(fail), \"Loose\", 0, {str(terminate): 1})\n",
        "    mdp.add_action(str(goal), \"Win\", 1, ___)\n",
        "    mdp.add_action(str(terminate), ___)\n",
        "    for s in range(1, goal): # states 1-99\n",
        "        s_l = str(s)\n",
        "        for a_l in mdp.get_action_keys(s_l):\n",
        "            a = int(a_l)\n",
        "            pr = {str(s+a): ph, str(s-a): ___}\n",
        "            if obj_eq_pr:\n",
        "                mdp.add_action(s_l, a_l, 0, pr)\n",
        "            else:\n",
        "                raise NotImplementedError(\"Not implemented yet\")\n",
        "    mdp.check()\n",
        "    print(\"Created MDP:\", mdp.get_mdp_info())\n",
        "    return mdp\n",
        "\n",
        "mdp = build_gambler_model()"
      ],
      "metadata": {
        "id": "Z644fZLqeXg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q2\n",
        "\n",
        "- Solve the problem using value iteration with $p_h = 0.4$.\n",
        "- Plot the state-values given states 1-99 on the x-axis. Use geom_line().\n",
        "- Plot the policy with states 1-99 on the x-axis and action on the y-axis. Use geom_col(). Your policy may not look like the policy in the book on page 84. Why?\n",
        "- What is the probability of winning if you start with 52\\$?\n"
      ],
      "metadata": {
        "id": "SKBzz26emffu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution (Q2)\n",
        "\n",
        "from plotnine import *\n",
        "from dfply import *\n",
        "\n",
        "mdp.___\n",
        "df_result = (mdp.get_policy(add_state_values = True) >>\n",
        "    rename(State = \"state\", Policy = \"action\", Value = \"v\") >>\n",
        "    mutate(State = X.State.astype(int)))\n",
        "display(df_result)\n",
        "\n",
        "## Plots\n",
        "df_result = df_result >> mask(X.State >= 1, X.State <= 99) >> mutate(Policy = X.Policy.astype(int))\n",
        "\n",
        "pt = (\n",
        "    ggplot(df_result, aes(\"State\", \"Value\"))\n",
        "    + geom_line()\n",
        "    + theme(legend_position='bottom')\n",
        "    + labs(title = \"Optimal value function\")\n",
        ")\n",
        "pt.show()\n",
        "\n",
        "pt = (\n",
        "    ___\n",
        ")\n",
        "pt.show()\n",
        "\n",
        "print(\"Probability of winning s = 52:\\n\", df_result >> mask(___))"
      ],
      "metadata": {
        "id": "qbwS65c4mgIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q3\n",
        "\n",
        "- Solve the problem using value iteration with $p_h = 0.25$ and $0.55$ and plot the state-values and the policy. What is the probability of winning in state 10 and 67?\n",
        "- Assume that $p_h = 0.55$ and consider the policy to bet $\\min(s, 100-s)$ in state $s\\in\\{1\\ldots,99\\}$. Plot the state-values and the policy."
      ],
      "metadata": {
        "id": "GaxCdmaIUBms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution (Q3)\n",
        "\n",
        "# Note using mdptoolbox as solver will not work here since the discount rate\n",
        "# is 1, i.e. convergence is not guaranteed.\n",
        "\n",
        "## p_h = 0.25\n",
        "print(\"Results ph = 0.25:\")\n",
        "mdp = build_gambler_model(ph = 0.25)\n",
        "mdp.value_iteration(1)\n",
        "df_result = mdp.get_policy(add_state_values = True) >> mutate(state = X.state.astype(int), alg = \"MDP\")\n",
        "df_result.set_index(\"state\")\n",
        "# display(df_result)\n",
        "print(\"Probability of winning s = 10 (p_h = 0.25):\", round(df_result.loc[10, 'v'],4))\n",
        "print(\"Probability of winning s = 67 (p_h = 0.25):\", round(df_result.loc[67, 'v'],4))\n",
        "\n",
        "# Plots\n",
        "\n",
        "df_result = df_result >> mask(X.state >= 1, X.state <= 99) >> mutate(action = X.action.astype(int))\n",
        "pt = (\n",
        "    ggplot(df_result, aes(\"state\", \"v\", color = \"alg\"))\n",
        "    + geom_line()\n",
        "    + theme(legend_position='bottom')\n",
        "    + labs(title = \"Optimal value function\")\n",
        ")\n",
        "pt.show()\n",
        "\n",
        "pt = (\n",
        "    ggplot(df_result, aes(\"state\", \"action\"))\n",
        "    + facet_grid(cols = \"alg\")\n",
        "    + geom_col()\n",
        "    + theme(legend_position='bottom')\n",
        "    + labs(title = \"Optimal policy\")\n",
        ")\n",
        "pt.show()\n",
        "\n",
        "\n",
        "\n",
        "## p_h = 0.55\n",
        "___\n",
        "\n",
        "\n",
        "## Fixed policy\n",
        "print(\"Results fixed policy:\")\n",
        "policy = {str(s): str(min(s, 100-s)) for s in range(1, 100)}\n",
        "policy.update({'0': 'Loose', '100': 'Win', '101': 'Dummy'})\n",
        "mdp.set_deterministic_policy(policy)\n",
        "mdp.policy_eval(1)\n",
        "df_result = mdp.get_policy(add_state_values = True) >> mutate(state = X.state.astype(int), alg = \"Pol\")\n",
        "df_result.set_index(\"state\")\n",
        "\n",
        "## Plots\n",
        "___"
      ],
      "metadata": {
        "id": "NpHDfh8yXrjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q4\n",
        "\n",
        "Modify you code and calculate the expected total reward of all actions instead when $p_h = 0.25, 0.5$ and 0.75. Use the policy iteration algorithm. Note here action rewards are the expected reward given a bet. Why is the expected total reward highest for $p_h = 0.5$?"
      ],
      "metadata": {
        "id": "7gqnvskFYl4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution (Q4)\n",
        "\n",
        "# We update the function\n",
        "def build_gambler_model(ph = 0.4, goal = 100, obj_eq_pr = True):\n",
        "    ___\n",
        "    return mdp\n",
        "\n",
        "## Next, we define a function for plotting the results\n",
        "def plot_results(ph):\n",
        "    mdp = build_gambler_model(ph = ph, obj_eq_pr = False)\n",
        "    mdp.value_iteration(1)\n",
        "    df_result = mdp.get_policy(add_state_values = True) >> mutate(state = X.state.astype(int), alg = \"MDP\")\n",
        "    df_result.set_index(\"state\")\n",
        "\n",
        "    # Plots\n",
        "    df_result = df_result >> mask(X.state >= 1, X.state <= 99) >> mutate(action = X.action.astype(int))\n",
        "\n",
        "    pt = (\n",
        "        ggplot(df_result, aes(\"state\", \"v\"))\n",
        "        + geom_line()\n",
        "        + theme(legend_position='bottom')\n",
        "        + labs(title = \"Optimal value function\")\n",
        "    )\n",
        "    pt.show()\n",
        "\n",
        "    pt = (\n",
        "        ggplot(df_result, aes(\"state\", \"action\"))\n",
        "        + geom_col()\n",
        "        + theme(legend_position='bottom')\n",
        "        + labs(title = \"Optimal policy\")\n",
        "    )\n",
        "    pt.show()\n",
        "\n",
        "    print(\"Expected total reward s = 50:\", round(df_result.loc[50, 'v'],4))\n",
        "\n",
        "print(\"Results p_h = 0.25\")\n",
        "plot_results(0.25)\n",
        "print(\"\\nResults p_h = 0.5\")\n",
        "plot_results(0.5)\n",
        "print(\"\\nResults p_h = 0.75\")\n",
        "plot_results(0.75)\n",
        "\n",
        "# Reward depends on ..."
      ],
      "metadata": {
        "id": "ncHLsJgrY515"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise - Maintenance problem\n",
        "\n",
        "Consider the [maintenance problem](https://bss-osca.github.io/rl/08_dp.html#sec-dp-maintain)."
      ],
      "metadata": {
        "id": "HQnlVuuufR_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q1\n",
        "\n",
        "\n",
        "Define the state space $S$, i.e. the the set of possible states of the system.\n",
        "\n",
        "<details>\n",
        "<summary>Solution</summary>\n",
        "$$S=\\{1,2,\\ \\ldots,\\ N,\\ N+1\\}.$$ State $s$ with $1\\leq s\\leq N$ corresponds to the situation in which an inspection reveals working condition $s$, while state $N+1$ corresponds to the situation in which an enforced repair is in progress already for one day.\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "tsPlf8IAZRhW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q2\n",
        "\n",
        "Consider actions\n",
        "$$\n",
        "a=\\left\\{\\begin{array}{ll}\n",
        "0 & \\text{if no repair.}\\\\\n",
        "1 & \\text{if preventive repair.}\\\\\n",
        "2 & \\text{if forced repair.}\\\\\n",
        "\\end{array}\\right.\n",
        "$$\n",
        "Define the action space $A(s)$ for all states $s$.\n",
        "\n",
        "<details>\n",
        "<summary>Solution</summary>\n",
        "   The set of possible actions in state $s$ is chosen as $$A(1)=\\{0\\},\\ A(s)=\\{0,1\\} \\text{ for } 1<s<N, A(N)=A(N+1)=\\{2\\}.$$\n",
        "</details>"
      ],
      "metadata": {
        "id": "skuC3iM2aCvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q3\n",
        "\n",
        "Assume that the number of possible working conditions equals $N=5$. What is the expected reward $r(s,a)$?\n",
        "\n",
        "<details>\n",
        "<summary>Solution</summary>\n",
        "The expected reward is $r(N+1,2) = 0$ and for $0 < s < N$ is $r(s,0) = 0$, $r(s,1) = -C_{ps}$. Finally, $r(N,2) = -C_{f}$ and $r(N+1,2) = 0$.\n",
        "</details>"
      ],
      "metadata": {
        "id": "YeSQvatMaNoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q4\n",
        "\n",
        "4) What is the transition probabilities?\n",
        "\n",
        "<details>\n",
        "<summary>Solution</summary>\n",
        "   The transition probabilities $p(s'|s,a)$: $$p(s'|s,0) = q_{ij}, \\text{ for } 1 \\leq s<N,$$ $$p(1|s,1) = 1 \\text{ for } 1<s<N,$$ $$p(N+1|N,2) = p(1|N+1,2) =1,$$ and zero otherwise.\n",
        "</details>"
      ],
      "metadata": {
        "id": "XN0Z0EEIaNwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q5\n",
        "\n",
        "Make a drawing of the state-expanded hypergraph for stage $t$ and $t+1$ (just by hand)."
      ],
      "metadata": {
        "id": "qpyBhBh2aN34"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q6\n",
        "\n",
        "The repair costs are given by $C_{f}=10,\\ C_{p2}=7,\\ C_{p3}=7$ and $C_{p4}=5$ and the deterioration probabilities $q_{ij}$ are given by matrix Q:"
      ],
      "metadata": {
        "id": "vtXqrrEva6ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "q_mat = np.array([\n",
        "   [0.90, 0.10, 0, 0, 0],\n",
        "   [0, 0.80, 0.10, 0.05, 0.05],\n",
        "   [0, 0, 0.70, 0.10, 0.20],\n",
        "   [0, 0, 0, 0.50, 0.50]\n",
        "])\n",
        "print(q_mat)\n",
        "print(\"Note q_ij = q_mat[i-1, j-1], e.g. q_23 =\", q_mat[1,2])"
      ],
      "metadata": {
        "id": "lUHLoowYbRvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the MDP."
      ],
      "metadata": {
        "id": "PGsZJXfeetlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution (Q6)\n",
        "\n",
        "conditions = q_mat.shape[1]  # N\n",
        "costs_prev = (0,0,7,7,5,0,0)  # cost prev rep\n",
        "action_labels = ['no rep', 'prev rep', 'forced rep']\n",
        "mdp = MDP()\n",
        "mdp.add_state_space(range(1, conditions + 2))\n",
        "\n",
        "def actions(s):\n",
        "    \"\"\"\n",
        "    Possible actions given state s.\n",
        "\n",
        "    Args:\n",
        "        s (int): Current state.\n",
        "\n",
        "    Returns:\n",
        "        list: List of possible actions.\n",
        "    \"\"\"\n",
        "    i = int(s)\n",
        "    if i == 1: return [\"no rep\"]\n",
        "    if i > 1 and i < conditions: return [\"no rep\", \"prev rep\"]\n",
        "    if i == conditions: return [\"forced rep\"]\n",
        "    if i == conditions+1: return [\"forced rep\"]\n",
        "    return None\n",
        "\n",
        "## Add trans pr and rewards\n",
        "for s in mdp.get_state_keys():\n",
        "    i = int(s)\n",
        "    for a in actions(s):\n",
        "        if a == \"no rep\":\n",
        "            rew = 0\n",
        "            q_vec = q_mat[i-1, (i-1):] # values in state row and col from state\n",
        "            pr = {str(key+i): val for key, val in enumerate(q_vec)}\n",
        "        if a == \"prev rep\":\n",
        "            rew = -costs_prev[i]\n",
        "            pr = ___\n",
        "        if a == 'forced rep' and i == conditions:\n",
        "            ___\n",
        "        if a == 'forced rep' and i == conditions + 1:\n",
        "            ___\n",
        "        mdp.add_action(s, a, rew, pr)\n",
        "mdp.check()\n",
        "mdp.get_mdp_info()"
      ],
      "metadata": {
        "id": "L-YP6NIHeyy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q7\n",
        "\n",
        "Find the optimal policy given a discount rate of $\\gamma = 0.6$. What is the expected total discounted cost of being in state 3?"
      ],
      "metadata": {
        "id": "IfWLCGEPa6m_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution (Q7)\n",
        "\n",
        "mdp.v___\n",
        "df_result = mdp.___\n",
        "df_result.set_index(\"state\", inplace = True)\n",
        "display(df_result)\n",
        "print(\"The expected total discounted cost in state 3 is:\", -round(___,2))"
      ],
      "metadata": {
        "id": "E8DRoS_XgZAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q8\n",
        "\n",
        "Find the optimal policy using value iteration given a discount rate of $\\gamma = 0.99$. Why do the algorithm need more iterations compared to when $\\gamma = 0.6$? Why are the average discounted costs higher compared to when $\\gamma = 0.6$? Is it more costly to use this optimal policy?"
      ],
      "metadata": {
        "id": "1rLqFUQJjEPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution (Q8)\n",
        "\n"
      ],
      "metadata": {
        "id": "UYM2NW-8jMh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise - Car rental problem\n",
        "\n",
        "Consider the [car [rental](https://) problem](https://bss-osca.github.io/rl/07_mdp-2.html#sec-mdp-2-car). The reward can be coded using:"
      ],
      "metadata": {
        "id": "xERxGYQDkR87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import poisson\n",
        "\n",
        "# Mean of min(D, z). Assume z >= 0\n",
        "def mean_min_d(z, lambda_):\n",
        "    \"\"\"\n",
        "    Compute the expected value of min(D, z) where D ~ Poisson(lambda).\n",
        "\n",
        "    Args:\n",
        "        z (int): Upper bound for the minimum (greater than 0).\n",
        "        lam (float): Lambda for the Poisson distribution.\n",
        "\n",
        "    Returns:\n",
        "        float: Expected value of min(D, z)\n",
        "    \"\"\"\n",
        "    if z == 0:\n",
        "        return 0\n",
        "    return np.sum(poisson.pmf(np.arange(z), lambda_) * np.arange(z)) + poisson.sf(z-1, lambda_) * z\n",
        "\n",
        "# Reward function\n",
        "def reward(x, y, a):\n",
        "    \"\"\"\n",
        "    Compute the reward based on input parameters for the car rental problem.\n",
        "\n",
        "    Args:\n",
        "        x (int): Number of cars at location 1.\n",
        "        y (int): Number of cars at location 2.\n",
        "        a (int): Number of cars moved from location 1 to location 2 (negative means moved from 2 to 1).\n",
        "\n",
        "    Returns:\n",
        "        float: Calculated reward\n",
        "    \"\"\"\n",
        "    return 10 * mean_min_d(x - a, 3) + 10 * mean_min_d(y + a, 4) - 2 * abs(a)\n",
        "\n",
        "# Test the reward function\n",
        "print(f'Reward given 20 cars at location 1, 0 cars at location 2, and 0 cars moved: {reward(20, 0, 0)}')\n",
        "print(f'Reward given 20 cars at location 1, 0 cars at location 2, and 5 cars moved: {reward(20, 0, 5)}')"
      ],
      "metadata": {
        "id": "yUih5NtKQN2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find the transition probabilities $$ p((x',y') | (x,y), a) = p(x' | \\bar{x} = x-a) p(y' | \\bar{y} = y+a),$$ we need to calculate $p(x'| \\bar{x})$ and $p(y'| \\bar{y})$."
      ],
      "metadata": {
        "id": "ow5B5M8P9Ui6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pr_min(i, k, d, lambda_demand = [3, 4], lambda_return = [3, 2]):\n",
        "    \"\"\"\n",
        "    Compute the probability that min(D_i, k) = d (assuming d <= k)\n",
        "\n",
        "    This is used to calculate the probability of the number of cars demanded or\n",
        "    rented being less than or equal to the number of cars available or requested.\n",
        "\n",
        "    Args:\n",
        "        i (int): Index for the lambda value (0 for location 1 demand, 1 for location 2 demand,\n",
        "                 2 for location 1 return, 3 for location 2 return).\n",
        "        k (int): The upper cap value.\n",
        "        d (int): The target minimum value.\n",
        "\n",
        "    Returns:\n",
        "        float: Probability that min(D_i, k) = d\n",
        "    \"\"\"\n",
        "    if k == 0:\n",
        "        return 1 if d == 0 else 0\n",
        "    # Determine which lambda to use based on the index i\n",
        "    if i == 0: # Location 1 demand\n",
        "        lam = lambda_demand[0]\n",
        "    elif i == 1: # Location 2 demand\n",
        "        lam = lambda_demand[1]\n",
        "    elif i == 2: # Location 1 return\n",
        "        lam = lambda_return[0]\n",
        "    elif i == 3: # Location 2 return\n",
        "        lam = lambda_return[1]\n",
        "    else:\n",
        "        raise ValueError(\"Invalid index for lambda.\")\n",
        "\n",
        "    prob_dist = np.concatenate([poisson.pmf(np.arange(k), lam), [poisson.sf(k-1, lam)]])\n",
        "    return prob_dist[d] if d < len(prob_dist) else 0\n",
        "\n",
        "\n",
        "def trans_pr_x(x_next, x_bar):\n",
        "    \"\"\"\n",
        "    Compute transition probability Pr(x_next | x_bar).\n",
        "\n",
        "    This calculates the probability of the number of cars at location 1 transitioning\n",
        "    to x_next given x_bar cars after the move and before return/return.\n",
        "\n",
        "    Args:\n",
        "        x_next (int): The number of cars at location 1 in the next state.\n",
        "        x_bar (int): The number of cars at location 1 after the action (movement).\n",
        "\n",
        "    Returns:\n",
        "        float: Transition probability\n",
        "    \"\"\"\n",
        "    prob = 0.0\n",
        "    for d1_actual in range(x_bar + 1): # d1_actual is min(D1, x_bar)\n",
        "        # Probability of min(D1, x_bar) being d1_actual\n",
        "        pr_d1_actual = pr_min(0, x_bar, d1_actual) # Use index 0 for location 1 demand\n",
        "\n",
        "        cars_after_demand1 = x_bar - d1_actual\n",
        "        cars_can_return_to_loc1 = 20 - cars_after_demand1\n",
        "\n",
        "        # The number of returns at location 1 is min(H1, cars_can_return_to_loc1)\n",
        "        # The next state x_next = cars_after_demand1 + min(H1, cars_can_return_to_loc1)\n",
        "        # So, min(H1, cars_can_return_to_loc1) = x_next - cars_after_demand1\n",
        "        h1_actual = x_next - cars_after_demand1\n",
        "\n",
        "        if h1_actual >= 0 and h1_actual <= cars_can_return_to_loc1:\n",
        "             prob += pr_d1_actual * pr_min(2, cars_can_return_to_loc1, h1_actual) # Use index 2 for location 1 return\n",
        "\n",
        "    return prob\n",
        "\n",
        "\n",
        "def trans_pr_y(y_next, y_bar):\n",
        "    \"\"\"\n",
        "    Compute transition probability Pr(y_next | y_bar).\n",
        "\n",
        "    This calculates the probability of the number of cars at location 2 transitioning\n",
        "    to y_next given y_bar cars after the move and before return/return.\n",
        "\n",
        "    Args:\n",
        "        y_next (int): The number of cars at location 2 in the next state.\n",
        "        y_bar (int): The number of cars at location 2 after the action (movement).\n",
        "\n",
        "    Returns:\n",
        "        float: Transition probability\n",
        "    \"\"\"\n",
        "    prob = 0.0\n",
        "    for d2_actual in range(y_bar + 1): # d2_actual is min(D2, y_bar)\n",
        "        # Probability of min(D2, y_bar) being d2_actual\n",
        "        pr_d2_actual = pr_min(1, y_bar, d2_actual) # Use index 1 for location 2 demand\n",
        "\n",
        "        cars_after_demand2 = y_bar - d2_actual\n",
        "        cars_can_return_to_loc2 = 20 - cars_after_demand2\n",
        "\n",
        "        # The number of returns at location 2 is min(H2, cars_can_return_to_loc2)\n",
        "        # The next state y_next = cars_after_demand2 + min(H2, cars_can_return_to_loc2)\n",
        "        # So, min(H2, cars_can_return_to_loc2) = y_next - cars_after_demand2\n",
        "        h2_actual = y_next - cars_after_demand2\n",
        "\n",
        "        if h2_actual >= 0 and h2_actual <= cars_can_return_to_loc2:\n",
        "             prob += pr_d2_actual * pr_min(3, cars_can_return_to_loc2, h2_actual) # Use index 3 for location 2 return\n",
        "\n",
        "    return prob"
      ],
      "metadata": {
        "id": "AbfIDcLnTGfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now store the values of $p(x'| \\bar{x})$ and $p(y'| \\bar{y})$ in two matrices (in the rows we have $\\bar{x}$):"
      ],
      "metadata": {
        "id": "Yu0nKCGIZiv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize matrices\n",
        "mat = np.zeros((21, 21))\n",
        "mat_trans_pr_x = mat.copy()\n",
        "mat_trans_pr_y = mat.copy()\n",
        "\n",
        "# Populate the matrices\n",
        "for r in range(21):\n",
        "    x_bar = r\n",
        "    for c in range(21):\n",
        "        x_next = c\n",
        "        mat_trans_pr_x[r, c] = trans_pr_x(x_next, x_bar)\n",
        "        mat_trans_pr_y[r, c] = trans_pr_y(x_next, x_bar)\n",
        "\n",
        "## Check if row sums are 1\n",
        "print(np.sum(mat_trans_pr_x, axis=1))\n",
        "print(np.sum(mat_trans_pr_y, axis=1))"
      ],
      "metadata": {
        "id": "5b37KI2N-JVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can define the transition function $$ p((x',y') | (x,y), a) = p(x' | \\bar{x} = x-a) p(y' | \\bar{y} = y+a):$$"
      ],
      "metadata": {
        "id": "95R6KPonTGJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trans_pr(x, y, x_next, y_next, a, mat_x = mat_trans_pr_x, mat_y = mat_trans_pr_y):\n",
        "    \"\"\"\n",
        "    Compute the joint transition probability from state (x, y) to (x_next, y_next) given action a.\n",
        "\n",
        "    Args:\n",
        "        x (int): Number of cars at location 1 in the current state.\n",
        "        y (int): Number of cars at location 2 in the current state.\n",
        "        x_next (int): Number of cars at location 1 in the next state.\n",
        "        y_next (int): Number of cars at location 2 in the next state.\n",
        "        a (int): Number of cars moved from location 1 to location 2 (negative means moved from 2 to 1).\n",
        "\n",
        "    Returns:\n",
        "        float: The joint transition probability P((x_next, y_next) | (x, y), a).\n",
        "    \"\"\"\n",
        "    x_bar = x - a\n",
        "    y_bar = y + a\n",
        "    return mat_x[x_bar, x_next] * mat_y[y_bar, y_next]"
      ],
      "metadata": {
        "id": "JRT4b5fL-wiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q1\n",
        "\n",
        "Build the model."
      ],
      "metadata": {
        "id": "mS42-G4qlgGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "state_labels = [f'({x}, {y})' for x in range(21) for y in range(21) ]\n",
        "mdp = MDP()\n",
        "mdp.add_state_space(state_labels)\n",
        "\n",
        "# Iterate through all states and actions to set up the MDP\n",
        "for x in range(21):\n",
        "    for y in range(21):\n",
        "        s_l = f'({x}, {y})' # Current state label\n",
        "        for a in range(-5, 6):\n",
        "            a_l = str(a) # Action label\n",
        "            x_bar = x - a # Cars at location 1 after move\n",
        "            y_bar = y + a # Cars at location 2 after move\n",
        "\n",
        "            # Check for illegal actions (moving more cars than available or exceeding capacity)\n",
        "            if x_bar < 0 or y_bar < 0 or x_bar > 20 or y_bar > 20:\n",
        "                continue\n",
        "\n",
        "            # Calculate reward for the state-action pair\n",
        "            rew = ___\n",
        "\n",
        "            # Calculate transition probabilities to all possible next states (x_next, y_next)\n",
        "            trans_probs = {}\n",
        "            for x_next in range(21):\n",
        "                for y_next in range(21):\n",
        "                    pr = trans_pr(___)\n",
        "                    if pr > 1e-16: # Only add non-negligible probabilities\n",
        "                        trans_probs[f'({x_next}, {y_next})'] = ___\n",
        "\n",
        "            # Set the action in the MDP builder\n",
        "            mdp.add_action(___)\n",
        "mdp.check()\n",
        "mdp.get_mdp_info()"
      ],
      "metadata": {
        "id": "itH82f7fqCx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q2\n",
        "\n",
        "Solve the problem using policy iteration with a discount rate $\\gamma = 0.5$.\n",
        "\n",
        "- What is the optimal action in state $(1,15)$?\n",
        "- What is the expected total discounted reward of being in state $(10,3)$?\n",
        "- Make a plot of the optimal policy with $x$ on the y-axis and $y$ on the x-axis, plotting the action."
      ],
      "metadata": {
        "id": "bAQ2cs7ICXda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "mdp.___\n",
        "df_result = mdp.___\n",
        "# display(df_result)\n",
        "\n",
        "# Plot\n",
        "df_result = (df_result\n",
        "   >> separate(X.state, into = [\"x\", \"y\"], sep = \",\", remove = False)\n",
        "   >> separate(X.x, into = [\"z\", \"x\"], remove = False, convert = True)\n",
        "   >> separate(X.y, into = [\"z\", \"y\"], remove = False, convert = True)\n",
        "   >> select(~X.z)\n",
        ")\n",
        "# df_result\n",
        "display(df_result >> mask((X.state == '(1, 15)') | (X.state == ___)))\n",
        "# Optimal action in (1, 15) is to move 3 cars from loc 2 to 1\n",
        "# The expected total discounted reward of being in state (10, 3) is approx 129.7.\n",
        "\n",
        "pt = (\n",
        "    ggplot(df_result, aes(\"x\", \"y\", label = \"action\", color = \"action\"))\n",
        "    + geom_label(size = 7)\n",
        "    + theme(legend_position='none')\n",
        "    + labs(title = \"Optimal policy for gamma = 0.5\")\n",
        ")\n",
        "pt.show()"
      ],
      "metadata": {
        "id": "B2zfScYgCmI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q3\n",
        "\n",
        "Solve the problem using value iteration. Check if the policy and state-values are the same as in Question 2. If not why could this be okay?\n"
      ],
      "metadata": {
        "id": "lIOSvWC5v05U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "___"
      ],
      "metadata": {
        "id": "4KZjPlrHv6zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q4\n",
        "\n",
        "Solve the problem using policy iteration with a discount rate $\\gamma = 0.9$. Why are the state-values higher now?"
      ],
      "metadata": {
        "id": "xXZgfP2Fy5BL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "___"
      ],
      "metadata": {
        "id": "NKrN6v7AzEzi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}