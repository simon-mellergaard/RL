{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simon-mellergaard/RL/blob/main/Exercises/05_bandit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Useful presteps\n",
        "\n",
        "## debugging (set off when finish coding)\n",
        "# Use ipdb.set_trace() to set breakpoint\n",
        "# Useful ipdb commands\n",
        "    # p <variable>: Print the value of a variable.\n",
        "    # n: Execute the next line of code.\n",
        "    # c: Continue execution until the next breakpoint or the end of the program.\n",
        "    # q: Quit the debugger.\n",
        "# %debug # Can be run in a code cell after the cell with error\n",
        "# !pip install ipdb\n",
        "# import ipdb\n",
        "# %pdb on\n",
        "# %pdb off\n",
        "\n",
        "\n",
        "## Missing packages\n",
        "!pip install dfply\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "np.set_printoptions(precision=4, suppress=True)\n",
        "import pandas as pd\n",
        "from plotnine import ggplot, aes, geom_line, labs, theme\n",
        "from dfply import *"
      ],
      "metadata": {
        "id": "KPtz0Z7vtCU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-armed bandits  \n",
        "\n",
        "This notebook consider the k-armed bandit problem which is a sequential decision problem with one state and $k$ actions.\n",
        "\n",
        "Imagine you are facing a wall with slot machines, and each one pays out at a different rate. A natural way to figure out how to make the most money (rewards) would be to try each at random for a while (exploration), and start playing the higher paying ones once you have gained some experience (exploitation). That is, from an agent/environment point of view the agent considers a single state and have to choose among actions given the environment representing the bandits. Only the rewards from the bandits are unknown, but the agent observe samples of the reward of an action and can use this to estimate the expected reward of that action. The objective is to find an optimal policy that maximize the total expected reward.\n",
        "\n",
        "Let us try to implement the algorithm using an agent and environment class. First we define the agent that do actions based on an $\\epsilon$-greedy strategy, stores the estimated $Q$ values and the number of times an action has been chosen:"
      ],
      "metadata": {
        "id": "cx3AzyFhkvDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Agent (code)\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class RLAgent:\n",
        "    \"\"\"\n",
        "    RLAgent class representing the RL agent.\n",
        "\n",
        "    Attributes:\n",
        "        qV (np.ndarray): Q estimates.\n",
        "        nV (np.ndarray): Action counter.\n",
        "        k (int): Number of bandits.\n",
        "        epsilon (float): Epsilon used in epsilon greedy action selection.\n",
        "    \"\"\"\n",
        "    def __init__(self, k=10, epsilon=0.01, ini=0):\n",
        "        \"\"\"\n",
        "        Create an RLAgent object.\n",
        "\n",
        "        Args:\n",
        "            k (int): Number of bandits.\n",
        "            epsilon (float): Epsilon used in epsilon greedy action selection.\n",
        "            ini (float): Initial qV values.\n",
        "        \"\"\"\n",
        "        self.epsilon = epsilon\n",
        "        self.qV = np.full(k, ini, dtype=float)  # Initialize Q estimates to ini values\n",
        "        self.nV = np.zeros(k, dtype=int)    # Initialize action counter\n",
        "        self.k = k\n",
        "\n",
        "    def clear_learning(self):\n",
        "        \"\"\"\n",
        "        Clear learning, resetting Q estimates and action counter.\n",
        "        \"\"\"\n",
        "        self.qV.fill(0)\n",
        "        self.nV.fill(0)\n",
        "\n",
        "    def select_action_eg(self):\n",
        "        \"\"\"\n",
        "        Select next action using an epsilon greedy strategy.\n",
        "\n",
        "        Returns:\n",
        "            int: Action (index).\n",
        "        \"\"\"\n",
        "        if random.random() <= self.epsilon:  # explore\n",
        "            a = random.choice(range(self.k))\n",
        "        else:  # exploit\n",
        "            max_q = max(self.qV)\n",
        "            # Choose a random action if more than one has the maximal Q value\n",
        "            a = random.choice([i for i, q in enumerate(self.qV) if q == max_q])\n",
        "        return a\n",
        "\n",
        "    def select_action_ucb(self, c, t):\n",
        "        \"\"\"\n",
        "        Select next action using Upper-Confidence Bound (UCB).\n",
        "\n",
        "        Args:\n",
        "            c (float): Exploration parameter.\n",
        "            t (int): Current time step.\n",
        "\n",
        "        Returns:\n",
        "            int: Action (index).\n",
        "        \"\"\"\n",
        "        # If any actions are not chosen yet, we pick one of those\n",
        "        if np.any(self.nV == 0):\n",
        "            a = np.where(self.nV == 0)[0][0]\n",
        "            return a\n",
        "        # Calculate the UCB value for each action\n",
        "        val = self.qV + c * np.sqrt(np.log(t) / (self.nV))\n",
        "        a = np.argmax(val)  # Select the action with the highest UCB value\n",
        "        return a\n",
        "\n",
        "    def update_q(self, a, r):\n",
        "        \"\"\"\n",
        "        Update learning values (including action counter).\n",
        "\n",
        "        Args:\n",
        "            a (int): Action taken.\n",
        "            r (float): Reward received.\n",
        "        \"\"\"\n",
        "        self.nV[a] += 1  # Increment action counter\n",
        "        self.qV[a] += (r - self.qV[a]) / self.nV[a] # Update Q estimate using the sample average formula\n"
      ],
      "metadata": {
        "id": "LTZO_LZNuQ1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, the environment generating rewards. The true mean reward $q_*(a)$ of an action is selected according to a normal (Gaussian) distribution with mean 0 and variance one. The observed reward is then generated using a normal distribution with mean $q_*(a)$ and variance one:"
      ],
      "metadata": {
        "id": "LMBdMWfdu3_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Environment (code)\n",
        "\n",
        "class RLEnvironment:\n",
        "    \"\"\"\n",
        "    RLEnvironment class representing the RL environment.\n",
        "\n",
        "    Assumes that bandits are normally distributed with a mean and standard deviation of one.\n",
        "    \"\"\"\n",
        "    def __init__(self, k=10, mean_val = None):\n",
        "        \"\"\"\n",
        "        Create an RLEnvironment object.\n",
        "\n",
        "        Args:\n",
        "            k (int): Number of bandits.\n",
        "        \"\"\"\n",
        "        self.k = k\n",
        "        if mean_val is None: # Check if mean_val was provided\n",
        "            self.mean_val = np.random.normal(0, 1, self.k)  # Generate means using self.k\n",
        "        else:\n",
        "            if len(mean_val) != self.k:\n",
        "                raise ValueError(\"mean_val must have the same length as k\")\n",
        "            self.mean_val = mean_val  # Use the provided mean_val\n",
        "\n",
        "    def reward(self, a):\n",
        "        \"\"\"\n",
        "        Sample reward of a bandit.\n",
        "\n",
        "        Args:\n",
        "            a (int): Bandit (index).\n",
        "\n",
        "        Returns:\n",
        "            float: The reward.\n",
        "        \"\"\"\n",
        "        # Pick a random value from N(self.mean_val[a], 1)\n",
        "        return np.random.normal(self.mean_val[a], 1)\n",
        "\n",
        "    def optimal_action(self):\n",
        "        \"\"\"\n",
        "        Returns action with the best mean.\n",
        "\n",
        "        Returns:\n",
        "            int: Index of the optimal action.\n",
        "        \"\"\"\n",
        "        return np.argmax(self.mean_val)"
      ],
      "metadata": {
        "id": "MAdnOZ_VuscL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimating the value of an action\n",
        "\n",
        "To test the RL algorithm we use a function returning two plots that compare the performance:"
      ],
      "metadata": {
        "id": "y1qE-4AFvNKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Measure performance (code)\n",
        "\n",
        "# Assuming RLAgent and RLEnvironment classes are defined in previous cells as provided in the context\n",
        "\n",
        "def performance(k=10, time_steps=1000, runs=500, epsilons=[0, 0.01, 0.1, 0.5], ini=0):\n",
        "    \"\"\"\n",
        "    Performance of the bandit algorithm using different epsilons.\n",
        "\n",
        "    Args:\n",
        "        k (int): Number of bandits.\n",
        "        time_steps (int): Time steps used to estimate the expected rewards.\n",
        "        runs (int): Number of runs with a new environment generated.\n",
        "        epsilons (list): List of epsilons to be tested.\n",
        "        ini (float): Initial action-value estimates.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing two plotnine plots ('ptR' for average reward,\n",
        "              'ptO' for average number of times optimal action chosen).\n",
        "    \"\"\"\n",
        "    rew = np.zeros((time_steps, len(epsilons)))   # rewards (one col for each eps)\n",
        "    best = np.zeros((time_steps, len(epsilons)))  # add 1 if find the best action\n",
        "\n",
        "    for run in range(runs):\n",
        "        env = RLEnvironment(k)\n",
        "        oA = env.optimal_action()  # the best action to choose\n",
        "        for i in range(len(epsilons)):\n",
        "            agent = RLAgent(k, epsilons[i], ini)\n",
        "            for t in range(time_steps):\n",
        "                a = agent.select_action_eg()\n",
        "                r = env.reward(a)\n",
        "                agent.update_q(a, r)\n",
        "                rew[t, i] += r  # sum of rewards generated at t (over all runs)\n",
        "                best[t, i] += (a == oA) # times find best actions\n",
        "\n",
        "    # Calculate average rewards and optimal action counts over runs\n",
        "    # Use pandas DataFrames as the base for dfply operations\n",
        "    rew_df = pd.DataFrame(rew / runs, columns=epsilons)\n",
        "    best_df = pd.DataFrame(best / runs, columns=epsilons)\n",
        "\n",
        "    # Process data for average reward plot using dfply and pandas\n",
        "    dat1 = (rew_df\n",
        "            >> gather('epsilon', 'reward', add_id=True) # to long format\n",
        "            >> rename(t = X._ID)\n",
        "            >> mutate(t = X.t + 1)\n",
        "            >> group_by(X.epsilon)\n",
        "            >> mutate(All=X.reward.cumsum() / X.t,\n",
        "                      moving_avg_50=X.reward.rolling(window=50, min_periods=1).mean())  # Moving average calculation requires pandas rolling directly\n",
        "            >> select(~X.reward)\n",
        "            >> gather('name', 'value', ['All', 'moving_avg_50']) # Use gather from dfply\n",
        "    )\n",
        "\n",
        "    # Process data for optimal action plot using dfply and pandas\n",
        "    dat2 = (best_df\n",
        "            >> gather('epsilon', 'optimal', add_id=True) # Use gather from dfply\n",
        "            >> rename(t = X._ID)\n",
        "            >> mutate(t = X.t + 1)\n",
        "            >> group_by(X.epsilon)\n",
        "            >> mutate(All=X.optimal.cumsum() / X.t,\n",
        "                      moving_avg_50=X.optimal.rolling(window=50, min_periods=1).mean())  # Moving average calculation requires pandas rolling directly\n",
        "            >> select(~X.optimal)\n",
        "            >> gather('name', 'value', ['All', 'moving_avg_50']) # Use gather from dfply\n",
        "    )\n",
        "\n",
        "    # Create the average reward plot\n",
        "    pt1 = (\n",
        "        ggplot(dat1, aes(x='t', y='value', color='epsilon', linetype='name'))\n",
        "        + geom_line()\n",
        "        + labs(y='Average reward per time unit', x='Time', title=f'Average over {runs} runs', color='Epsilon', linetype='')\n",
        "        + theme(legend_position='bottom')\n",
        "    )\n",
        "    pt1\n",
        "\n",
        "    # Create the optimal action plot\n",
        "    pt2 = (\n",
        "        ggplot(dat2, aes(x='t', y='value', color='epsilon', linetype='name'))\n",
        "        + geom_line()\n",
        "        + labs(y='Average number of times optimal action chosen', x='Time', title=f'Average over {runs} runs', color='Epsilon', linetype='')\n",
        "        + theme(legend_position='bottom')\n",
        "    )\n",
        "\n",
        "    return {'ptR': pt1, 'ptO': pt2}\n",
        "\n",
        "# Set seed for reproducibility\n",
        "random.seed(3276)\n",
        "np.random.seed(3276)\n",
        "\n",
        "# Run the performance test\n",
        "plots = performance(runs=2000, time_steps=1000)\n",
        "# Print the plots\n",
        "plots['ptR'].show()\n",
        "plots['ptO'].show()"
      ],
      "metadata": {
        "id": "e8f0P6ng9NTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The solid line shows averages over all the runs from $t=1$ to the considered time-step while the dotted line is a moving average over the last 50 time-steps.\n"
      ],
      "metadata": {
        "id": "UX6bvvg19L8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Your turn\n",
        "\n",
        "1. Why is the 50 time-step average higher than the overall average?\n",
        "\n",
        "2. Why is a greedy approach ($\\epsilon = 0$) in general bad?\n",
        "\n",
        "3. Why is an $\\epsilon$-greedy approach better ($\\epsilon = 0.1$)? That is, exploration is beneficial.\n",
        "\n",
        "4. Why is to much exploration not good ($\\epsilon = 0.5$)?\n",
        "\n",
        "5. What is the effect of reducing the number of time steps on the plots?"
      ],
      "metadata": {
        "id": "jru_S5wH9L5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solutions to questions (code)\n",
        "\n",
        "# Q2\n",
        "eps = 0\n",
        "print(f'\\nEpsilon = {eps}')\n",
        "env = RLEnvironment(k = 5)\n",
        "print(f'True expected reward values: {env.mean_val}')\n",
        "agent = RLAgent(k = 5, epsilon = eps)\n",
        "for t in range(1000):\n",
        "    a = agent.select_action_eg()\n",
        "    r = env.reward(a)\n",
        "    agent.update_q(a, r)\n",
        "print(f'Estimated expected reward values: {agent.qV}')\n",
        "print(f'Action counts: {agent.nV}')\n",
        "print(f'Optimal action: {env.optimal_action()}')\n",
        "\n",
        "\n",
        "# Q3\n",
        "\n",
        "\n",
        "# Q4\n"
      ],
      "metadata": {
        "id": "2PJI8gDKPkhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimistic initial values\n",
        "\n",
        "The methods discussed so far are dependent to some extent on the initial action-value estimate i.e. they are biased by their initial estimates. For methods with constant \\(\\alpha\\) this bias is permanent. We may set initial value estimates artificially high to encourage exploration in the short run.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "For instance, by setting initial values of $Q$ to 5 rather than 0 we encourage exploration, even in the greedy case. Here the agent will almost always be disappointed with it's samples because they are less than the initial estimate and so will explore elsewhere until the values converge."
      ],
      "metadata": {
        "id": "WF0dZGW7ssju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Your Turn (code)\n",
        "\n",
        "# Set seed for reproducibility\n",
        "random.seed(3276)\n",
        "np.random.seed(3276)\n",
        "\n",
        "# What will be the effect of setting the initial values to 5 instead of 0?\n",
        "# Run the performance function and comment on the best exploration rates.\n"
      ],
      "metadata": {
        "id": "CAennMsps3Xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "## Upper-Confidence Bound Action Selection\n",
        "\n",
        "An $\\epsilon$-greed algorithm choose the action to explore with equal probability in an exploration step. It would be better to select among non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainty in those estimates. One way to do this is to select actions using the *upper-confidence bound (UCB)*:\n",
        "\\begin{equation}\n",
        "\tA_t = \\arg\\max_a \\left(Q_t(a) + c\\sqrt{\\frac{\\ln t}{N_t(a)}}\\right),\n",
        "\\end{equation}\n",
        "\n",
        "Note the square root term is a measure of the uncertainty in our estimate (see Figure \\@ref(fig:srt)).\n",
        "\n",
        "* It is proportional to \\(t\\) i.e. how many time-steps have passed and inversely proportional to \\(N_t(a)\\) i.e. how many times that action has been visited.\n",
        "* The more time has passed, and the less we have sampled an action, the higher our upper-confidence-bound.\n",
        "* As the timesteps increases, the denominator dominates the numerator as the ln term flattens.\n",
        "* Each time we select an action our uncertainty decreases because $N$ is the denominator of this equation.\n",
        "* If $N_t(a) = 0$ then we consider $a$ as a maximal action, i.e. we select first among actions with $N_t(a) = 0$.\n",
        "* The parameter $c>0$ controls the degree of exploration. Higher $c$ results in more weight on the uncertainty.\n",
        "\n",
        "Since upper-confidence bound action selection select actions according to their potential, it is expected to perform better than $\\epsilon$-greedy methods.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xjYmtu7vkppr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Your Turn (code)\n",
        "\n",
        "# Run the algorithm using UCB Action Selection with c = 10, 500 runs, and 300 timesteps\n",
        "def performance_ucb(k=10, time_steps=2000, runs=500, ini=0, c = 5):\n",
        "    \"\"\"\n",
        "    Performance of the bandit algorithm using different epsilons (Python version).\n",
        "\n",
        "    Args:\n",
        "        k (int): Number of bandits.\n",
        "        time_steps (int): Time steps.\n",
        "        runs (int): Number of runs with a new environment generated.\n",
        "        epsilons (list): List of epsilons to be tested.\n",
        "        ini (float): Initial value estimates.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing two plotnine plots ('ptR' for average reward,\n",
        "              'ptO' for average number of times optimal action chosen).\n",
        "    \"\"\"\n",
        "    # <Your code here>\n",
        "\n",
        "    # Calculate average rewards and optimal action counts over runs\n",
        "    # Use pandas DataFrames as the base for dfply operations\n",
        "    rew_df = pd.DataFrame(rew / runs)\n",
        "    best_df = pd.DataFrame(best / runs)\n",
        "\n",
        "    # Process data for average reward plot using dfply and pandas\n",
        "    dat1 = (rew_df\n",
        "            >> gather('epsilon', 'reward', add_id=True) # Use gather from dfply\n",
        "            >> rename(t = X._ID)\n",
        "            >> mutate(t = X.t + 1)\n",
        "            >> group_by(X.epsilon)\n",
        "            >> mutate(All=X.reward.cumsum() / X.t,\n",
        "                      moving_avg_50=X.reward.rolling(window=50, min_periods=1).mean())  # Moving average calculation requires pandas rolling directly\n",
        "            >> select(~X.reward)\n",
        "            >> gather('name', 'value', ['All', 'moving_avg_50']) # Use gather from dfply\n",
        "\n",
        "    )\n",
        "\n",
        "    # Process data for optimal action plot using dfply and pandas\n",
        "    dat2 = (best_df\n",
        "            >> gather('epsilon', 'optimal', add_id=True) # Use gather from dfply\n",
        "            >> rename(t = X._ID)\n",
        "            >> mutate(t = X.t + 1)\n",
        "            >> group_by(X.epsilon)\n",
        "            >> mutate(All=X.optimal.cumsum() / X.t,\n",
        "                      moving_avg_50=X.optimal.rolling(window=50, min_periods=1).mean())  # Moving average calculation requires pandas rolling directly\n",
        "            >> select(~X.optimal)\n",
        "            >> gather('name', 'value', ['All', 'moving_avg_50']) # Use gather from dfply\n",
        "    )\n",
        "\n",
        "    # Create the average reward plot\n",
        "    pt1 = (\n",
        "        ggplot(dat1, aes(x='t', y='value', linetype='name'))\n",
        "        + geom_line()\n",
        "        + labs(y='Average reward per time unit', x='Time', title=f'Average over {runs} runs', color='', linetype='')\n",
        "        + theme(legend_position='bottom')\n",
        "    )\n",
        "\n",
        "    # Create the optimal action plot\n",
        "    pt2 = (\n",
        "        ggplot(dat2, aes(x='t', y='value', linetype='name'))\n",
        "        + geom_line()\n",
        "        + labs(y='Average number of times optimal action chosen', x='Time', title=f'Average over {runs} runs', color='', linetype='')\n",
        "        + theme(legend_position='bottom')\n",
        "    )\n",
        "\n",
        "    return {'ptR': pt1, 'ptO': pt2}\n",
        "\n",
        "# Set seed for reproducibility\n",
        "random.seed(3276)\n",
        "np.random.seed(3276)\n",
        "\n",
        "# Run the performance test\n",
        "plots = performance_ucb()\n",
        "# Print the plots\n",
        "plots['ptR'].show()\n",
        "plots['ptO'].show()"
      ],
      "metadata": {
        "id": "qArJZLUxtt_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises"
      ],
      "metadata": {
        "id": "JUKOdK_UqKRJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise - Advertising\n",
        "\n",
        "Suppose you are an advertiser seeking to optimize which ads to show visitors on a particular website. For each visitor, you can choose one out of a collection of ads, and your goal is to maximize the number of clicks over time. Assume that:\n",
        "\n",
        "* You have $k=5$ adds to choose among.\n",
        "* If add $A$ is chosen then the user clicks the add with probability $p_A$ which can be seen as the unknown click trough rate CTR (or an average reward).\n",
        "* The CTRs are unknown and samples can be picked using the `RLAdEnv` class and the reward function which returns 1 if click on ad and 0 otherwise.\n",
        "\n",
        "In the class the true CTRs can be observed but in practice this is hidden from the agent (you).\n",
        "\n"
      ],
      "metadata": {
        "id": "0iIkDZxxbtMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class RLAdEnv:\n",
        "    \"\"\"\n",
        "    RLAdEnv class representing the RL advertising environment.\n",
        "\n",
        "    Attributes:\n",
        "        pr_succes (list): Click-through rates (unknown to the agent).\n",
        "        k (int): Number of ads.\n",
        "    \"\"\"\n",
        "    def __init__(self, rewards=(1,1,1,1,1)):\n",
        "        \"\"\"\n",
        "        Create an RLAdEnv object.\n",
        "        Initializes the click-through rates and the number of ads.\n",
        "        \"\"\"\n",
        "        self.pr_succes = [0.1, 0.83, 0.85, 0.5, 0.7] # True click through rates (unknown to the agent)\n",
        "        self.k = len(self.pr_succes) # Number of ads.\n",
        "        self.rewards = rewards # rewards given if click\n",
        "\n",
        "    def reward(self, a):\n",
        "        \"\"\"\n",
        "        Sample reward for a given ad.\n",
        "\n",
        "        Args:\n",
        "            a (int): Ad index (0-based).\n",
        "\n",
        "        Returns:\n",
        "            int: 1 if click on ad, 0 otherwise.\n",
        "        \"\"\"\n",
        "        # Sample from a binomial distribution with n=1 (single trial)\n",
        "        # and probability of success equal to the CTR of the selected ad.\n",
        "        # np.random.binomial(n, p, size)\n",
        "        return np.random.binomial(1, self.pr_succes[a]) * self.rewards[a]\n",
        "\n",
        "    def optimal_action(self):\n",
        "        \"\"\"\n",
        "        Returns the index of the ad with the best mean (highest CTR).\n",
        "\n",
        "        Returns:\n",
        "            int: Index of the optimal action (0-based).\n",
        "        \"\"\"\n",
        "        # np.argmax returns the index of the maximum value in an array/list.\n",
        "        return np.argmax(np.array(self.pr_succes) * np.array(self.rewards))\n",
        "\n",
        "# Example usage:\n",
        "# Create an instance of the advertising environment\n",
        "env = RLAdEnv()\n",
        "\n",
        "# Get a sample reward for choosing ad at index 1 (the second ad)\n",
        "# This will return 0 or 1 based on the ad's CTR\n",
        "reward_for_ad_1 = env.reward(1)\n",
        "print(f\"Reward for choosing ad at index 1: {reward_for_ad_1}\")\n",
        "\n",
        "# Find the index of the optimal ad\n",
        "best_ad_index = env.optimal_action()\n",
        "print(f\"Index of the optimal ad: {best_ad_index}\")\n",
        "\n",
        "# Display the true click-through rates (these are usually unknown to the agent)\n",
        "true_ctrs = env.pr_succes\n",
        "print(f\"True CTRs of the ads: {true_ctrs}\")"
      ],
      "metadata": {
        "id": "Df1pWZ-DZB7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q1\n",
        "\n",
        "Consider an $\\epsilon$-greedy algorithm to find the best ad. Assume the webpage is visited by 10000 users per day.\n",
        "\n",
        "Run the $\\epsilon$-greedy algorithm with $\\epsilon = 0.01, 0.1, 0.5$ over the 10000 steps. What are the estimated CTRs for each action ($Q_t(a)$)? What is the average number of clicks per user (average total reward)? Hints: You may use the RL agent defined and do a `for` loop that find the action, reward and the total reward so far. Next, you just have to calculate averages and return the correct values."
      ],
      "metadata": {
        "id": "xeQcxt7gb0yL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your solution here\n",
        "\n",
        "def test_eg(env, epsilon, steps=10000):\n",
        "    \"\"\"\n",
        "    Tests the performance of the epsilon-greedy bandit algorithm in the advertising environment.\n",
        "\n",
        "    Args:\n",
        "        epsilon (float): The probability of selecting a random action (exploration).\n",
        "        steps (int): The total number of time steps (users) for the simulation. Defaults to 10000.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing:\n",
        "              - 'qV': The final estimated Q-values (CTRs) for each ad.\n",
        "              - 'avg_reward': The average reward (clicks) per user over the simulation.\n",
        "    \"\"\"\n",
        "    # Create a new RLAgent instance for the simulation.\n",
        "    # The agent is initialized with 5 actions (ads) and the specified epsilon.\n",
        "    agent = RLAgent(k=5, epsilon=epsilon)\n",
        "\n",
        "    # Initialize the total reward accumulated over the simulation.\n",
        "    total_reward = 0\n",
        "\n",
        "    # Loop through each time step, simulating a new user.\n",
        "    for t in range(steps):\n",
        "        # The agent selects an action (ad) using the epsilon-greedy strategy.\n",
        "\n",
        "\n",
        "        # The environment provides a reward (click or no click) for the chosen ad.\n",
        "\n",
        "\n",
        "        # Add the received reward to the total reward.\n",
        "\n",
        "\n",
        "        # The agent updates its internal estimates (Q-values and action counts)\n",
        "        # based on the action taken and the reward received.\n",
        "\n",
        "\n",
        "    # Calculate the average reward per step (user).\n",
        "    # average_reward =\n",
        "\n",
        "    # Return the final estimated Q-values and the average reward.\n",
        "\n",
        "\n",
        "\n",
        "# Set seeds for reproducibility of results\n",
        "random.seed(327)\n",
        "np.random.seed(327)\n",
        "\n",
        "## Run the test_eg function with different epsilon values to compare their performance.\n",
        "# The results for each epsilon value (estimated CTRs and average clicks) will be printed.\n",
        "env = RLAdEnv()\n",
        "results_001 = ___\n",
        "print(f\"Results for epsilon=0.01: qV = {results_001['q']}, Average Reward (clicks) = {results_001['avg_reward']:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "## Access and print the true optimal action and the true click-through rates.\n",
        "# This is for comparison with the agent's estimates.\n",
        "# Note: In a real-world scenario, the agent would not have access to these true values.\n",
        "print(\"\\n--- True Values (unknown to the agent during learning) ---\")\n",
        "print(f\"True optimal action index: {env.optimal_action()}\")\n",
        "print(f\"True CTRs of the ads: {env.pr_succes}\")"
      ],
      "metadata": {
        "id": "y8R9d7fTCMix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution (do not look too early)\n",
        "\n",
        "# This cell demonstrates the application of the epsilon-greedy multi-armed bandit algorithm\n",
        "# to an advertising scenario. It simulates an agent choosing between 5 ads over 10000\n",
        "# time steps (representing users) and evaluates the performance based on estimated\n",
        "# click-through rates (CTRs) and average reward (clicks).\n",
        "\n",
        "def test_eg(env, epsilon, steps=10000):\n",
        "    \"\"\"\n",
        "    Tests the performance of the epsilon-greedy bandit algorithm in the advertising environment.\n",
        "\n",
        "    Args:\n",
        "        epsilon (float): The probability of selecting a random action (exploration).\n",
        "        steps (int): The total number of time steps (users) for the simulation. Defaults to 10000.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing:\n",
        "              - 'qV': The final estimated Q-values (CTRs) for each ad.\n",
        "              - 'avg_reward': The average reward (clicks) per user over the simulation.\n",
        "    \"\"\"\n",
        "    # Create a new RLAgent instance for the simulation.\n",
        "    # The agent is initialized with 5 actions (ads) and the specified epsilon.\n",
        "    agent = RLAgent(k=5, epsilon=epsilon)\n",
        "\n",
        "    # Initialize the total reward accumulated over the simulation.\n",
        "    total_reward = 0\n",
        "\n",
        "    # Loop through each time step, simulating a new user.\n",
        "    for t in range(steps):\n",
        "        # The agent selects an action (ad) using the epsilon-greedy strategy.\n",
        "        action = agent.select_action_eg()\n",
        "\n",
        "        # The environment provides a reward (click or no click) for the chosen ad.\n",
        "        reward = env.reward(action)\n",
        "\n",
        "        # Add the received reward to the total reward.\n",
        "        total_reward += reward\n",
        "\n",
        "        # The agent updates its internal estimates (Q-values and action counts)\n",
        "        # based on the action taken and the reward received.\n",
        "        agent.update_q(action, reward)\n",
        "\n",
        "    # Calculate the average reward per step (user).\n",
        "    average_reward = total_reward / steps\n",
        "\n",
        "    # Return the final estimated Q-values and the average reward.\n",
        "    return {'q': agent.qV, 'avg_reward': average_reward}\n",
        "\n",
        "\n",
        "# Set seeds for reproducibility of results\n",
        "random.seed(327)\n",
        "np.random.seed(327)\n",
        "\n",
        "## Run the test_eg function with different epsilon values to compare their performance.\n",
        "# The results for each epsilon value (estimated CTRs and average clicks) will be printed.\n",
        "env = RLAdEnv()\n",
        "results_001 = test_eg(env, 0.01)\n",
        "print(f\"Results for epsilon=0.01: qV = {results_001['q']}, Average Reward (clicks) = {results_001['avg_reward']:.4f}\")\n",
        "results_01 = test_eg(env, 0.1)\n",
        "print(f\"Results for epsilon=0.1: qV = {results_01['q']}, Average Reward (clicks) = {results_01['avg_reward']:.4f}\")\n",
        "results_05 = test_eg(env, 0.5)\n",
        "print(f\"Results for epsilon=0.5: qV = {results_05['q']}, Average Reward (clicks) = {results_05['avg_reward']:.4f}\")\n",
        "\n",
        "## Access and print the true optimal action and the true click-through rates.\n",
        "# This is for comparison with the agent's estimates.\n",
        "# Note: In a real-world scenario, the agent would not have access to these true values.\n",
        "print(\"\\n--- True Values (unknown to the agent during learning) ---\")\n",
        "print(f\"True optimal action index: {env.optimal_action()}\")\n",
        "print(f\"True CTRs of the ads: {env.pr_succes}\")"
      ],
      "metadata": {
        "id": "DGyPDRBRaEsL",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q2\n",
        "\n",
        "Make a plot of the empirical CTRs for $\\epsilon = 0.01$ and $0.5$ over 10000 time-steps, i.e. plot $Q_t(a)$ over time. Is it best to have a high or a small exploration rate?"
      ],
      "metadata": {
        "id": "BPs3iSp-eub4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Your solution here\n",
        "\n",
        "# Basically we modify the previous test function a bit\n",
        "def test_eg_with_plot(env, epsilon, steps=10000):\n",
        "    \"\"\"\n",
        "    Tests the performance of the epsilon-greedy bandit algorithm and plots estimated CTRs.\n",
        "\n",
        "    Args:\n",
        "        epsilon (float): The probability of selecting a random action (exploration).\n",
        "        steps (int): The total number of time steps (users) for the simulation.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing:\n",
        "              - 'qV': The final estimated Q-values (CTRs) for each ad.\n",
        "              - 'avg_reward': The average reward (clicks) per user over the simulation.\n",
        "              - 'plt': The plotnine plot of estimated CTRs over time.\n",
        "    \"\"\"\n",
        "    # ___\n"
      ],
      "metadata": {
        "id": "xKooKJ3WCRnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution (don't look too early)\n",
        "\n",
        "# Basically we modify the previous test function a bit\n",
        "def test_eg_with_plot(env, epsilon, steps=10000):\n",
        "    \"\"\"\n",
        "    Tests the performance of the epsilon-greedy bandit algorithm and plots estimated CTRs.\n",
        "\n",
        "    Args:\n",
        "        epsilon (float): The probability of selecting a random action (exploration).\n",
        "        steps (int): The total number of time steps (users) for the simulation.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing:\n",
        "              - 'qV': The final estimated Q-values (CTRs) for each ad.\n",
        "              - 'avg_reward': The average reward (clicks) per user over the simulation.\n",
        "              - 'plt': The plotnine plot of estimated CTRs over time.\n",
        "    \"\"\"\n",
        "    agent = RLAgent(k=env.k, epsilon=epsilon)  # Use env.k for the number of ads\n",
        "\n",
        "    total_reward = 0\n",
        "    # Initialize a list to store Q-values at each time step\n",
        "    q_values_history = []\n",
        "\n",
        "    for t in range(steps):\n",
        "        action = agent.select_action_eg()\n",
        "        reward = env.reward(action)\n",
        "        total_reward += reward\n",
        "        agent.update_q(action, reward)\n",
        "        # Store a copy of the current Q-values\n",
        "        q_values_history.append(agent.qV.copy())\n",
        "\n",
        "    average_reward = total_reward / steps\n",
        "\n",
        "    # Convert the history of Q-values to a pandas DataFrame for plotting\n",
        "    q_values_df = pd.DataFrame(q_values_history, columns=[f'A{i}' for i in range(env.k)])\n",
        "    q_values_df['t'] = range(1, steps + 1)\n",
        "\n",
        "    # Melt the DataFrame to long format for plotting\n",
        "    dat = q_values_df.melt(id_vars='t', var_name='action', value_name='ctr')\n",
        "\n",
        "    # Create the plot using plotnine\n",
        "    pt = (\n",
        "        ggplot(dat, aes(x='t', y='ctr', color='action'))\n",
        "        + geom_line()\n",
        "        + labs(y='Empirical CTRs', x='Time', title=f'CTRs eps = {epsilon} avg = {average_reward}', color='Action')\n",
        "        + theme(legend_position='bottom')\n",
        "    )\n",
        "\n",
        "    return {'q': agent.qV, 'avg_reward': average_reward, 'plt': pt}\n",
        "\n",
        "# Test different epsilon values and showing the plots:\n",
        "env = RLAdEnv()\n",
        "random.seed(327) # Keep the same seed as the R code for reproducibility\n",
        "np.random.seed(327)\n",
        "\n",
        "results_001_plot = test_eg_with_plot(env, 0.01)\n",
        "results_001_plot['plt'].show()\n",
        "\n",
        "results_05_plot = test_eg_with_plot(env, 0.5)\n",
        "results_05_plot['plt'].show()"
      ],
      "metadata": {
        "id": "bc7L8eDLzgTj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q3\n",
        "\n",
        "Assume that the rewards of ad clicks is equal to (10, 8, 5, 15, 2). Modify the algorithm so you look at rewards instead of CTRs. What is the best action to choose? Hint: Have a look at the rewards argument when you define the `RLAgent`class."
      ],
      "metadata": {
        "id": "OXfWFCKR3ryH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Your solution here\n"
      ],
      "metadata": {
        "id": "ruZ8m2iFCSQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution (don't look too early)\n",
        "\n",
        "# Example usage with different epsilon values and showing the plots:\n",
        "random.seed(3527) # Keep the same seed as the R code for reproducibility\n",
        "np.random.seed(3527)\n",
        "\n",
        "# Run the test_eg function with different epsilon values to compare their performance.\n",
        "# The results for each epsilon value (estimated CTRs and average clicks) will be printed.\n",
        "env = RLAdEnv(rewards=(10, 8, 5, 15, 2))\n",
        "results_001_plot = test_eg_with_plot(env, 0.01)\n",
        "results_001_plot['plt'].show()\n",
        "print(f\"Results for epsilon=0.01: Average Reward = {results_001_plot['avg_reward']:.4f}\")\n",
        "\n",
        "results_01_plot = test_eg_with_plot(env, 0.1)\n",
        "results_01_plot['plt'].show()\n",
        "print(f\"Results for epsilon=0.1: Average Reward = {results_01_plot['avg_reward']:.4f}\")\n",
        "\n",
        "results_05_plot = test_eg_with_plot(env, 0.5)\n",
        "results_05_plot['plt'].show()\n",
        "print(f\"Results for epsilon=0.5: Average Reward = {results_05_plot['avg_reward']:.4f}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PLMTL1tM36Cx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q4\n",
        "\n",
        "We now focus on choosing the action using an upper-confidence bound function. Have a look at the function `select_action_ucb` in the `RLAgent` class.\n",
        "\n",
        "Test the UCB algorithm for $c$ values $(0.1, 5, 10, 20)$. Which algorithm seems to find the best average reward (use the rewards from Q3)?\n",
        "\n"
      ],
      "metadata": {
        "id": "AD26eFW6BKU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Your solution here\n",
        "\n",
        "def test_ucb_with_plot(env, steps=10000, c=2):\n",
        "    \"\"\"\n",
        "    Tests the performance of the UCB bandit algorithm and plots estimated CTRs.\n",
        "\n",
        "    Args:\n",
        "        env: RL environment\n",
        "        steps (int): The total number of time steps (users) for the simulation.\n",
        "        c (float): The parameter for UCB.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing:\n",
        "              - 'q': The final estimated Q-values (CTRs) for each ad.\n",
        "              - 'visits': The number of times each ad has been visited.\n",
        "              - 'avg_reward': The average reward (clicks) per user over the simulation.\n",
        "    \"\"\"\n",
        "    # ___"
      ],
      "metadata": {
        "id": "XGdpu_zjCSzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution (don't look too early)\n",
        "\n",
        "def test_ucb_with_plot(env, steps=10000, c=2):\n",
        "    \"\"\"\n",
        "    Tests the performance of the UCB bandit algorithm and plots estimated CTRs.\n",
        "\n",
        "    Args:\n",
        "        env: RL environment\n",
        "        steps (int): The total number of time steps (users) for the simulation.\n",
        "        c (float): The parameter for UCB.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing:\n",
        "              - 'q': The final estimated Q-values (CTRs) for each ad.\n",
        "              - 'visits': The number of times each ad has been visited.\n",
        "              - 'avg_reward': The average reward (clicks) per user over the simulation.\n",
        "    \"\"\"\n",
        "    agent = RLAgent(k=env.k)  # Use env.k for the number of ads\n",
        "\n",
        "    total_reward = 0\n",
        "    # Initialize a list to store Q-values at each time step\n",
        "    q_values_history = []\n",
        "\n",
        "    for t in range(steps):\n",
        "        action = agent.select_action_ucb(c, t)\n",
        "        reward = env.reward(action)\n",
        "        total_reward += reward\n",
        "        agent.update_q(action, reward)\n",
        "        # Store a copy of the current Q-values\n",
        "        q_values_history.append(agent.qV.copy())\n",
        "\n",
        "    average_reward = total_reward / steps\n",
        "\n",
        "    # Convert the history of Q-values to a pandas DataFrame for plotting\n",
        "    q_values_df = pd.DataFrame(q_values_history, columns=[f'A{i}' for i in range(env.k)])\n",
        "    q_values_df['t'] = range(1, steps + 1)\n",
        "\n",
        "    # Melt the DataFrame to long format for plotting\n",
        "    dat = q_values_df.melt(id_vars='t', var_name='action', value_name='ctr')\n",
        "\n",
        "    # Create the plot using plotnine\n",
        "    pt = (\n",
        "        ggplot(dat, aes(x='t', y='ctr', color='action'))\n",
        "        + geom_line()\n",
        "        + labs(y='Empirical reward', x='Time', title=f'Rewards c = {c}', color='Action')\n",
        "        + theme(legend_position='bottom')\n",
        "    )\n",
        "\n",
        "    rew_diff = abs(np.array(env.pr_succes) * np.array(env.rewards) - np.array(agent.qV) )\n",
        "    return {'q': agent.qV, 'visits': agent.nV, 'avg_reward': average_reward, 'plt': pt}\n",
        "\n",
        "# Example usage with different epsilon values and showing the plots:\n",
        "random.seed(3527) # Keep the same seed as the R code for reproducibility\n",
        "np.random.seed(3527)\n",
        "\n",
        "# Run the test_eg function with different epsilon values to compare their performance.\n",
        "# The results for each epsilon value (estimated CTRs and average clicks) will be printed.\n",
        "env = RLAdEnv(rewards=(10, 8, 5, 15, 2))\n",
        "print(\"True expected reward:\", np.array(env.pr_succes) * np.array(env.rewards))\n",
        "\n",
        "c = [0.1,5,10,20]\n",
        "for c_val in c:\n",
        "    results = test_ucb_with_plot(env, c=c_val)\n",
        "    print(f\"Results for c = {c_val}: q = {results['q']}, visits = {results['visits']}, Average total reward = {results['avg_reward']:.4f}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PI7NsHpPCxLd",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q5\n",
        "\n",
        "Assume that you get insider tips from different people:\n",
        "\n",
        "1. The best commercial is either action 1 or 3 of commercial 0 to 4.\n",
        "2. I think the best commercial is 2.\n",
        "3. Always use commercial 3 and you will have good earnings.\n",
        "\n",
        "Assume that you only have 50 trials.\n",
        "Modify your UCB learning algorithm (set $c$ and modify initial $q$ and $n$ values), so learn best."
      ],
      "metadata": {
        "id": "-Ex90SUx52fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Your solution here\n",
        "\n",
        "def test_ucb_with_plot(env, initial_q, initial_n, steps=50, c=5):\n",
        "    \"\"\"\n",
        "    Tests the performance of the epsilon-greedy bandit algorithm and plots estimated CTRs.\n",
        "\n",
        "    Args:\n",
        "        env: RL environment\n",
        "        initial_q: Initial estimated Q-values (CTRs) for each ad.\n",
        "        initial_n: Initial number of times each ad has been visited.\n",
        "        steps (int): The total number of time steps (users) for the simulation.\n",
        "        c (float): The parameter for UCB.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing:\n",
        "              - 'q': The final estimated Q-values (CTRs) for each ad.\n",
        "              - 'visits': The number of times each ad has been visited.\n",
        "              - 'avg_reward': The average reward (clicks) per user over the simulation.\n",
        "              - 'plt': The plotnine plot of estimated CTRs over time.\n",
        "\n",
        "    \"\"\"\n",
        "    # ___"
      ],
      "metadata": {
        "id": "k8GGeQPrw5xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution (don't look too early)\n",
        "\n",
        "def test_ucb_with_plot(env, initial_q, initial_n, steps=50, c=5):\n",
        "    \"\"\"\n",
        "    Tests the performance of the epsilon-greedy bandit algorithm and plots estimated CTRs.\n",
        "\n",
        "    Args:\n",
        "        env: RL environment\n",
        "        initial_q: Initial estimated Q-values (CTRs) for each ad.\n",
        "        initial_n: Initial number of times each ad has been visited.\n",
        "        steps (int): The total number of time steps (users) for the simulation.\n",
        "        c (float): The parameter for UCB.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing:\n",
        "              - 'q': The final estimated Q-values (CTRs) for each ad.\n",
        "              - 'visits': The number of times each ad has been visited.\n",
        "              - 'avg_reward': The average reward (clicks) per user over the simulation.\n",
        "              - 'plt': The plotnine plot of estimated CTRs over time.\n",
        "\n",
        "    \"\"\"\n",
        "    agent = RLAgent(k=env.k)  # Use env.k for the number of ads\n",
        "    agent.qV = initial_q\n",
        "    agent.nV = initial_n\n",
        "    total_reward = 0\n",
        "    # Initialize a list to store Q-values at each time step\n",
        "    q_values_history = []\n",
        "\n",
        "    for t in range(steps):\n",
        "        action = agent.select_action_ucb(c, t)\n",
        "        reward = env.reward(action)\n",
        "        total_reward += reward\n",
        "        agent.update_q(action, reward)\n",
        "        print(\"Choose a =\", action, \"rew =\", reward, \"q =\", agent.qV.tolist())\n",
        "        # Store a copy of the current Q-values\n",
        "        q_values_history.append(agent.qV.copy())\n",
        "\n",
        "    average_reward = total_reward / steps\n",
        "\n",
        "    # Convert the history of Q-values to a pandas DataFrame for plotting\n",
        "    q_values_df = pd.DataFrame(q_values_history, columns=[f'A{i}' for i in range(env.k)])\n",
        "    q_values_df['t'] = range(1, steps + 1)\n",
        "\n",
        "    # Melt the DataFrame to long format for plotting\n",
        "    dat = q_values_df.melt(id_vars='t', var_name='action', value_name='ctr')\n",
        "\n",
        "    # Create the plot using plotnine\n",
        "    pt = (\n",
        "        ggplot(dat, aes(x='t', y='ctr', color='action'))\n",
        "        + geom_line()\n",
        "        + labs(y='Empirical reward', x='Time', title=f'Rewards c = {c}', color='Action')\n",
        "        + theme(legend_position='bottom')\n",
        "    )\n",
        "\n",
        "    return {'q': agent.qV, 'visits': agent.nV, 'avg_reward': average_reward, 'plt': pt}\n",
        "\n",
        "# Example usage with different epsilon values and showing the plots:\n",
        "random.seed(3527) # Keep the same seed as the R code for reproducibility\n",
        "np.random.seed(3527)\n",
        "\n",
        "# Run the test_eg function with different epsilon values to compare their performance.\n",
        "# The results for each epsilon value (estimated CTRs and average clicks) will be printed.\n",
        "env = RLAdEnv(rewards= (10, 8, 5, 15, 2))\n",
        "print(\"True expected reward:\", np.array(env.pr_succes) * np.array(env.rewards))\n",
        "\n",
        "results = test_ucb_with_plot(env, initial_q = np.array([0, 1.0, 1.0, 2.0, 0]), initial_n = np.array([0, 10, 10, 10, 0]), c = 5)\n",
        "results['plt'].show()\n",
        "print(f\"Results for c = 5: q = {results['q']}, visits = {results['visits']}, Average total reward = {results['avg_reward']:.4f}\")\n"
      ],
      "metadata": {
        "id": "loKJlbuqfWnO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise - A coin game\n",
        "\n",
        "Consider a game where you choose to flip one of two (possibly unfair) coins. You win 1 if your chosen coin shows heads and lose 1 if it shows tails. Answer the following questions (you don't need to code anything)."
      ],
      "metadata": {
        "id": "4lFNej6qO9lD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q1\n",
        "\n",
        "Model this as a K-armed bandit problem: define the action set.\n",
        "\n",
        "<details>\n",
        "<summary>Solution (don't look too early)</summary>\n",
        "This is a 2-bandit problem with actions of choosing coin 1 or 2.\n",
        "</details>"
      ],
      "metadata": {
        "id": "gRGiE26m3inM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q2\n",
        "\n",
        "Is the reward a deterministic or stochastic function of your action? What is the expected reward?\n",
        "\n",
        "<details>\n",
        "<summary>Solution (don't look too early)</summary>\n",
        "The reward is stochastic. If consider coin $i$ then $\\mathbb{E}[R_t | a_i] = \\Pr(H)\\cdot 1.$\n",
        "</details>"
      ],
      "metadata": {
        "id": "6O4dp5p83nLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q3\n",
        "\n",
        "You do not know the coin flip probabilities. Instead, you are able to view 6 sample flips for each coin respectively: (T,H,H,T,T,T) and (H,T,H,H,H,T). Use the sample average formula to compute the estimates of the value of each action.\n",
        "\n",
        "<details>\n",
        "<summary>Solution (don't look too early)</summary>\n",
        "The estimates are $$Q_t(a_1) = (-1+1+1-1-1-1)/6 = -1/3$$ and $$Q_t(a_2) = (1-1+1+1+1-1)/6 = 1/3$$.\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "ZmESLqR04Kop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q4\n",
        "\n",
        "Decide on which coin to flip next assuming that you exploit.\n",
        "\n",
        "<details>\n",
        "<summary>Solution (don't look too early)</summary>\n",
        "Coin 2 is chosen since the best action-value.\n",
        "</details>"
      ],
      "metadata": {
        "id": "cA7hQEhN4yM4"
      }
    }
  ]
}