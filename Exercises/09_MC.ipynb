{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simon-mellergaard/RL/blob/main/Exercises/09_MC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlrET7d3WOQ2"
      },
      "source": [
        "# Notebook pre steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0vEJAmyWKPb"
      },
      "outputs": [],
      "source": [
        "#@title Installations\n",
        "\n",
        "# ALWAYS SAVE YOUR OWN COPY OF THIS NOTEBOOK: File > Save a copy in Drive\n",
        "# IF DANISH MENU DO: Hjælp > Engelsk version\n",
        "\n",
        "# To clear output do: Edit > Clear all outputs\n",
        "\n",
        "## Useful shortscuts\n",
        "# Run current cell: Cmd+Enter\n",
        "# Run current cell and goto next: Shift+Enter\n",
        "# Run selection (or line if no selection): Cmd+Shift+Enter\n",
        "\n",
        "# install missing packages\n",
        "!pip install dfply\n",
        "\n",
        "from dfply import *\n",
        "from plotnine import *\n",
        "import numpy as np  # RNG and vector ops\n",
        "import pandas as pd  # tabular outputs\n",
        "import json\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRfJ8WsMofFf"
      },
      "outputs": [],
      "source": [
        "#@title MDP class\n",
        "\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "class MDP:\n",
        "    \"\"\"\n",
        "    A class representing a Markov Decision Process (MDP) using defaultdict structures.\n",
        "\n",
        "    This implementation includes state management, action specification, transition\n",
        "    probabilities, rewards, policies, and iterative algorithms for policy and value iteration.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes an empty MDP with model and state values.\n",
        "        \"\"\"\n",
        "        self.model = defaultdict(lambda: {\"pi\": None, \"actions\": defaultdict(dict)})\n",
        "        self.v = defaultdict(float)\n",
        "\n",
        "    def add_state_space(self, states):\n",
        "        \"\"\"\n",
        "        Adds states to the MDP.\n",
        "\n",
        "        Args:\n",
        "            states (list): List of state identifiers (strings or convertible to strings).\n",
        "        \"\"\"\n",
        "        for state in states:\n",
        "            _ = self.model[str(state)]\n",
        "        self.set_state_value()\n",
        "\n",
        "    def add_action_space(self, state_str, actions):\n",
        "        \"\"\"\n",
        "        Adds actions to a given state. Note you have to update the action\n",
        "        afterwards using `add_action`.\n",
        "\n",
        "        Args:\n",
        "            state_str (str): The state identifier.\n",
        "            actions (list): List of action identifiers.\n",
        "        \"\"\"\n",
        "        if not isinstance(state_str, str):\n",
        "            raise ValueError(\"State is not a sting!\")\n",
        "        if isinstance(actions, str):\n",
        "            # If it's a string, put it in a list to treat it as a single item\n",
        "            actions = [actions]\n",
        "        for action in actions:\n",
        "            # Initialize the action dictionary with 'pr' and 'r' keys\n",
        "            self.model[state_str][\"actions\"][str(action)] = {\"pr\": {}, \"r\": None}\n",
        "\n",
        "    def add_action(self, state_str, action_str, reward, pr):\n",
        "        \"\"\"\n",
        "        Adds a transition action with reward and transition probabilities.\n",
        "\n",
        "        Args:\n",
        "            state_str (str): State from which the action is taken.\n",
        "            action_str (str): Action identifier.\n",
        "            reward (float): Expected reward for taking the action.\n",
        "            pr (dict): Transition probabilities as {next_state: probability}.\n",
        "        \"\"\"\n",
        "        ## remove keys with zero trans pr\n",
        "        keys_to_remove = [key for key, value in pr.items() if value == 0]\n",
        "        for key in keys_to_remove:\n",
        "            del pr[key]\n",
        "        self.model[state_str][\"actions\"][action_str] = {\"r\": reward, \"pr\": pr}\n",
        "\n",
        "    def check(self, delta = 10*np.spacing(np.float64(1))):\n",
        "        \"\"\"\n",
        "        Performs checks on the built MDP model.\n",
        "\n",
        "        Verifies that transition probabilities sum to approximately 1.0 for each\n",
        "        state-action pair and checks for rewards less than the high_neg_reward.\n",
        "        Prints warnings if any issues are found.\n",
        "\n",
        "        Args:\n",
        "            delta (float, optional): Tolerance for the sum of transition probabilities. Defaults to 1e-10.\n",
        "        \"\"\"\n",
        "        ok = True\n",
        "        # Check if transition pr of an action sum to one\n",
        "        for state_label, state_content in self.model.items():\n",
        "            for action_label, action_content in state_content[\"actions\"].items():\n",
        "                pr = action_content[\"pr\"]\n",
        "                pr_sum = np.sum(list(pr.values()))\n",
        "                absdiff = np.abs(1-pr_sum)\n",
        "                if absdiff >= delta:\n",
        "                    print(f\"Warning: Transition probabilities for action '{action_label}' in state '{state_label}' do not sum to 1.0. Diff is: {absdiff}\")\n",
        "                    ok = False\n",
        "\n",
        "        # Check if there are states with no actions\n",
        "        for state_label, state_content in self.model.items():\n",
        "            if len(state_content[\"actions\"]) == 0:\n",
        "                print(f\"Warning: State '{state_label}' has no actions.\")\n",
        "                ok = False\n",
        "\n",
        "        # Check if all action transitions are to a state\n",
        "        states = list(self.model.keys())\n",
        "        for state_label, state_content in self.model.items():\n",
        "            for action_label, action_content in state_content[\"actions\"].items():\n",
        "                if not all(key in self.model for key in action_content['pr'].keys()):\n",
        "                    print(f\"Warning: Action '{action_label}' in state '{state_label}' has a transition to a non-existing state.\")\n",
        "                    ok = False\n",
        "        if ok:\n",
        "            print(\"All checks passed!\")\n",
        "\n",
        "\n",
        "    def normalize(self):\n",
        "        \"\"\"\n",
        "        Normalizes the transition probabilities for each state-action pair.\n",
        "        \"\"\"\n",
        "        for state_label, state_content in self.model.items():\n",
        "            for action_label, action_content in state_content[\"actions\"].items():\n",
        "                pr = action_content[\"pr\"]\n",
        "                pr_sum = np.sum(list(pr.values()))\n",
        "                for next_state_label, prob in pr.items():\n",
        "                    pr[next_state_label] = prob / pr_sum\n",
        "                action_content[\"pr\"] = pr\n",
        "\n",
        "    def set_state_value(self, states=None, value=0):\n",
        "        \"\"\"\n",
        "        Initializes or updates the value of states.\n",
        "\n",
        "        Args:\n",
        "            states (list, optional): List of state identifiers. Defaults to all states.\n",
        "            value (float, optional): Value to assign. Defaults to 0.\n",
        "        \"\"\"\n",
        "        states = states or list(self.model.keys())\n",
        "        for state in states:\n",
        "            self.v[state] = value\n",
        "\n",
        "    def set_random_deterministic_policy(self):\n",
        "        \"\"\"\n",
        "        Sets a random deterministic policy for each state.\n",
        "        \"\"\"\n",
        "        for state in self.model:\n",
        "            actions = list(self.model[state][\"actions\"].keys())\n",
        "            if actions:\n",
        "                chosen_action = random.choice(actions)\n",
        "                self.model[state][\"pi\"] = {chosen_action: 1}\n",
        "\n",
        "    def set_deterministic_policy(self, state_actions):\n",
        "        \"\"\"\n",
        "        Sets a deterministic policy from a state-action mapping.\n",
        "\n",
        "        Args:\n",
        "            state_actions (dict): Mapping {state: action}.\n",
        "        \"\"\"\n",
        "        for state, action in state_actions.items():\n",
        "            self.model[state][\"pi\"] = {action: 1}\n",
        "\n",
        "    def set_policy(self, states, pi):\n",
        "        \"\"\"\n",
        "        Sets a stochastic or deterministic policy for a list of states.\n",
        "\n",
        "        Args:\n",
        "            states (list): List of states to assign the policy.\n",
        "            pi (dict): Policy as {action: probability}.\n",
        "        \"\"\"\n",
        "        for state in states:\n",
        "            self.model[state][\"pi\"] = pi.copy()\n",
        "\n",
        "    def get_state_keys(self):\n",
        "        \"\"\"\n",
        "        Returns the list of state identifiers.\n",
        "\n",
        "        Returns:\n",
        "            list: List of state keys.\n",
        "        \"\"\"\n",
        "        return list(self.model.keys())\n",
        "\n",
        "    def get_action_keys(self, state):\n",
        "        \"\"\"\n",
        "        Returns the action identifiers for a given state.\n",
        "\n",
        "        Args:\n",
        "            state (str): State identifier.\n",
        "\n",
        "        Returns:\n",
        "            list: List of action keys.\n",
        "        \"\"\"\n",
        "        return list(self.model[state][\"actions\"].keys())\n",
        "\n",
        "    def get_action_info(self, state):\n",
        "        \"\"\"\n",
        "        Gets reward and transition probabilities for each action in a state.\n",
        "\n",
        "        Args:\n",
        "            state (str): State identifier.\n",
        "\n",
        "        Returns:\n",
        "            dict: Action information.\n",
        "        \"\"\"\n",
        "        return dict(self.model[state][\"actions\"])\n",
        "\n",
        "    def get_reward(self, state, action):\n",
        "        \"\"\"\n",
        "        Returns the reward for a given state-action pair.\n",
        "\n",
        "        Args:\n",
        "            state (str): State identifier.\n",
        "            action (str): Action identifier.\n",
        "\n",
        "        Returns:\n",
        "            float: Reward value.\n",
        "        \"\"\"\n",
        "        return self.model[state][\"actions\"][action][\"r\"]\n",
        "\n",
        "    def get_mdp_info(self):\n",
        "        \"\"\"\n",
        "        Returns information about the MDP.\n",
        "\n",
        "        Returns:\n",
        "            dict: The info.\n",
        "        \"\"\"\n",
        "        res = {}\n",
        "        res[\"states\"] = len(self.model)\n",
        "        res[\"actions\"] = sum([len(self.model[k][\"actions\"]) for k in self.model.keys()])\n",
        "        return res\n",
        "\n",
        "    def get_policy(self, add_state_values = False):\n",
        "        \"\"\"\n",
        "        Retrieves the current policy.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Table of state, action, and probability.\n",
        "        \"\"\"\n",
        "        policy = []\n",
        "        for state in self.get_state_keys():\n",
        "            for action, prob in self.model[state][\"pi\"].items():\n",
        "                if not add_state_values:\n",
        "                    policy.append({\"state\": state, \"action\": action, \"pr\": prob})\n",
        "                else:\n",
        "                    policy.append({\"state\": state, \"action\": action, \"pr\": prob, \"v\": self.v[state]})\n",
        "        df = pd.DataFrame(policy)\n",
        "        df.set_index(\"state\")\n",
        "        return df\n",
        "\n",
        "    def get_state_values(self, states=None):\n",
        "        \"\"\"\n",
        "        Returns the current value of each state.\n",
        "\n",
        "        Args:\n",
        "            states (list, optional): Subset of states. Defaults to all.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Table of state values.\n",
        "        \"\"\"\n",
        "        states = states or list(self.v.keys())\n",
        "        return pd.DataFrame([{\"state\": s, \"v\": self.v[s]} for s in states])\n",
        "\n",
        "    def get_mdp_matrices(self, high_neg_reward = -100000):\n",
        "        \"\"\"\n",
        "        Returns transition probability and reward matrices.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                p_mat (list): List of transition probability matrices.\n",
        "                r_mat (ndarray): Reward matrix.\n",
        "                states (list): List of state identifiers.\n",
        "                actions (list): List of action identifiers.\n",
        "        \"\"\"\n",
        "        states = self.get_state_keys()\n",
        "        actions = set(\n",
        "            action for state in states for action in self.get_action_keys(state)\n",
        "        )\n",
        "        actions = list(actions)\n",
        "        actions.sort()\n",
        "        p_mat = [pd.DataFrame(0.0, index=states, columns=states) for _ in actions]\n",
        "        for df in p_mat:\n",
        "            np.fill_diagonal(df.values, 1) # set default to transition to same state (so illigal actions work)\n",
        "        r_mat = pd.DataFrame(high_neg_reward, index=states, columns=actions)\n",
        "        for state in states:\n",
        "            for action in self.get_action_keys(state):\n",
        "                p_mat[actions.index(action)].at[state, state] = 0  # reset to 0 again (since action is not illigal)\n",
        "                pr = self.model[state][\"actions\"][action][\"pr\"]\n",
        "                r = self.model[state][\"actions\"][action][\"r\"]\n",
        "                r_mat.at[state, action] = r\n",
        "                for next_state, prob in pr.items():\n",
        "                    p_mat[actions.index(action)].at[state, next_state] = prob\n",
        "        p_mat = [m.to_numpy() for m in p_mat]  # convert to matrices\n",
        "        r_mat = r_mat.to_numpy()\n",
        "        return p_mat, r_mat, states, actions\n",
        "\n",
        "    def save_mdp(self, path: str | Path):\n",
        "        \"\"\"\n",
        "        Saves the MDP to a JSON file.\n",
        "\n",
        "        Args:\n",
        "            path (str | Path): Path to the JSON file.\n",
        "        \"\"\"\n",
        "        path = Path(path)\n",
        "        with path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(self.model, f, indent=2, sort_keys=True, ensure_ascii=False)\n",
        "\n",
        "    def load_mdp(self, path: str | Path):\n",
        "        \"\"\"\n",
        "        Loads the MDP from a JSON file.\n",
        "\n",
        "        Args:\n",
        "            path (str | Path): Path to the JSON file.\n",
        "        \"\"\"\n",
        "        with Path(path).open(\"r\", encoding=\"utf-8\") as f:\n",
        "            self.model = json.load(f)\n",
        "        self.check()\n",
        "\n",
        "    def bellman_calc(self, gamma, state, action):\n",
        "        \"\"\"\n",
        "        Computes Bellman update for a given state-action pair.\n",
        "\n",
        "        Args:\n",
        "            gamma (float): Discount factor.\n",
        "            state (str): State identifier.\n",
        "            action (str): Action identifier.\n",
        "\n",
        "        Returns:\n",
        "            float: Updated value.\n",
        "        \"\"\"\n",
        "        pr = self.model[state][\"actions\"][action][\"pr\"]\n",
        "        reward = self.model[state][\"actions\"][action][\"r\"]\n",
        "        return reward + gamma * sum(pr[s] * self.v[s] for s in pr)\n",
        "\n",
        "    def policy_eval(self, gamma, theta=1e-5, max_iter=10000, reset=True):\n",
        "        \"\"\"\n",
        "        Iteratively evaluates the current policy.\n",
        "\n",
        "        Args:\n",
        "            gamma (float): Discount factor.\n",
        "            theta (float): Convergence threshold.\n",
        "            max_iter (int): Max iterations.\n",
        "            reset (bool): Whether to reset state values to 0.\n",
        "        \"\"\"\n",
        "        if reset:\n",
        "            self.set_state_value()\n",
        "        for _ in range(max_iter):\n",
        "            delta = 0\n",
        "            for state in self.model:\n",
        "                v_old = self.v[state]\n",
        "                pi = self.model[state][\"pi\"]\n",
        "                value = sum(pi[a] * self.bellman_calc(gamma, state, a) for a in pi)\n",
        "                self.v[state] = value\n",
        "                delta = max(delta, abs(v_old - value))\n",
        "            if delta < theta:\n",
        "                break\n",
        "        else:\n",
        "            print(f\"Policy evaluation stopped at max iterations: {max_iter}\")\n",
        "\n",
        "    def policy_iteration(self, gamma, theta=1e-5, max_iter_eval=10000, max_iter_policy=100):\n",
        "        \"\"\"\n",
        "        Performs policy iteration with evaluation and improvement steps.\n",
        "\n",
        "        Args:\n",
        "            gamma (float): Discount factor.\n",
        "            theta (float): Convergence threshold.\n",
        "            max_iter_eval (int): Max iterations during policy evaluation.\n",
        "            max_iter_policy (int): Max policy improvement steps.\n",
        "        \"\"\"\n",
        "        self.set_random_deterministic_policy()\n",
        "        for i in range(max_iter_policy):\n",
        "            self.policy_eval(gamma, theta, max_iter_eval, reset=False)\n",
        "            stable = True\n",
        "            for state in self.model:\n",
        "                old_action = next(iter(self.model[state][\"pi\"]))\n",
        "                best_val, best_action = float(\"-inf\"), None\n",
        "                for action in self.get_action_keys(state):\n",
        "                    val = self.bellman_calc(gamma, state, action)\n",
        "                    if val > best_val:\n",
        "                        best_val = val\n",
        "                        best_action = action\n",
        "                self.model[state][\"pi\"] = {best_action: 1}\n",
        "                if best_action != old_action:\n",
        "                    stable = False\n",
        "            if stable:\n",
        "                break\n",
        "        else:\n",
        "            print(f\"Policy iteration stopped at max iterations: {max_iter_policy}\")\n",
        "        print(f\"Policy iteration finished in {i + 1} iterations.\")\n",
        "\n",
        "    def value_iteration(self, gamma, theta=1e-5, max_iter=10000, reset=True):\n",
        "        \"\"\"\n",
        "        Performs value iteration algorithm.\n",
        "\n",
        "        Args:\n",
        "            gamma (float): Discount factor.\n",
        "            theta (float): Convergence threshold.\n",
        "            max_iter (int): Max number of iterations.\n",
        "            reset (bool): Whether to reinitialize state values.\n",
        "        \"\"\"\n",
        "        self.set_random_deterministic_policy()\n",
        "        if reset:\n",
        "            self.set_state_value()\n",
        "        for i in range(max_iter):\n",
        "            delta = 0\n",
        "            for state in self.model:\n",
        "                v_old = self.v[state]\n",
        "                best_val, best_action = float(\"-inf\"), None\n",
        "                for action in self.get_action_keys(state):\n",
        "                    val = self.bellman_calc(gamma, state, action)\n",
        "                    if val > best_val:\n",
        "                        best_val = val\n",
        "                        best_action = action\n",
        "                self.v[state] = best_val\n",
        "                self.model[state][\"pi\"] = {best_action: 1}\n",
        "                delta = max(delta, abs(v_old - best_val))\n",
        "            if delta < theta:\n",
        "                break\n",
        "        else:\n",
        "            print(f\"Value iteration stopped at max iterations: {max_iter}\")\n",
        "        print(f\"Value iteration finished in {i + 1} iterations.\")\n",
        "\n",
        "    def get_steady_state_pr(self, as_dataframe = True, tol=1e-8):\n",
        "        \"\"\"\n",
        "        Calculates the steady-state probabilities for the MDP under the optimal policy.\n",
        "\n",
        "        Args:\n",
        "            as_dataframe (bool): Whether to return the result as a DataFrame, or otherwise as an array.\n",
        "\n",
        "        Returns:\n",
        "            if as_dataframe:\n",
        "                pd.DataFrame: A DataFrame with states and their steady-state probabilities.\n",
        "            else:\n",
        "                ndarray: An array of steady-state probabilities.\n",
        "        \"\"\"\n",
        "        state_labels_to_index = {label: index for index, label in enumerate(self.get_state_keys())}\n",
        "        num_states = len(state_labels_to_index)\n",
        "        transition_matrix = np.zeros((num_states, num_states))\n",
        "        policy = self.get_policy()\n",
        "        policy['s_idx'] = policy['state'].map(state_labels_to_index)\n",
        "        policy = policy.set_index(['s_idx', 'action'])\n",
        "        # calc transition matrix\n",
        "        for s_label in self.get_state_keys():\n",
        "            s_idx = state_labels_to_index[s_label]\n",
        "            action_rows = policy.loc[s_idx]\n",
        "            for action, row in action_rows.iterrows():\n",
        "                pi = row['pr']\n",
        "                a = self.model[s_label]['actions'][action]\n",
        "                for s_next_label, prob in a['pr'].items():\n",
        "                    s_next_idx = state_labels_to_index[s_next_label]\n",
        "                    transition_matrix[s_idx, s_next_idx] += prob * pi\n",
        "\n",
        "        transition_matrix.sum(axis=1)\n",
        "\n",
        "        ## calc steady state pr\n",
        "        # # alternative 1\n",
        "        # eigenvalues, left_eigenvectors = np.linalg.eig(transition_matrix.T)\n",
        "        # # Find the eigenvalue closest to 1\n",
        "        # closest_eigenvalue_index = np.abs(eigenvalues - 1).argmin()\n",
        "        # # Extract the corresponding left eigenvector\n",
        "        # steady_state_vector = left_eigenvectors[:, closest_eigenvalue_index]\n",
        "        # # Ensure the eigenvector contains real values and take the real part\n",
        "        # steady_state_vector = np.real(steady_state_vector)\n",
        "        # # Normalize the vector to sum to 1\n",
        "        # steady_state_vector = steady_state_vector / np.sum(steady_state_vector)\n",
        "        # # Handle potential negative values due to numerical precision by taking absolute value\n",
        "        # steady_state_vector = np.abs(steady_state_vector)\n",
        "        # steady_state_vector = steady_state_vector / np.sum(steady_state_vector)\n",
        "        # # Verify that the sum of the steady-state probabilities is approximately 1\n",
        "        # print(\"Sum of steady-state probabilities:\", np.sum(steady_state_vector))\n",
        "        # # Verify that all probabilities are non-negative\n",
        "        # print(\"Minimum steady-state probability:\", np.min(steady_state_vector))\n",
        "        # steady = steady_state_vector\n",
        "\n",
        "        # Alternative 2\n",
        "        eigvals, eigvecs = np.linalg.eig(transition_matrix.T)\n",
        "        steady = np.real(eigvecs[:, np.isclose(eigvals, 1)])\n",
        "        steady = steady[:,0]\n",
        "        sum(steady)\n",
        "        steady = steady/steady.sum()\n",
        "\n",
        "        # # Alternative 3 (slow)\n",
        "        # # Solve (P^T - I) d^T = 0 with sum(d)=1 by replacing one equation with the normalization\n",
        "        # A = transition_matrix.T - np.eye(num_states)\n",
        "        # b = np.zeros(num_states)\n",
        "        # A[-1, :] = 1.0\n",
        "        # b[-1] = 1.0\n",
        "        # # Least-squares for robustness\n",
        "        # d, *_ = np.linalg.lstsq(A, b, rcond=None)\n",
        "        # # Clean numerical noise\n",
        "        # d = np.maximum(d, 0)\n",
        "        # d = d / d.sum()\n",
        "\n",
        "        # abs(steady - steady_state_vector) < 0.00000001\n",
        "        # abs(d - steady_state_vector) < 0.00000001\n",
        "        # abs(steady - d) < 0.00000001\n",
        "\n",
        "        if abs(sum(steady) - 1) > tol:\n",
        "            raise ValueError(\"Steady state probabilities do not sum to 1.\")\n",
        "\n",
        "        if as_dataframe:\n",
        "            policy.reset_index(inplace=True)\n",
        "            policy['steady_pr'] = [steady[s_idx] for s_idx in policy['s_idx']]\n",
        "            return policy\n",
        "        else:\n",
        "            return steady\n",
        "\n",
        "# self = mdp\n",
        "# mdp.get_mdp_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTjOLakAkCLV"
      },
      "source": [
        "# Monte Carlo methods for prediction and control\n",
        "\n",
        "This notebook considers Monte Carlo algorithms of RL which learn state and action values by sampling and averaging returns. MC methods estimate the action-values by considering different *sample-paths* (state, action and reward realizations). Compared to a Markov decision process, MC methods are model-free since they do not require full knowledge of the transition probabilities and rewards (a model of the environment); instead, MC methods learn the value function directly from experience.\n",
        "\n",
        "MC methods can be used for processes with episodes, i.e. where there is a terminal state. This reduces the length of the sample-path and the value of the states visited on the path can be updated based on the reward received.\n",
        "\n",
        "Recall that RL considers an agent in an environment:\n",
        "\n",
        "- Agent: The one who takes the action (computer, robot, decision maker).\n",
        "- Environment: The system/world where observations (states) and rewards are found.\n",
        "\n",
        "Data are revealed sequentially as the agent take actions $$(S_0, A_0, R_1, S_1, A_1, R_2, S_2, \\ldots).$$ At time $t$ the agent are in state $S_t$, takes action $A_{t}$ and observed reward $R_{t+1}$. Hence, to run algorithms, we need to code an agent and environment class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pkl_Q-c4Yah"
      },
      "source": [
        "## Seasonal inventory and sales planning\n",
        "\n",
        "We consider the [seasonal inventory and sales planning example](url) and try to implement an algorithm that uses generalised policy iteration with every-visit estimation using epsilon-greedy action selection.\n",
        "\n",
        "First, we define an environment that mimics the problem we consider.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCUt0ZQbzU8E"
      },
      "outputs": [],
      "source": [
        "#@title RL environment - Seasonal example\n",
        "\n",
        "from __future__ import annotations  # forward refs\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class RLEnvSeasonal:\n",
        "    \"\"\"\n",
        "    Seasonal single-item pricing/sales environment.\n",
        "\n",
        "    State space:\n",
        "      - non-terminal states are strings \"q,t\" where q∈{1..max_inv}, t∈{1..max_t}\n",
        "      - terminal state is \"0\"\n",
        "\n",
        "    Action space:\n",
        "      - at t < max_t: choose a price from self.prices (as string)\n",
        "      - at t == max_t: action is effectively a dummy (we scrap remaining inventory)\n",
        "\n",
        "    Stochastic demand depends on price via a piecewise curve and has early-season uplift.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 max_inv: int,\n",
        "                 max_t: int,\n",
        "                 scrap_price: float,\n",
        "                 purchase_price: float,\n",
        "                 prices: list[float]) -> None:\n",
        "        \"\"\"\n",
        "        Initializes the seasonal RL environment.\n",
        "\n",
        "        Args:\n",
        "            max_inv: Maximum inventory units.\n",
        "            max_t: Number of selling weeks.\n",
        "            scrap_price: Value per leftover at final week.\n",
        "            purchase_price: Unit purchase cost at t=1.\n",
        "            prices: Allowable sale prices.\n",
        "        \"\"\"\n",
        "        self.max_inv = int(max_inv)\n",
        "        self.max_t = int(max_t)\n",
        "        self.scrap_price = float(scrap_price)\n",
        "        self.purchase_price = float(purchase_price)\n",
        "        self.prices = list(map(float, prices))\n",
        "\n",
        "    # ----------------------------- state & action spaces ----------------------\n",
        "\n",
        "    def get_states(self) -> list[str]:\n",
        "        \"\"\"\n",
        "        Return all state keys as strings plus terminal '0'.\n",
        "\n",
        "        Returns:\n",
        "            A list of state identifiers.\n",
        "        \"\"\"\n",
        "        # Cartesian product of q=1..max_inv and t=1..max_t\n",
        "        grid = pd.MultiIndex.from_product(\n",
        "            [range(1, self.max_inv + 1), range(1, self.max_t + 1)],\n",
        "            names=[\"q\", \"t\"],\n",
        "        ).to_frame(index=False)\n",
        "        states = [f\"{int(row.q)},{int(row.t)}\" for row in grid.itertuples(index=False)]\n",
        "        return states + [\"0\"]\n",
        "\n",
        "    def get_actions(self, s: str) -> list[str]:\n",
        "        \"\"\"\n",
        "        Return available action keys (as strings) for state s.\n",
        "\n",
        "        Args:\n",
        "            s: The state identifier.\n",
        "\n",
        "        Returns:\n",
        "            A list of action identifiers for the given state.\n",
        "        \"\"\"\n",
        "        if s not in self.get_states():\n",
        "            raise ValueError(f\"Invalid state: {s}\")\n",
        "        if s == \"0\":\n",
        "            return [\"dummy\"]\n",
        "        q_str, t_str = s.split(\",\")\n",
        "        t = int(t_str)\n",
        "        if t == self.max_t:\n",
        "            return [f\"{self.scrap_price}\"]\n",
        "        return [f\"{p}\" for p in self.prices]\n",
        "\n",
        "    # ----------------------------- demand model --------------------------------\n",
        "\n",
        "    def get_demand(self, price: float, t: int) -> int:\n",
        "        \"\"\"\n",
        "        Sample a stochastic demand for a given price and week.\n",
        "\n",
        "        Piecewise base demand (approx of the R code):\n",
        "          - linear between (10,20) and (12,12)\n",
        "          - linear between (12,12) and (15,10)\n",
        "          - log tail beyond 15\n",
        "        Early-season uplift for t <= max_t/2. Ranges mirror runif() factors in R code.\n",
        "\n",
        "        Args:\n",
        "            price: The price.\n",
        "            t: The week.\n",
        "\n",
        "        Returns:\n",
        "            The sampled demand as an integer.\n",
        "        \"\"\"\n",
        "        # segments as in R\n",
        "        # l1: between price 10..12\n",
        "        # l2: between price 12..15\n",
        "        # l3: beyond 15, log decay anchored at (15,10)\n",
        "        if price <= 12.0:\n",
        "            a = (20.0 - 12.0) / (10.0 - 12.0)\n",
        "            b = 20.0 - a * 10.0\n",
        "            d = a * price + b\n",
        "            d_s = d * np.random.uniform(0.75, 1.25)\n",
        "        elif 12.0 <= price <= 15.0:\n",
        "            a = (12.0 - 10.0) / (12.0 - 15.0)\n",
        "            b = 12.0 - a * 12.0\n",
        "            d = a * price + b\n",
        "            d_s = d * np.random.uniform(0.75, 1.25)\n",
        "        else:\n",
        "            d = -4.0 * np.log(price - 15.0 + 1.0) + 10.0\n",
        "            d_s = d * np.random.uniform(1.0, 2.0)\n",
        "        if t <= self.max_t / 2.0:\n",
        "            d_s *= np.random.uniform(1.0, 1.2)\n",
        "        return int(round(max(0.0, d_s)))\n",
        "\n",
        "    # ----------------------------- transition simulation -----------------------\n",
        "\n",
        "    def get_trans_pr_r(self, s: str, a: str, runs: int = 10000) -> dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Monte Carlo transition model from (s, a).\n",
        "\n",
        "        Args:\n",
        "            s: The current state.\n",
        "            a: The action taken.\n",
        "            runs: The number of simulations to run.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary with 'pr' (transition probabilities) and 'r' (expected reward).\n",
        "        \"\"\"\n",
        "        if s == \"0\":\n",
        "            return {\"pr\": {\"0\": 1.0}, \"r\": 0.0}\n",
        "        q_str, t_str = s.split(\",\")\n",
        "        q, t = int(q_str), int(t_str)\n",
        "        if q == 0:\n",
        "            return {\"pr\": {\"0\": 1.0}, \"r\": 0.0}\n",
        "        if t == self.max_t:\n",
        "            return {\"pr\": {\"0\": 1.0}, \"r\": self.scrap_price * q}\n",
        "\n",
        "        price = float(a)\n",
        "        # simulat   e demands\n",
        "        t_arr = np.full(runs, t, dtype=int)\n",
        "        price_arr = np.full(runs, price, dtype=float)\n",
        "        # vectorized sampling by looping in numpy-friendly manner\n",
        "        demands = np.array([self.get_demand(p, tt) for p, tt in zip(price_arr, t_arr)], dtype=int)\n",
        "        sales = np.minimum(q, demands)\n",
        "        q_next = q - sales\n",
        "        rewards = price * sales.astype(float)\n",
        "        if t == 1:\n",
        "            rewards = rewards - q * self.purchase_price\n",
        "        exp_r = float(rewards.mean())\n",
        "        # probabilities of next state\n",
        "        unique, counts = np.unique(q_next, return_counts=True)\n",
        "        pr = {}\n",
        "        for qn, c in zip(unique.tolist(), counts.tolist()):\n",
        "            if qn == 0:\n",
        "                key = \"0\"\n",
        "            else:\n",
        "                key = f\"{qn},{t+1}\"\n",
        "            pr[key] = pr.get(key, 0.0) + c / float(runs)\n",
        "        return {\"pr\": pr, \"r\": exp_r}\n",
        "\n",
        "    # ----------------------------- single-step API for RLAgent -----------------\n",
        "\n",
        "    def get_time_step_data(self, s: str, a: str) -> dict[str, Any]:\n",
        "        \"\"\"\n",
        "        One simulated step: given (s,a) -> {'r': reward, 'sN': next_state or None}.\n",
        "\n",
        "        Args:\n",
        "            s: The current state.\n",
        "            a: The action taken.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary with the reward and the next state (or None if terminal).\n",
        "        \"\"\"\n",
        "        if s == \"0\":\n",
        "            return {\"r\": 0.0, \"sN\": None}\n",
        "        q_str, t_str = s.split(\",\")\n",
        "        q, t = int(q_str), int(t_str)\n",
        "        if q == 0:\n",
        "            return {\"r\": 0.0, \"sN\": None}\n",
        "        if t == self.max_t:\n",
        "            r = self.scrap_price * q\n",
        "            return {\"r\": float(r), \"sN\": None}\n",
        "\n",
        "        price = float(a)\n",
        "        d = self.get_demand(price, t)\n",
        "        sold = min(q, d)\n",
        "        r = price * sold\n",
        "        if t == 1:\n",
        "            r -= q * self.purchase_price\n",
        "        q_n = q - sold\n",
        "        if q_n == 0:\n",
        "            return {\"r\": float(r), \"sN\": None}\n",
        "        return {\"r\": float(r), \"sN\": f\"{q_n},{t+1}\"}\n",
        "\n",
        "    # ----------------------------- episode generators -------------------------\n",
        "\n",
        "    def get_episode_pi(self, agent: Any, start_state: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Roll out an episode under the agent's stored policy π.\n",
        "\n",
        "        Args:\n",
        "            agent: The RL agent.\n",
        "            start_state: The starting state for the episode.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with columns ['s','a','r'] (terminal reward in last row).\n",
        "        \"\"\"\n",
        "        if start_state == \"0\":\n",
        "            return pd.DataFrame(columns=[\"s\", \"a\", \"r\"])\n",
        "        q_str, t_str = start_state.split(\",\")\n",
        "        q, t = int(q_str), int(t_str)\n",
        "        rows = []\n",
        "        cap = 100 * self.max_inv\n",
        "        for _ in range(cap):\n",
        "            if q == 0:\n",
        "                break\n",
        "            s = f\"{q},{t}\"\n",
        "            a = agent.get_action_pi(s)\n",
        "            if t == self.max_t:\n",
        "                r = self.scrap_price * q\n",
        "                q = 0\n",
        "            else:\n",
        "                price = float(a)\n",
        "                d = self.get_demand(price, t)\n",
        "                r = price * min(q, d)\n",
        "                if t == 1:\n",
        "                    r -= q * self.purchase_price\n",
        "                q = q - min(q, d)\n",
        "                t = t + 1\n",
        "            rows.append({\"s\": s, \"a\": str(a), \"r\": float(r)})\n",
        "        return pd.DataFrame(rows, columns=[\"s\", \"a\", \"r\"])\n",
        "\n",
        "    def get_episode(self,\n",
        "                    agent: Any,\n",
        "                    start_state: str,\n",
        "                    coeff: float = 1.0,\n",
        "                    eps: float = 0.1) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Roll out an episode with agent's *selection* logic (UCB on final week, ε-greedy otherwise).\n",
        "\n",
        "        Args:\n",
        "            agent: The RL agent.\n",
        "            start_state: The starting state for the episode.\n",
        "            coeff: Coefficient for UCB action selection.\n",
        "            eps: Epsilon value for epsilon-greedy action selection.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with columns ['s','a','r'].\n",
        "        \"\"\"\n",
        "        if start_state == \"0\":\n",
        "            return pd.DataFrame(columns=[\"s\", \"a\", \"r\"])\n",
        "        q_str, t_str = start_state.split(\",\")\n",
        "        q, t = int(q_str), int(t_str)\n",
        "        rows = []\n",
        "        cap = 100 * self.max_inv\n",
        "        for _ in range(cap):\n",
        "            if q == 0:\n",
        "                break\n",
        "            s = f\"{q},{t}\"\n",
        "            if t == self.max_t:\n",
        "                a = agent.get_action_ucb(s, coeff)\n",
        "                r = self.scrap_price * q\n",
        "                q = 0\n",
        "            else:\n",
        "                # You can use a decaying epsilon if desired; we keep constant\n",
        "                a = agent.get_action_eg(s, eps)\n",
        "                price = float(a)\n",
        "                d = self.get_demand(price, t)\n",
        "                r = price * min(q, d)\n",
        "                if t == 1:\n",
        "                    r -= q * self.purchase_price\n",
        "                q = q - min(q, d)\n",
        "                t = t + 1\n",
        "            rows.append({\"s\": s, \"a\": str(a), \"r\": float(r)})\n",
        "        return pd.DataFrame(rows, columns=[\"s\", \"a\", \"r\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMVp_JSq5d6X"
      },
      "source": [
        "Note that we define methods for getting the state and actions, an episode and the demand. Moreover, for this problem we many also use simulation to get the transition probabilities and the expected reward of a state-action pair (see method `get_trans_pr_r`). That is, we may solve the problem using an MDP first and compare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glMq2lMK6rD8"
      },
      "outputs": [],
      "source": [
        "# set RNG seed for reproducibility\n",
        "np.random.seed(876)\n",
        "\n",
        "# define candidate sale prices\n",
        "prices = [10, 15, 20, 25]\n",
        "\n",
        "# instantiate environment\n",
        "env = RLEnvSeasonal(\n",
        "    max_inv=100,       # maximum inventory\n",
        "    max_t=15,          # selling horizon (weeks)\n",
        "    scrap_price=5.0,   # scrap value at final week\n",
        "    purchase_price=14.0,  # cost of each unit purchased at t=1\n",
        "    prices=prices      # allowable selling prices\n",
        ")\n",
        "\n",
        "# all possible states (including terminal \"0\")\n",
        "states = env.get_states()\n",
        "print(f\"Number of states: {len(states)}\")\n",
        "print(\"First 10 states (q,t):\", states[:10])\n",
        "print(\"Last 5 states:\", states[-5:])  # includes \"0\"\n",
        "\n",
        "# pick an example state\n",
        "s_example = states[0]  # full inventory at week 1\n",
        "print(f\"\\nActions available in {s_example}: {env.get_actions(s_example)}\")\n",
        "\n",
        "# pick another state at final week\n",
        "s_final = states[-2]\n",
        "print(f\"Actions available in {s_final}: {env.get_actions(s_final)}\")\n",
        "\n",
        "# terminal state\n",
        "s_terminal = \"0\"\n",
        "print(f\"Actions available in {s_terminal}: {env.get_actions(s_terminal)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdmbIKFA7k90"
      },
      "source": [
        "### Optimal policy using an MDP\n",
        "\n",
        "First, let us try to find the optimal policy using an MDP. We build the MDP:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rd74y9vj7zdr"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "## Build the MDP\n",
        "mdp = MDP() # initialize\n",
        "mdp.add_state_space(env.get_states())  # add states\n",
        "for s in mdp.get_state_keys():  # add actions\n",
        "   mdp.add_action_space(s, env.get_actions(s))\n",
        "\n",
        "# Add actions with progress bar (this may take approx 15 min)\n",
        "for s in tqdm(mdp.get_state_keys(), desc=f\"States\"):\n",
        "    for a in mdp.get_action_keys(s):\n",
        "        lst = env.get_trans_pr_r(s, a)\n",
        "        mdp.add_action(s, a, lst['r'], lst['pr'])\n",
        "mdp.get_mdp_info()\n",
        "mdp.check()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us try to find the optimal policy:"
      ],
      "metadata": {
        "id": "hKMTKZO0UeKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Solve the MDP\n",
        "mdp.policy_iteration(gamma = 1)\n",
        "\n",
        "dat_mdp = (\n",
        "    mdp.get_policy(add_state_values=True)\n",
        "    >> select(~X.pr)\n",
        "    >> separate(X.state, into = [\"inv\", \"t\"], remove = False, convert = True)\n",
        ")\n",
        "\n",
        "# Plot\n",
        "pt = (\n",
        "    ggplot(dat_mdp >> mask(X.t.notna()), aes(x = \"t\", y = \"inv\", color = \"action\"))\n",
        "    + geom_point()\n",
        "    + theme(legend_position='bottom')\n",
        "    + labs(title = \"Optimal policy\")\n",
        ")\n",
        "pt.show()"
      ],
      "metadata": {
        "id": "jMuASdiLUdmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note in general we change the price based on a diagonal line running from upper-left to lower-right. Some time two prices oscillate given a time due to state-values close to each other.\n",
        "\n",
        "The best inventory level to order can be found by searching among the state-values at time one."
      ],
      "metadata": {
        "id": "rajGMpbrSZ9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = (\n",
        "    dat_mdp\n",
        "    >> mask(X.t == 1)\n",
        "    >> arrange(desc(X.v))\n",
        ")\n",
        "display(res)"
      ],
      "metadata": {
        "id": "xLdvD8v3SbqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is approximately 63 items. If ordering 100 items, we would lose:\n"
      ],
      "metadata": {
        "id": "DSsIhDBOS6pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = res.set_index('inv')\n",
        "round(res.loc[63,'v'] - res.loc[100,'v'], 2)"
      ],
      "metadata": {
        "id": "cQCFpAjsTAgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqfijEcjk2bo"
      },
      "source": [
        "## A generic RL agent\n",
        "\n",
        "Let us now try to use RL and MC to approximate the best policy. First, we define a generic RL agent that can be used for all environments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szh8HtKZl2n6"
      },
      "outputs": [],
      "source": [
        "#@title Generic RL agent\n",
        "\n",
        "import math  # math helpers\n",
        "import random  # tie-breaking choices\n",
        "from collections import defaultdict  # lazy nested dicts\n",
        "from typing import Optional, List, Dict, Any  # typing\n",
        "\n",
        "import numpy as np  # vector ops and RNG\n",
        "import pandas as pd  # tabular data\n",
        "\n",
        "# Optional tidy/plot helpers (only used by the convenience functions below)  # these are optional\n",
        "try:\n",
        "    from dfply import mutate  # tidy-style transform\n",
        "except Exception:  # pragma: no cover\n",
        "    mutate = None  # fallback if dfply isn't installed\n",
        "\n",
        "try:\n",
        "    from plotnine import ggplot, aes, geom_col, geom_tile, labs, theme_minimal  # plotting\n",
        "except Exception:  # pragma: no cover\n",
        "    ggplot = None  # fallback if plotnine isn't installed\n",
        "\n",
        "\n",
        "class RLAgent:\n",
        "    \"\"\"\n",
        "    Tabular RL agent with:\n",
        "      - per-state action dictionaries {'q': value, 'n': visits}\n",
        "      - behavior policy pi (dict action->prob)\n",
        "      - state value v and state visit counter n\n",
        "\n",
        "    Uses defaultdict so states/actions can be created lazily.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        # model[state] = dict with keys:\n",
        "        #   'pi': policy dict(action->prob)\n",
        "        #   'v': state value\n",
        "        #   'n': state visit count\n",
        "        #   'actions': dict(action -> {'q': float, 'n': int})\n",
        "        self.model: Dict[str, Dict[str, Any]] = defaultdict(\n",
        "            lambda: {\n",
        "                \"pi\": None,     # policy probabilities\n",
        "                \"v\": float(\"nan\"),  # state value\n",
        "                \"n\": 0,         # state visits\n",
        "                \"actions\": defaultdict(lambda: {\"q\": 0.0, \"n\": 0}),  # actions\n",
        "            }\n",
        "        )  # core store\n",
        "\n",
        "    # ----------------------------- helpers ------------------------------------\n",
        "\n",
        "    def add_states(self, states: List[str]) -> None:\n",
        "        \"\"\"Force creation of states (defaultdict makes them auto-create).\"\"\"  # eager create\n",
        "        for s in states:\n",
        "            _ = self.model[str(s)]  # touch to ensure creation\n",
        "\n",
        "    def add_state_action(self, s: str, a: str) -> None:\n",
        "        \"\"\"Ensure a state and a specific action exist.\"\"\"  # lazy create\n",
        "        _ = self.model[str(s)][\"actions\"][str(a)]  # touch to ensure creation\n",
        "\n",
        "    def add_actions(self, s: str, actions: List[str]) -> None:\n",
        "        \"\"\"Force creation of actions in state s.\"\"\"  # batch add\n",
        "        for a in actions:\n",
        "            _ = self.model[str(s)][\"actions\"][str(a)]  # touch to ensure creation\n",
        "\n",
        "    def add_states_and_actions(self, df: pd.DataFrame) -> None:\n",
        "        \"\"\"Bulk add (state, action) pairs from DataFrame with columns 's' and 'a'.\"\"\"  # bulk\n",
        "        for s, a in zip(df[\"s\"].astype(str), df[\"a\"].astype(str)):\n",
        "            _ = self.model[s][\"actions\"][a]  # touch-create\n",
        "\n",
        "    # ----------------------------- setters ------------------------------------\n",
        "\n",
        "    def set_action_value(self, value: float = 0.0) -> None:\n",
        "        \"\"\"Set q(s,a) to constant for all actions.\"\"\"  # initializer/reset\n",
        "        for s in self.model:\n",
        "            for a in self.model[s][\"actions\"]:\n",
        "                self.model[s][\"actions\"][a][\"q\"] = float(value)  # assign\n",
        "\n",
        "    def set_state_value(self,\n",
        "                        states: Optional[List[str]] = None,\n",
        "                        value: float = 0.0) -> None:\n",
        "        \"\"\"Set v(s) for given states (all if None).\"\"\"  # V setter\n",
        "        states = states or list(self.model.keys())\n",
        "        for s in states:\n",
        "            self.model[s][\"v\"] = float(value)  # assign\n",
        "\n",
        "    def set_action_ctr_value(self, ctr_value: int = 0) -> None:\n",
        "        \"\"\"Set all action counters to ctr_value.\"\"\"  # reset N(s,a)\n",
        "        for s in self.model:\n",
        "            for a in self.model[s][\"actions\"]:\n",
        "                self.model[s][\"actions\"][a][\"n\"] = int(ctr_value)  # assign\n",
        "\n",
        "    def set_state_ctr_value(self, ctr_value: int = 0) -> None:\n",
        "        \"\"\"Set all state visit counters to ctr_value.\"\"\"  # reset N(s)\n",
        "        for s in self.model:\n",
        "            self.model[s][\"n\"] = int(ctr_value)  # assign\n",
        "\n",
        "    def set_action_value_single(self,\n",
        "                                s: str,\n",
        "                                a: str,\n",
        "                                value: float = 0.0,\n",
        "                                ctr_value: int = 0) -> None:\n",
        "        \"\"\"Set q(s,a) and n(s,a) for a single state-action.\"\"\"  # direct set\n",
        "        _ = self.model[str(s)][\"actions\"][str(a)]  # ensure exists\n",
        "        self.model[s][\"actions\"][a][\"q\"] = float(value)  # set q\n",
        "        self.model[s][\"actions\"][a][\"n\"] = int(ctr_value)  # set n\n",
        "\n",
        "    def set_random_eps_greedy_policy(self, eps: float) -> None:\n",
        "        \"\"\"Set π(s) to random ε-greedy (random greedy action per state).\"\"\"  # init π\n",
        "        for s in self.model:\n",
        "            actions = list(self.model[s][\"actions\"].keys())  # available actions\n",
        "            if not actions:\n",
        "                self.model[s][\"pi\"] = None  # no policy if no actions\n",
        "                continue  # skip\n",
        "            base = eps / len(actions)  # exploration mass\n",
        "            pi = {a: base for a in actions}  # flat\n",
        "            a_star = random.choice(actions)  # random greedy pick\n",
        "            pi[a_star] += 1.0 - eps  # exploitation bump\n",
        "            self.model[s][\"pi\"] = pi  # store\n",
        "\n",
        "    def set_eps_greedy_policy(self, eps: float, states: List[str] | str) -> None:\n",
        "        \"\"\"Make policy epsilon-greedy w.r.t current q-values.\"\"\"  # improve π\n",
        "        states_list = [states] if isinstance(states, str) else list(states)\n",
        "        for s in states_list:\n",
        "            actions = list(self.model[s][\"actions\"].keys())\n",
        "            if not actions:\n",
        "                self.model[s][\"pi\"] = None  # no policy\n",
        "                continue  # skip\n",
        "            q_vals = np.array([self.model[s][\"actions\"][a][\"q\"] for a in actions])  # q-vector\n",
        "            max_mask = q_vals == q_vals.max()  # tie mask\n",
        "            idx = int(np.random.choice(np.flatnonzero(max_mask)))  # random tie\n",
        "            base = eps / len(actions)  # exploration mass\n",
        "            pi = {a: base for a in actions}  # flat init\n",
        "            pi[actions[idx]] += 1.0 - eps  # greedy bump\n",
        "            self.model[s][\"pi\"] = pi  # assign\n",
        "\n",
        "    def set_greedy_policy(self, states: Optional[List[str]] = None) -> None:\n",
        "        \"\"\"Set greedy deterministic policy from q-values.\"\"\"  # greedy π\n",
        "        states = states or list(self.model.keys())\n",
        "        for s in states:\n",
        "            actions = list(self.model[s][\"actions\"].keys())\n",
        "            if not actions:\n",
        "                self.model[s][\"pi\"] = None  # no policy\n",
        "                continue  # skip\n",
        "            q_vals = [self.model[s][\"actions\"][a][\"q\"] for a in actions]  # q list\n",
        "            best = actions[int(np.argmax(q_vals))]  # greedy idx\n",
        "            self.model[s][\"pi\"] = {best: 1.0}  # point mass\n",
        "\n",
        "    def set_policy(self, states: List[str], pi: Dict[str, float]) -> None:\n",
        "        \"\"\"Set π(s) explicitly for each s in states (probabilities need not be normalized).\"\"\"  # explicit π\n",
        "        total = sum(pi.values())  # sum\n",
        "        norm = {a: (p / total) for a, p in pi.items()} if total > 0 else {a: 0.0 for a in pi}  # normalize\n",
        "        for s in states:\n",
        "            self.model[s][\"pi\"] = dict(norm)  # copy in\n",
        "\n",
        "    # ----------------------------- getters ------------------------------------\n",
        "\n",
        "    def get_info(self):\n",
        "        \"\"\"\n",
        "        Returns information about the agent.\n",
        "\n",
        "        Returns:\n",
        "            dict: The info.\n",
        "        \"\"\"\n",
        "        res = {}\n",
        "        res[\"states\"] = len(self.model)\n",
        "        res[\"actions\"] = sum([len(self.model[k][\"actions\"]) for k in self.model.keys()])\n",
        "        return res\n",
        "\n",
        "    def get_state_keys(self) -> List[str]:\n",
        "        return list(self.model.keys())  # all states\n",
        "\n",
        "    def get_action_keys(self, s: str) -> List[str]:\n",
        "        return list(self.model[s][\"actions\"].keys())  # actions in s\n",
        "\n",
        "    def get_action_info(self, s: str) -> Dict[str, Dict[str, Any]]:\n",
        "        \"\"\"Return shallow copy of the actions dict for state s.\"\"\"  # inspection\n",
        "        return dict(self.model[s][\"actions\"])  # shallow copy\n",
        "\n",
        "    def get_state_value_q(self, s: str) -> float:\n",
        "        \"\"\"Compute v_pi(s) = sum_a pi(a|s) q(s,a).\"\"\"  # V from Q & π\n",
        "        pi = self.model[s][\"pi\"] or {}  # distribution\n",
        "        return float(sum(p * self.model[s][\"actions\"][a][\"q\"]\n",
        "                         for a, p in pi.items()))  # dot product\n",
        "\n",
        "    def get_state_values(self,\n",
        "                         states: Optional[List[str]] = None) -> pd.DataFrame:\n",
        "        \"\"\"Return DataFrame of (state, v). Uses dfply::mutate if available.\"\"\"  # tidy\n",
        "        states = states or list(self.model.keys())\n",
        "        df = pd.DataFrame({\"state\": states})  # seed\n",
        "        return pd.DataFrame({\n",
        "            \"state\": states,\n",
        "            \"v\": [self.model[s][\"v\"] for s in states],\n",
        "        })  # basic\n",
        "\n",
        "    def get_policy(self, states: Optional[List[str]] = None) -> pd.DataFrame:\n",
        "        \"\"\"Return DataFrame (state, action, pr) for current π.\"\"\"  # tidy π\n",
        "        states = states or list(self.model.keys())\n",
        "        rows = []  # collect\n",
        "        for s in states:\n",
        "            pi = self.model[s][\"pi\"] or {}  # distribution\n",
        "            for a, p in pi.items():\n",
        "                rows.append({\"state\": s, \"action\": a, \"pr\": float(p)})  # row\n",
        "        return pd.DataFrame(rows)  # frame\n",
        "\n",
        "    def get_state_action_q_mat(self) -> pd.DataFrame:\n",
        "        \"\"\"Return wide Q-matrix DataFrame (rows=states, cols=actions).\"\"\"  # matrix\n",
        "        states = list(self.model.keys())  # rows\n",
        "        actions = sorted({a for s in states for a in self.model[s][\"actions\"].keys()})  # unique cols\n",
        "        mat = pd.DataFrame(np.nan, index=states, columns=actions)  # init\n",
        "        for s in states:\n",
        "            for a, rec in self.model[s][\"actions\"].items():\n",
        "                mat.loc[s, a] = rec[\"q\"]  # fill\n",
        "        return mat  # matrix\n",
        "\n",
        "    def get_action_values(self,\n",
        "                          states: Optional[List[str]] = None) -> pd.DataFrame:\n",
        "        \"\"\"Return long-form DataFrame of q-values and counts.\"\"\"  # tidy Q\n",
        "        states = [states] if isinstance(states, str) else states\n",
        "        states = states or list(self.model.keys())\n",
        "        rows = []\n",
        "        for s in states:\n",
        "            for a, info in self.model[s][\"actions\"].items():\n",
        "                rows.append({\n",
        "                    \"state\": s,\n",
        "                    \"action\": a,\n",
        "                    \"q\": info[\"q\"],\n",
        "                    \"n\": info[\"n\"],\n",
        "                })\n",
        "        return pd.DataFrame(rows)  # frame\n",
        "\n",
        "    # ----------------------------- action selection ---------------------------\n",
        "\n",
        "    def get_action_ucb(self, s: str, coeff: float = 1.0) -> Optional[str]:\n",
        "        \"\"\"UCB1-like selection; updates n(s) and n(s,a).\"\"\"  # UCB\n",
        "        actions = list(self.model[s][\"actions\"].keys())  # available\n",
        "        if not actions:\n",
        "            return None  # no action\n",
        "        self.model[s][\"n\"] += 1  # visit state\n",
        "        qv = np.array([self.model[s][\"actions\"][a][\"q\"] for a in actions], dtype=float)  # q\n",
        "        na = np.array([max(1, self.model[s][\"actions\"][a][\"n\"]) for a in actions], dtype=float)  # counts\n",
        "        ns = float(self.model[s][\"n\"])  # state count\n",
        "        bonus = coeff * np.sqrt(np.log(ns + 1e-4) / na)  # exploration term\n",
        "        idx = int(np.argmax(qv + bonus))  # argmax\n",
        "        a = actions[idx]  # pick\n",
        "        self.model[s][\"actions\"][a][\"n\"] += 1  # visit action\n",
        "        return a  # chosen\n",
        "\n",
        "    def get_action_eg(self, s: str, eps: float) -> str:\n",
        "        \"\"\"Epsilon-greedy action selection (increments counters).\"\"\"  # ε-greedy\n",
        "        self.model[s][\"n\"] += 1  # visit state\n",
        "        actions = list(self.model[s][\"actions\"].keys())  # list\n",
        "        q = np.array([self.model[s][\"actions\"][a][\"q\"] for a in actions])  # q-values\n",
        "        max_mask = q == q.max()  # ties\n",
        "        idx_greedy = int(np.random.choice(np.flatnonzero(max_mask)))  # random tie\n",
        "        probs = np.full(len(actions), eps / len(actions), dtype=float)  # base mass\n",
        "        probs[idx_greedy] += 1.0 - eps  # greedy bump\n",
        "        idx = int(np.random.choice(np.arange(len(actions)), p=probs))  # sample\n",
        "        a = actions[idx]  # chosen action\n",
        "        self.model[s][\"actions\"][a][\"n\"] += 1  # visit action\n",
        "        return a  # return\n",
        "\n",
        "    def get_action_pi(self, s: str) -> Optional[str]:\n",
        "        \"\"\"Sample an action from stored pi(a|s) (increments counters).\"\"\"  # sample π\n",
        "        self.model[s][\"n\"] += 1  # visit state\n",
        "        pi = self.model[s][\"pi\"] or {}  # distribution\n",
        "        if not pi:\n",
        "            return None  # no policy\n",
        "        actions, probs = zip(*pi.items())  # unpack\n",
        "        probs = np.array(probs, dtype=float)  # array\n",
        "        probs /= probs.sum() if probs.sum() > 0 else 1.0  # normalize\n",
        "        a = str(np.random.choice(list(actions), p=probs))  # draw\n",
        "        self.model[s][\"actions\"][a][\"n\"] += 1  # visit action\n",
        "        return a  # chosen\n",
        "\n",
        "    def get_max_action_value(self, s: str) -> float:\n",
        "        \"\"\"Return max_a Q(s,a).\"\"\"  # convenience\n",
        "        q = [rec[\"q\"] for rec in self.model[s][\"actions\"].values()]  # list\n",
        "        return float(max(q)) if q else float(\"nan\")  # handle empty\n",
        "\n",
        "    def get_exp_action_value(self, s: str) -> float:\n",
        "        \"\"\"Return E_{a~π}[Q(s,a)] under current π(s).\"\"\"  # expectation\n",
        "        pi = self.model[s][\"pi\"] or {}  # distribution\n",
        "        return float(sum(p * self.model[s][\"actions\"][a][\"q\"] for a, p in pi.items()))  # dot\n",
        "\n",
        "    # ----------------------------- learning -----------------------------------\n",
        "\n",
        "    def policy_eval_td0(self,\n",
        "                        env: Any,\n",
        "                        gamma: float = 1.0,\n",
        "                        alpha: float = 0.1,\n",
        "                        max_e: int = 1000,\n",
        "                        max_el: int = 10000,\n",
        "                        reset: bool = True,\n",
        "                        states: Optional[List[str]] = None) -> None:\n",
        "        \"\"\"\n",
        "        TD(0) policy evaluation on V(s).\n",
        "        Env must implement: get_time_step_data(s, a) -> {'r': float, 'sN': next_state or None}.  # contract\n",
        "        \"\"\"  # doc\n",
        "        if reset:\n",
        "            self.set_state_value(value=0.0)  # V=0\n",
        "        starts = states or self.get_state_keys()  # candidate starts\n",
        "        for _ in range(max_e):  # episodes\n",
        "            s = random.choice(starts)  # start\n",
        "            for i in range(max_el):  # steps\n",
        "                a = self.get_action_pi(s)  # act under π\n",
        "                if a is None:  # no policy\n",
        "                    break  # abort\n",
        "                dat = env.get_time_step_data(s, a)  # env step\n",
        "                r, sN = dat[\"r\"], dat[\"sN\"]  # unpack\n",
        "                if sN is None:  # terminal\n",
        "                    break  # end\n",
        "                old_v = self.model[s][\"v\"]  # current V\n",
        "                td_target = r + gamma * self.model[sN][\"v\"]  # target\n",
        "                self.model[s][\"v\"] = old_v + alpha * (td_target - old_v)  # update\n",
        "                s = sN  # advance\n",
        "            if i + 1 == max_el:  # cap hit\n",
        "                break  # stop\n",
        "\n",
        "    def policy_eval_mc(self,\n",
        "                       env: Any,\n",
        "                       gamma: float = 1.0,\n",
        "                       theta: float = 0.1,\n",
        "                       min_ite: int = 100,\n",
        "                       max_ite: int = 2000,\n",
        "                       reset: bool = True,\n",
        "                       states: Optional[List[str]] = None,\n",
        "                       verbose: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        Every-visit Monte Carlo evaluation of V(s).\n",
        "        Env must implement: get_episode_pi(agent, s0) -> DataFrame with columns ['s','a','r'].  # contract\n",
        "        \"\"\"  # doc\n",
        "        if reset:\n",
        "            self.set_state_value(value=0.0)  # V=0\n",
        "            self.set_action_ctr_value(0)  # reset N(s,a)\n",
        "            self.set_state_ctr_value(0)  # reset N(s)\n",
        "        starts = states or self.get_state_keys()  # start set\n",
        "        for ite in range(1, max_ite + 1):  # iterations\n",
        "            delta = 0.0  # max change\n",
        "            for s0 in starts:  # episode per start\n",
        "                df = env.get_episode_pi(self, s0)  # generate under π\n",
        "                if df is None or len(df) == 0:\n",
        "                    continue  # skip\n",
        "                if verbose:\n",
        "                    df['g'] = np.nan\n",
        "                    df['n_s'] = np.nan\n",
        "                    df['old_v'] = np.nan\n",
        "                    df['new_v'] = np.nan\n",
        "                g = 0.0  # return accumulator\n",
        "                for i in range(len(df) - 1, -1, -1):  # reverse pass\n",
        "                    s = str(df.iloc[i][\"s\"])  # state\n",
        "                    r = float(df.iloc[i][\"r\"])  # reward\n",
        "                    g = r + gamma * g  # return update\n",
        "                    n_s = max(1, self.model[s][\"n\"])  # denom\n",
        "                    old_v = self.model[s][\"v\"]  # prev\n",
        "                    step = 1.0 / n_s  # 1/N schedule\n",
        "                    self.model[s][\"v\"] = old_v + step * (g - old_v)  # update\n",
        "                    delta = max(delta, abs(old_v - self.model[s][\"v\"]))  # track\n",
        "                    if verbose:\n",
        "                        df.at[i,\"g\"] = g\n",
        "                        df.at[i,\"n_s\"] = n_s\n",
        "                        df.at[i,\"old_v\"] = old_v\n",
        "                        df.at[i,\"new_v\"] = self.model[s][\"v\"]\n",
        "                if verbose:\n",
        "                    print(\"Episode:\")\n",
        "                    print(df)  # trace\n",
        "            if delta < theta and ite >= min_ite:  # convergence\n",
        "                break  # stop\n",
        "        if ite == max_ite:\n",
        "            print(f\"Policy eval algorithm stopped at max iterations allowed: {max_ite}\")  # warn\n",
        "        print(f\"Policy eval algorithm finished in {ite} iterations.\")  # info\n",
        "\n",
        "    def gpi_on_policy_mc(self,\n",
        "                         env: Any,\n",
        "                         gamma: float = 1.0,\n",
        "                         theta: float = 0.1,\n",
        "                         min_ite: int = 100,\n",
        "                         max_ite: int = 1000,\n",
        "                         reset: bool = True,\n",
        "                         states: Optional[List[str]] = None,\n",
        "                         eps: float = 0.1,\n",
        "                         verbose: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        On-policy GPI via Every-Visit MC control on Q(s,a).\n",
        "        Env must implement: get_episode(agent, s0, eps) -> DataFrame ['s','a','r']\n",
        "        and update visit counters.\n",
        "\n",
        "        Args:\n",
        "            env (Any): Environment with get_episode method.\n",
        "            gamma (float, optional): Discount factor. Defaults to 1.0.\n",
        "            theta (float, optional): Convergence threshold. Defaults to 0.1.\n",
        "            min_ite (int, optional): Minimum number of iterations. Defaults to 100.\n",
        "            max_ite (int, optional): Maximum number of iterations. Defaults to 1000.\n",
        "            reset (bool, optional): Reset action-values, state and action counters to 0.\n",
        "            states (list, optional): Starting states. For each iteration, generate\n",
        "                an episiode for each state. If `None uses all states.\n",
        "            eps (float, optional): Epsilon for eps-greedy policy. Defaults to 0.1.\n",
        "            verbose (bool, optional): Print episode info. Defaults to False.\n",
        "        \"\"\"\n",
        "        if reset:\n",
        "            self.set_action_value(0.0)  # Q=0\n",
        "            self.set_action_ctr_value(0)  # reset N(s,a)\n",
        "            self.set_state_ctr_value(0)  # reset N(s)\n",
        "        self.set_eps_greedy_policy(eps, self.get_state_keys())  # init π_ε\n",
        "        starts = states or self.get_state_keys()  # start pool\n",
        "        for ite in range(1, max_ite + 1):  # iterations\n",
        "            delta = 0.0  # track |ΔV|\n",
        "            for s0 in starts:  # episode per start\n",
        "                df = env.get_episode(self, s0, eps)  # behavior inside env\n",
        "                if verbose:\n",
        "                    df['g'] = np.nan\n",
        "                    df['n_sa'] = np.nan\n",
        "                    df['old_q'] = np.nan\n",
        "                    df['step'] = np.nan\n",
        "                    df['new_q'] = np.nan\n",
        "                    df['old_v'] = np.nan\n",
        "                    df['new_v'] = np.nan\n",
        "                if df is None or len(df) == 0:\n",
        "                    continue  # skip\n",
        "                g = 0.0  # return\n",
        "                for i in range(len(df) - 1, -1, -1):  # reverse sweep\n",
        "                    s = str(df.iloc[i][\"s\"])  # state\n",
        "                    a = str(df.iloc[i][\"a\"])  # action\n",
        "                    r = float(df.iloc[i][\"r\"])  # reward\n",
        "                    g = r + gamma * g  # return update\n",
        "                    # step size: (1 / n_sa) ** 0.5 as in R  # schedule\n",
        "                    n_sa = max(1, self.model[s][\"actions\"][a][\"n\"])  # visits\n",
        "                    old_q = self.model[s][\"actions\"][a][\"q\"]  # prev Q\n",
        "                    old_v = self.get_state_value_q(s)  # V before update\n",
        "                    step = (1.0 / n_sa) ** 0.5  # step-size\n",
        "                    new_q = old_q + step * (g - old_q)  # MC update\n",
        "                    self.model[s][\"actions\"][a][\"q\"] = new_q  # MC update\n",
        "                    self.set_eps_greedy_policy(eps, [s])  # improve π(s)\n",
        "                    new_v = self.get_state_value_q(s)  # V after\n",
        "                    delta = max(delta, abs(old_v - new_v))  # track\n",
        "                    if verbose:\n",
        "                        df.at[i,\"g\"] = g\n",
        "                        df.at[i,\"n_sa\"] = n_sa\n",
        "                        df.at[i,\"old_q\"] = old_q\n",
        "                        df.at[i,\"step\"] = step\n",
        "                        df.at[i,\"new_q\"] = new_q\n",
        "                        df.at[i,\"old_v\"] = old_v\n",
        "                        df.at[i,\"new_v\"] = new_v\n",
        "                if verbose:\n",
        "                    print(\"Episode:\")\n",
        "                    print(df)\n",
        "\n",
        "            if delta < theta and ite >= min_ite:\n",
        "                break  # stop\n",
        "        if ite == max_ite:\n",
        "            print(f\"GPI algorithm stopped at max iterations allowed: {max_ite}\")\n",
        "        print(f\"GPI algorithm finished in {ite} iterations.\")  # info\n",
        "\n",
        "    def gpi_on_policy_sarsa(self,\n",
        "                            env: Any,\n",
        "                            gamma: float = 1.0,\n",
        "                            max_e: int = 1000,\n",
        "                            max_el: int = 10000,\n",
        "                            reset: bool = True,\n",
        "                            states: Optional[List[str]] = None,\n",
        "                            eps: float = 0.1,\n",
        "                            alpha: float = 0.1,\n",
        "                            verbose: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        On-policy SARSA with fixed α.\n",
        "        Env must implement: get_time_step_data(s, a) -> {'r': float, 'sN': next_state or None}.  # contract\n",
        "        \"\"\"  # doc\n",
        "        if reset:\n",
        "            self.set_action_value(0.0)  # Q=0\n",
        "            self.set_action_ctr_value(0)  # counts\n",
        "        self.set_eps_greedy_policy(eps, self.get_state_keys())  # init π_ε\n",
        "        starts = states or self.get_state_keys()  # start pool\n",
        "        for _ in range(max_e):  # episodes\n",
        "            s = random.choice(starts)  # pick start\n",
        "            a = self.get_action_pi(s)  # first action under π\n",
        "            for i in range(max_el):  # steps\n",
        "                if a is None:\n",
        "                    break  # no action available\n",
        "                dat = env.get_time_step_data(s, a)  # env step\n",
        "                r, sN = dat[\"r\"], dat[\"sN\"]  # unpack\n",
        "                if sN is None:\n",
        "                    break  # terminal\n",
        "                aN = self.get_action_pi(sN)  # next action\n",
        "                old_q = self.model[s][\"actions\"][a][\"q\"]  # current\n",
        "                target = r + gamma * (self.model[sN][\"actions\"][aN][\"q\"] if aN is not None else 0.0)  # SARSA target\n",
        "                self.model[s][\"actions\"][a][\"q\"] = old_q + alpha * (target - old_q)  # update\n",
        "                if verbose:\n",
        "                    print(f\"(s,a,r,sN,aN)=({s},{a},{r},{sN},{aN}) oldQ={old_q} newQ={self.model[s]['actions'][a]['q']}\")  # trace\n",
        "                self.set_eps_greedy_policy(eps, [s])  # local improve\n",
        "                s, a = sN, aN  # advance\n",
        "            if i + 1 == max_el:\n",
        "                break  # cap hit\n",
        "        print(\"GPI/SARSA finished.\")  # info\n",
        "\n",
        "    def gpi_off_policy_q_learning(self,\n",
        "                                  env: Any,\n",
        "                                  gamma: float = 1.0,\n",
        "                                  max_e: int = 1000,\n",
        "                                  max_el: int = 10000,\n",
        "                                  reset: bool = True,\n",
        "                                  states: Optional[List[str]] = None,\n",
        "                                  eps: float = 0.1,\n",
        "                                  alpha: float = 0.1,\n",
        "                                  verbose: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        Off-policy Q-learning with behavior π_ε and greedy target.  # control\n",
        "        \"\"\"  # doc\n",
        "        if reset:\n",
        "            self.set_action_value(0.0)  # Q=0\n",
        "            self.set_action_ctr_value(0)  # counts\n",
        "        self.set_eps_greedy_policy(eps, self.get_state_keys())  # behavior π_ε\n",
        "        starts = states or self.get_state_keys()  # start pool\n",
        "        for _ in range(max_e):  # episodes\n",
        "            s = random.choice(starts)  # start\n",
        "            for i in range(max_el):  # steps\n",
        "                a = self.get_action_pi(s)  # behavior action\n",
        "                if a is None:\n",
        "                    break  # no action\n",
        "                dat = env.get_time_step_data(s, a)  # env step\n",
        "                r, sN = dat[\"r\"], dat[\"sN\"]  # unpack\n",
        "                if sN is None:\n",
        "                    break  # terminal\n",
        "                q_next = [rec[\"q\"] for rec in self.model[sN][\"actions\"].values()]  # next Qs\n",
        "                max_q = max(q_next) if q_next else 0.0  # greedy target\n",
        "                old_q = self.model[s][\"actions\"][a][\"q\"]  # current\n",
        "                self.model[s][\"actions\"][a][\"q\"] = old_q + alpha * (r + gamma * max_q - old_q)  # update\n",
        "                if verbose:\n",
        "                    print(f\"(s,a,r,sN)=({s},{a},{r},{sN}) oldQ={old_q} maxQ={max_q} newQ={self.model[s]['actions'][a]['q']}\")  # trace\n",
        "                self.set_eps_greedy_policy(eps, [s])  # refresh behavior at s\n",
        "                s = sN  # advance\n",
        "            if i + 1 == max_el:\n",
        "                break  # cap\n",
        "        self.set_greedy_policy()  # finalize with greedy π\n",
        "        print(\"GPI/Q-learning finished.\")  # info\n",
        "\n",
        "    def gpi_on_policy_exp_sarsa(self,\n",
        "                                env: Any,\n",
        "                                gamma: float = 1.0,\n",
        "                                max_e: int = 1000,\n",
        "                                max_el: int = 10000,\n",
        "                                reset: bool = True,\n",
        "                                states: Optional[List[str]] = None,\n",
        "                                eps: float = 0.1,\n",
        "                                alpha: float = 0.1,\n",
        "                                verbose: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        On-policy Expected SARSA with fixed α.  # control\n",
        "        \"\"\"  # doc\n",
        "        if reset:\n",
        "            self.set_action_value(0.0)  # Q=0\n",
        "            self.set_action_ctr_value(0)  # counts\n",
        "        self.set_eps_greedy_policy(eps, self.get_state_keys())  # init π_ε\n",
        "        starts = states or self.get_state_keys()  # start pool\n",
        "        for _ in range(max_e):  # episodes\n",
        "            s = random.choice(starts)  # start\n",
        "            for i in range(max_el):  # steps\n",
        "                a = self.get_action_pi(s)  # act under π\n",
        "                if a is None:\n",
        "                    break  # no action\n",
        "                dat = env.get_time_step_data(s, a)  # env step\n",
        "                r, sN = dat[\"r\"], dat[\"sN\"]  # unpack\n",
        "                if sN is None:\n",
        "                    break  # terminal\n",
        "                exp_q = self.get_exp_action_value(sN)  # expectation under π(sN)\n",
        "                old_q = self.model[s][\"actions\"][a][\"q\"]  # current\n",
        "                self.model[s][\"actions\"][a][\"q\"] = old_q + alpha * (r + gamma * exp_q - old_q)  # update\n",
        "                if verbose:\n",
        "                    print(f\"(s,a,r,sN)=({s},{a},{r},{sN}) oldQ={old_q} expQ={exp_q} newQ={self.model[s]['actions'][a]['q']}\")  # trace\n",
        "                self.set_eps_greedy_policy(eps, [s])  # improve behavior at s\n",
        "                s = sN  # advance\n",
        "            if i + 1 == max_el:\n",
        "                break  # cap\n",
        "        print(\"GPI/Expected-SARSA finished.\")  # info"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pay attention to the `gpi_on_policy_mc` method which uses generalized policy iteration with every-visit estimation using on-policy sampling and an epsilon greedy policy for action selection. Note that `gpi_on_policy_mc` also takes a set of states as input which are the starting states of the episodes generated (all used in each iteration). Moreover, we do not use a step size equal $1/n_a$ but $(1/n_a)^{0.5}$ which decrease slower. Finally, the stopping criteria is added by comparing the differences in the state-values for each state in an episode. Note the value of $\\theta$ gives no guarantee that the action-values will be close to the optimal ones, since we not sample episodes. For instance if two similar episodes are generated early in run then they may be so alike that that the state-values are almost equal and hence the algorithm stops.  \n",
        "\n",
        "We define the RL agent:"
      ],
      "metadata": {
        "id": "nld5zOW9ZVid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = RLAgent()  # create agent\n",
        "agent.add_states(env.get_states())  # add states\n",
        "for s in agent.get_state_keys():  # loop over states\n",
        "    agent.add_actions(s, env.get_actions(s))  # add available actions for each state\n",
        "print(agent.get_info())\n",
        "\n",
        "agent.get_action_keys(\"2,5\")  # inspect action set for state \"2,5\""
      ],
      "metadata": {
        "id": "XeO2SMp6kjyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the current policy, an episode can be generated (ε-greedy):"
      ],
      "metadata": {
        "id": "DElIdO_zlok9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example rollout\n",
        "env.get_episode(agent, \"50,1\", eps=0.2)"
      ],
      "metadata": {
        "id": "TpcUq3Gml0Sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each row contains the state, action, and reward for a time-step.\n",
        "Now, approximate the optimal action-values around state (50, 1) via on-policy MC control:"
      ],
      "metadata": {
        "id": "F_vGc5txmJzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(232)  # reproducibility\n",
        "time = 1  # initial week\n",
        "i = 50  # initial inventory\n",
        "state = f\"{i},{time}\"  # \"50,1\"\n",
        "\n",
        "agent.gpi_on_policy_mc(\n",
        "    env,\n",
        "    min_ite=2000,    # minimum iterations before convergence check\n",
        "    max_ite=50000,   # hard cap on iterations\n",
        "    states=[state],  # focus on start state (like states = state)\n",
        "    reset=True,      # reset Q and counters\n",
        "    eps=0.2,         # ε-greedy behavior\n",
        "    theta=0.2,       # convergence tolerance\n",
        "    verbose=False,   # no per-episode prints\n",
        ")\n",
        "\n",
        "dat_rl = (\n",
        "    agent.get_action_values()\n",
        "    >> separate(X.state, into = [\"inv\", \"t\"], remove = False, convert = True)\n",
        ")\n",
        "\n",
        "# Let us consider the action-values at state $(50,1)$\n",
        "dat_rl >> mask(X.inv == i, X.t == time) >> left_join(dat_mdp)"
      ],
      "metadata": {
        "id": "BQKYkx39mLGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that actions in state $(50,1)$ are sampled differently (based on the epsilon-greedy policy). The best action found (the one with the highest action-value) also seems to be the optimal action w.r.t. the MDP.\n",
        "\n",
        "Let us make a plot of the greedy action for the visited states:"
      ],
      "metadata": {
        "id": "KKQ2jicufdCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visits = dat_rl >> group_by(X.state) >> summarize(n_s = X.n.sum())\n",
        "df = (\n",
        "    dat_rl\n",
        "    >> group_by(X.state)\n",
        "    >> mutate(q_rank = min_rank(X.q, ascending=False))\n",
        "    >> mask(X.q_rank == 1, X.n != 0)\n",
        "    >> left_join(visits)\n",
        ")\n",
        "\n",
        "pt = (\n",
        "    ggplot(df, aes(x = 't', y = 'inv', size = 'n_s', color = 'action')) +\n",
        "        geom_point()\n",
        ")\n",
        "pt.show()"
      ],
      "metadata": {
        "id": "x4ZNpgf_fh5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe that the states are now sampled differently. Some states are visited many times (many episodes visit that state) while others are not visited often. For the states visited often we have a better approximation of best action compared to states which are visited rarely. Here the action is more or less random, e.g. at $t=3$ som actions set the price to 10 (setting the price to 25 would be better since we only have few items left).\n",
        "\n"
      ],
      "metadata": {
        "id": "M6zx-0XApPmn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elsnCKZDH9ep"
      },
      "source": [
        "## Exercises\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Exercise - Seasonal inventory and sales planning\n",
        "\n",
        "Consider the seasonal product in\n",
        "\n"
      ],
      "metadata": {
        "id": "1BzUCPQxstvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q1\n",
        "\n",
        "Consider the plot with the optimal policy for the MDP. Here, the optimal price is high at the lower-left and decreases as we approach the upper-right. Explain why this, from an intuitive point of view, seems correct.\n",
        "\n",
        "<details>\n",
        "   <summary>Solution</summary>\n",
        "   With a low inventory level, there is a high probability of selling everything even for the highest price. This effect is more dominant if the number of time-steps left is high and decrease as we approach week 15. Contrary with a high inventory level there is a low probability of selling everything (if the price is set to high) and therefore we have to use a lower price. This effect is more dominant if the number of time-steps left is low.\n",
        "</details>"
      ],
      "metadata": {
        "id": "0rh9O3_0tgAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q2\n",
        "\n",
        "Consider the plot of the optimal policy for the MDP. For a fixed time $t$, the optimal price does not always decrease as the inventory level increases. Sometimes two prices oscillate (e.g. first green, then red and then green again). Give possible reasons for this happening.\n",
        "\n",
        "<details>\n",
        "   <summary>Solution</summary>\n",
        "   The optimal policy is found using a threshold value, i.e the MDP finds an optimal policy with state-values within the threshold. Hence, small differences in state-value may produce different optimal prices. Another reason could be that we have multiple optimal policies.\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "rj4MafNeuB0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q3\n",
        "\n",
        "3) Let us consider two extreme states $(1,1)$ and $(100,15)$. From an intuitively point of view, what is the optimal action/price to do in these states?\n",
        "\n",
        "<details>\n",
        "   <summary>Solution</summary>\n",
        "   In state $(1,1)$ the optimal price is 25 (we only have one item left to sell over 15 weeks) and in state $(100,15)$ the optimal price is 10 (we can not sell all items in one week, so set a low price.\n",
        "</details>"
      ],
      "metadata": {
        "id": "AePAWNF6t2fs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q4\n",
        "\n",
        "Consider state $(1,1)$ and let us try approximating best epsilon-greedy policy using MC with the verbose option so it prints out info about each episode:"
      ],
      "metadata": {
        "id": "GbIMuMRRujzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(232)  # reproducibility\n",
        "time = 1  # initial week\n",
        "i = 1  # initial inventory\n",
        "state = f\"{i},{time}\"\n",
        "\n",
        "agent.gpi_on_policy_mc(\n",
        "    env,\n",
        "    min_ite=10,      # minimum iterations before convergence check\n",
        "    max_ite=50000,   # hard cap on iterations\n",
        "    states=[state],  # focus on start state (like states = state)\n",
        "    reset=True,      # reset Q and counters\n",
        "    eps=0.2,         # ε-greedy behavior\n",
        "    theta=0.2,       # convergence tolerance\n",
        "    verbose=True,    # no per-episode prints\n",
        ")\n",
        "\n",
        "dat_rl = (\n",
        "    agent.get_action_values()\n",
        "    >> separate(X.state, into = [\"inv\", \"t\"], remove = False, convert = True)\n",
        ")\n",
        "\n",
        "# Let us consider the action-values\n",
        "dat_rl >> mask(X.inv == i, X.t == time) >> left_join(dat_mdp)"
      ],
      "metadata": {
        "id": "IamD6I4swptM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why do we not estimate the optimal action here, and why does the algorithm stop? Hint: You may have a look at the GPI method used in the RLAgent class.\n",
        "\n",
        "<details>\n",
        "   <summary>Solution</summary>\n",
        "   Since the epsilon-greedy policy selects $a=20$ (at random) first, $a=20$ becomes the greedy action that is selected with highest probability. In this run, when selecting the next action, 20 is always selected and hence only $q$ for $a=20$ is updated 10 times (minimum number of iterations). Afterwards differences in state-values of states in the current episode are compared (`old_v` and `v` column). Since they are the same, the algorithm stops. That is, we never get a chance to have a view on action $a=25$ or put differently, the stopping criteria may not work well for episodes with small length. You may try to increase `min_ite` to 1000 and see what happens.\n",
        "</details>"
      ],
      "metadata": {
        "id": "SFWqTdZWw4zk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q5\n",
        "\n",
        "\n",
        "\n",
        "Let us try to set the action-value high initially:\n",
        "\n"
      ],
      "metadata": {
        "id": "6fusAUJDuj5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = RLAgent()  # create agent\n",
        "agent.add_states(env.get_states())  # add states\n",
        "for s in agent.get_state_keys():  # loop over states\n",
        "    agent.add_actions(s, env.get_actions(s))  # add available actions for each state\n",
        "print(agent.get_info())\n",
        "\n",
        "np.random.seed(105)  # reproducibility\n",
        "\n",
        "time = 1  # initial week\n",
        "i = 1  # initial inventory\n",
        "state = f\"{i},{time}\"\n",
        "agent.set_action_value(1000)\n",
        "agent.set_action_ctr_value(0)\n",
        "agent.set_state_ctr_value(0)\n",
        "agent.get_action_values(state)\n",
        "\n",
        "agent.gpi_on_policy_mc(\n",
        "    env,\n",
        "    min_ite=10,      # minimum iterations before convergence check\n",
        "    max_ite=50000,   # hard cap on iterations\n",
        "    states=[state],  # focus on start state (like states = state)\n",
        "    reset=False,      # reset Q and counters\n",
        "    eps=0.2,         # ε-greedy behavior\n",
        "    theta=0.2,       # convergence tolerance\n",
        "    verbose=True,    # no per-episode prints\n",
        ")\n",
        "\n",
        "dat_rl = (\n",
        "    agent.get_action_values()\n",
        "    >> separate(X.state, into = [\"inv\", \"t\"], remove = False, convert = True)\n",
        ")\n",
        "\n",
        "# Let us consider the action-values at the state\n",
        "dat_rl >> mask(X.inv == i, X.t == time) >> left_join(dat_mdp)"
      ],
      "metadata": {
        "id": "h_ir8bHC1O2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happens with the sequence of episodes?\n",
        "<details>\n",
        "   <summary>Solution</summary>\n",
        "   Since the initial action-value is high, all actions will be explored with high probability first. This may help the algorithm get a good start.\n",
        "</details>"
      ],
      "metadata": {
        "id": "02xrNj8OEuL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q6\n",
        "\n",
        "Consider state $(1,1)$ and let us try approximating best epsilon-greedy policy using MC using more episodes:\n"
      ],
      "metadata": {
        "id": "G0Wap7Y0uj8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(749)\n",
        "\n",
        "agent.gpi_on_policy_mc(env, min_ite=100, states=[state], reset=True, eps=0.2)\n",
        "print(f\"State-value at state {state}:\", agent.get_state_value_q(state))\n",
        "agent.gpi_on_policy_mc(env, min_ite=500, states=[state], reset=True, eps=0.2)\n",
        "print(f\"State-value at state {state}:\", agent.get_state_value_q(state))\n",
        "agent.gpi_on_policy_mc(env, min_ite=1000, states=[state], reset=True, eps=0.2)\n",
        "print(f\"State-value at state {state}:\", agent.get_state_value_q(state))"
      ],
      "metadata": {
        "id": "CXVAH3ONF55c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Why is the state-value not increasing monotone as number of iterations increase?\n",
        "\n",
        "\n",
        "<details>\n",
        "   <summary>Solution</summary>\n",
        "   Since we do simulations (we here do 3) each simulation may produce different results Depending on how the algorithm starts. That is, we need more iterations to reach the correct state-value.\n",
        "</details>"
      ],
      "metadata": {
        "id": "3aB0dy-2Hxh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q7\n",
        "\n",
        "Consider state $(25,8)$ and let us approximate the best epsilon-greedy policy:\n",
        "\n"
      ],
      "metadata": {
        "id": "RIgsp8mNuj_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(281)  # reproducibility\n",
        "\n",
        "time = 8  # initial week\n",
        "i = 25  # initial inventory\n",
        "state = f\"{i},{time}\"\n",
        "agent.set_action_value(1000)\n",
        "agent.set_action_ctr_value(0)\n",
        "agent.set_state_ctr_value(0)\n",
        "agent.get_action_values(state)\n",
        "\n",
        "agent.gpi_on_policy_mc(\n",
        "    env,\n",
        "    min_ite=5000,      # minimum iterations before convergence check\n",
        "    max_ite=50000,   # hard cap on iterations\n",
        "    states=[state],  # focus on start state (like states = state)\n",
        "    reset=False,      # reset Q and counters\n",
        "    eps=0.5         # ε-greedy behaviour\n",
        ")\n",
        "\n",
        "dat_rl = (\n",
        "    agent.get_action_values()\n",
        "    >> separate(X.state, into = [\"inv\", \"t\"], remove = False, convert = True)\n",
        ")\n",
        "\n",
        "# Let us consider the action-values at the state\n",
        "dat_rl >> mask(X.inv == i, X.t == time) >> left_join(dat_mdp)"
      ],
      "metadata": {
        "id": "D99RWKSCI1T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen the the optimal state value for the MDP is not equal the action-value for that action. Should that have been the case if run enough simulations?\n",
        "\n",
        "\n",
        "<details>\n",
        "   <summary>Solution</summary>\n",
        "   No, the q-values are the averages over all the generated episodes (also those generated using bad policies). With enough simulations these numbers will converge against the action-values for the best epsilon-greedy policy ($\\epsilon = 0.5$ here), not the optimal greedy policy.\n",
        "</details>"
      ],
      "metadata": {
        "id": "xfs9SIcIImTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q8\n",
        "\n",
        "Consider the very optimistic policy, which sets the price to 25 (where possible):\n",
        "\n"
      ],
      "metadata": {
        "id": "A5SU8V7FIGJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "agent = RLAgent()  # create agent\n",
        "agent.add_states(env.get_states())  # add states\n",
        "for s in agent.get_state_keys():  # loop over states\n",
        "    agent.add_actions(s, env.get_actions(s))  # add available actions for each state\n",
        "print(agent.get_info())\n",
        "\n",
        "# Set policy\n",
        "states = [s for s in agent.get_state_keys() if s != \"0\"]  # all states except terminal \"0\"\n",
        "states = [s for s in states if not re.search(r\",15$\", s)]  # states without \",15\" (all weeks except last)\n",
        "pi = {\"25\": 1.0}  # always set price 25\n",
        "agent.set_policy(states, pi)\n",
        "states = [s for s in states if re.search(r\",15$\", s)]  # states at last time-step (\",15\")\n",
        "pi = {\"5\": 1.0}  # scrap price at last week\n",
        "agent.set_policy(states, pi)\n",
        "pi = {\"dummy\": 1.0}  # terminal state \"0\"\n",
        "agent.set_policy([\"0\"], pi)\n",
        "display(agent.get_policy())"
      ],
      "metadata": {
        "id": "XTtar1tcKTuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We may generate an episode using:\n",
        "\n"
      ],
      "metadata": {
        "id": "DPgO8TIRMF5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.get_episode_pi(agent, \"5,1\")"
      ],
      "metadata": {
        "id": "MrQv_zuaMIsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us try to evaluate this policy in states $(5,1)$ and $(100,10)$:\n",
        "\n"
      ],
      "metadata": {
        "id": "CQhnrEg4IGO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(678)\n",
        "\n",
        "state = \"5,1\"\n",
        "agent.policy_eval_mc(env, states=[state])\n",
        "print(f\"State-value at state {state}:\\n\", agent.get_state_values([state]))\n",
        "print(f\"State-value MDP at state {state}:\\n\", mdp.get_state_values([state]))\n",
        "state = \"100,10\"\n",
        "agent.policy_eval_mc(env, states=[state])\n",
        "print(f\"State-value at state {state}:\\n\", agent.get_state_values([state]))\n",
        "print(f\"State-value MDP at state {state}:\\n\", mdp.get_state_values([state]))"
      ],
      "metadata": {
        "id": "Fzkb7RO5N1Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why do we have state-values close and not close to the optimal values for the MDP?\n",
        "\n",
        "<details>\n",
        "   <summary>Solution</summary>\n",
        "   For state $(5,1)$ it is optimal to set the price high for all states in the sample-path and we estimate the optimal policy. For state $(100,10)$ it is not optimal to set the price high, i.e. the state-value we estimate here is not the optimal one.\n",
        "</details>"
      ],
      "metadata": {
        "id": "-B-gbub8IGSa"
      }
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}