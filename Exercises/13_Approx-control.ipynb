{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simon-mellergaard/RL/blob/main/Exercises/13_Approx-control.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlrET7d3WOQ2"
      },
      "source": [
        "# Notebook pre steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o0vEJAmyWKPb",
        "outputId": "5b10155e-237b-4a32-f894-89a7741503ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/612.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.8/612.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m604.2/612.4 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.4/612.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hPython 3.12.12\n"
          ]
        }
      ],
      "source": [
        "#@title Installations\n",
        "\n",
        "# install missing packages\n",
        "%pip install -q dfply\n",
        "!python --version\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"colab\"\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from dfply import *\n",
        "from plotnine import *\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np  # RNG and vector ops\n",
        "import pandas as pd  # tabular outputs\n",
        "import math\n",
        "import random\n",
        "from IPython.display import display, Markdown\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# On-policy Control with approximation\n",
        "\n",
        "In the previous module, the focus was on predicting the state values of a policy using function approximation. Here, the emphasis is on control, specifically finding an optimal policy through function approximation of *action values* $\t\\hat q(s, a, \\textbf{w})$. Once again, the focus remains on **on-policy** methods.\n",
        "\n",
        "In the episodic case, the extension from prediction to control is straightforward, but in the continuing case, discounting is not suitable to find an optimal policy. Surprisingly, once we have a genuine function approximation, we have to give up discounting and switch to an “average-reward” formulation.\n"
      ],
      "metadata": {
        "id": "bX7eDfGGmcZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Episodic Semi-gradient Control\n",
        "\n",
        "Consider episodic tasks. To find a good policy, the action-value function is approximated by a differentiable function $\\hat q(s,a,\\mathbf{w}) \\approx q^\\pi(s,a)$ with weight vector $\\mathbf{w}$.\n",
        "\n",
        "Training examples now take the form $(S_t, A_t) \\mapsto U_t$, where $U_t$ is any target approximating $q^\\pi(S_t,A_t)$. For instance, for a one-step semi-gradient, we have   \n",
        "$$\n",
        "U_t = R_{t+1} + \\gamma\\, \\hat q(S_{t+1}, A_{t+1}, \\mathbf{w}_t),\n",
        "$$\n",
        "which bootstraps from the next state–action estimate produced by the same $\\varepsilon$-greedy policy.\n",
        "\n",
        "Learning is done using semi-gradient stochastic gradient descent on the squared error, yielding the generic update\n",
        "$$\n",
        "\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha\\big[U_t - \\hat q(S_t,A_t,\\mathbf{w}_t)\\big]\\nabla_{\\mathbf{w}} \\hat q(S_t,A_t,\\mathbf{w}_t).\n",
        "$$\n",
        "This is the direct action-value analogue of semi-gradient TD for state values.\n",
        "\n",
        "If the action set is discrete and not too\n",
        "large, then we can use the techniques already developed in previous chapters.\n"
      ],
      "metadata": {
        "id": "qkMEyCjmp0Qx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us implement the episodic Semi-Gradient Sarsa algorithm (Sutton et al. p244):\n",
        "\n",
        "![alg fig](https://raw.githubusercontent.com/bss-osca/rl/refs/heads/master/book/img/1001_Ep_Semi_Grad_Sarsa.png)"
      ],
      "metadata": {
        "id": "9xb0j853FTLe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w59cZqfB5m39"
      },
      "outputs": [],
      "source": [
        "#@title Episodic semi-gradient SARSA\n",
        "\n",
        "def ep_semi_grad_sarsa(\n",
        "    env,\n",
        "    episodes,\n",
        "    gamma,\n",
        "    eps,\n",
        "    q_hat,\n",
        "    s_start=None,\n",
        "    callback=None,\n",
        "    trace=None,\n",
        "    max_steps=None,\n",
        "    eps_decay=None  # e.g., 0.995 or a callable: lambda ep, eps: ...\n",
        "):\n",
        "    \"\"\"\n",
        "    Episodic Semi-Gradient SARSA.\n",
        "\n",
        "    Params:\n",
        "        env: has get_actions(s), get_step(s, a), reset()\n",
        "        episodes: number of episodes\n",
        "        gamma: discount factor in [0, 1]\n",
        "        eps: initial epsilon for epsilon-greedy\n",
        "        q_hat: approximator with:\n",
        "              eval(s, a) -> float\n",
        "              train(s, a, target) -> None  (performs semi-gradient update internally)\n",
        "        s_start: None -> use env.reset(); else either a single start state or\n",
        "                 a sequence to sample from.\n",
        "        callback(ep, q_hat, trace, info): called at end of each episode (1-based ep)\n",
        "        trace: passed through to callback\n",
        "        max_steps: optional cap on steps per episode\n",
        "        eps_decay: optional epsilon schedule. If float in (0,1), eps *= eps_decay each ep.\n",
        "                   If callable, eps = eps_decay(ep, eps).\n",
        "    \"\"\"\n",
        "\n",
        "    def argmax_rand(values):\n",
        "        \"\"\"Index of a max value, breaking ties uniformly at random.\"\"\"\n",
        "        vmax = np.max(values)\n",
        "        idxs = np.flatnonzero(values == vmax)\n",
        "        return np.random.choice(idxs)\n",
        "\n",
        "    def policy(s, eps_curr):\n",
        "        \"\"\"Epsilon-greedy over env.get_actions(s) using q_hat.\"\"\"\n",
        "        actions = env.get_actions(s)\n",
        "        if len(actions) == 0:\n",
        "            raise ValueError(\"env.get_actions(s) returned no actions.\")\n",
        "        if np.random.rand() < eps_curr:\n",
        "            return np.random.choice(actions)\n",
        "        q_values = np.array([q_hat.eval(s, a) for a in actions], dtype=float)\n",
        "        return actions[argmax_rand(q_values)]\n",
        "\n",
        "    def pick_start(s_start):\n",
        "        if s_start is None:\n",
        "            return env.reset()\n",
        "        if len(s_start) == 1:\n",
        "            return s_start[0]\n",
        "        idx = np.random.choice(range(len(s_start)))\n",
        "        s = s_start[idx]\n",
        "        return s\n",
        "\n",
        "    eps_curr = float(eps)\n",
        "\n",
        "\n",
        "    for ep in tqdm(range(1, episodes + 1), desc=\"Episode\", unit=\"episode\",\n",
        "                   bar_format='{l_bar}{bar}| {n}/{total} [{elapsed}<{remaining}, {rate_fmt}{postfix}]'):\n",
        "        s = pick_start(s_start)\n",
        "        a = policy(s, eps_curr)\n",
        "\n",
        "        steps = 0\n",
        "        while True:\n",
        "            s_n, r, done = env.get_step(s, a)\n",
        "            # print(\"s,a,s_n,r,done:\",s,a,s_n,r,done)\n",
        "            if done:\n",
        "                td_target = r  # Q(s', a') = 0 for terminal\n",
        "                q_hat.train(s, a, td_target)\n",
        "                break\n",
        "            else:\n",
        "                a_n = policy(s_n, eps_curr)\n",
        "                td_target = r + gamma * q_hat.eval(s_n, a_n)\n",
        "                q_hat.train(s, a, td_target)\n",
        "                s, a = s_n, a_n\n",
        "\n",
        "            steps += 1\n",
        "            if max_steps is not None and steps >= max_steps:\n",
        "                # Force end of episode if a cap is set\n",
        "                break\n",
        "\n",
        "        if callback is not None:\n",
        "            info = {\"epsilon\": eps_curr, \"steps\": steps}\n",
        "            callback(ep, q_hat, trace, info)\n",
        "\n",
        "        # Update epsilon if requested\n",
        "        if eps_decay is not None:\n",
        "            if callable(eps_decay):\n",
        "                eps_curr = float(eps_decay(ep, eps_curr))\n",
        "            else:\n",
        "                eps_curr *= float(eps_decay)\n",
        "            eps_curr = max(0.0, min(1.0, eps_curr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlyb00Pd5m39"
      },
      "source": [
        "Note the following differences in the implementation:\n",
        "\n",
        "* $\\hat{q}$ is a black box trainable function approximator class so  avoid passing around $\\mathbf{w}$, computing and storing $\\nabla\\hat{q}(S,A,\\mathbf{w})$ explicitly.\n",
        "* Don't initialize $\\mathbf{w}$ since we may want to continue training.\n",
        "* Possible to choose start states.\n",
        "* May use a decay for the epsilon policy.\n",
        "* Add callback and trace params, so we can track what is going on."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seasonal inventory and sales planning\n",
        "\n",
        "Let us solve the seasonal inventory and sales planning problem in [Example 9.4.4](https://bss-osca.github.io/rl/09_mc.html#sec-mc-seasonal)  \n",
        "\n",
        "We consider seasonal product such as garden furnitures. Assume that the maximum inventory level is $Q$ items, i.e. we can buy at most $Q$ items at the start of the season for a fixed price. The product can be sold for at most $T$ weeks and at the end of the period (week $T$), the remaining inventory is sold to an outlet store for as scrap price.\n",
        "\n",
        "Let $s = (q,t)$ denote the state of the system at the start of a week. We have a terminal state (inventory empty). Actions are which price to choose. Let us limit to actions $\\{10, 15, 20, 25\\}$,\n",
        "\n",
        "The inventory dynamics and rewards are coded in the environment below:"
      ],
      "metadata": {
        "id": "pivYdAQoEvSM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCUt0ZQbzU8E"
      },
      "outputs": [],
      "source": [
        "#@title RL environment - Seasonal\n",
        "\n",
        "from __future__ import annotations  # forward refs\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class RLEnvSeasonal:\n",
        "    \"\"\"\n",
        "    Seasonal single-item pricing/sales environment.\n",
        "\n",
        "    State space:\n",
        "      - non-terminal states are strings \"q,t\" where q∈{1..max_inv}, t∈{1..max_t}\n",
        "      - terminal state is \"0\"\n",
        "\n",
        "    Action space:\n",
        "      - at t < max_t: choose a price from self.prices (as string)\n",
        "      - at t == max_t: action is effectively a dummy (we scrap remaining inventory)\n",
        "\n",
        "    Stochastic demand depends on price via a piecewise curve and has early-season uplift.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 max_inv: int,\n",
        "                 max_t: int,\n",
        "                 scrap_price: float,\n",
        "                 purchase_price: float,\n",
        "                 prices: list[float],\n",
        "                 seed = None) -> None:\n",
        "\n",
        "        \"\"\"\n",
        "        Initializes the seasonal RL environment.\n",
        "\n",
        "        Args:\n",
        "            max_inv: Maximum inventory units.\n",
        "            max_t: Number of selling weeks.\n",
        "            scrap_price: Value per leftover at final week.\n",
        "            purchase_price: Unit purchase cost at t=1.\n",
        "            prices: Allowable sale prices.\n",
        "        \"\"\"\n",
        "        self.max_inv = int(max_inv)\n",
        "        self.max_t = int(max_t)\n",
        "        self.scrap_price = float(scrap_price)\n",
        "        self.purchase_price = float(purchase_price)\n",
        "        self.prices = list(map(float, prices))\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.rng_seed = seed\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the environment to the initial state.\n",
        "\n",
        "        Returns:\n",
        "            The initial state.\n",
        "        \"\"\"\n",
        "        t = 1  # start at time 1\n",
        "        q = self.rng.integers(1, self.max_inv + 1)\n",
        "        return [int(q), t]\n",
        "\n",
        "    def reset_rng(self):\n",
        "        \"\"\"\n",
        "        Reset the random number generator.\n",
        "\n",
        "        Returns:\n",
        "            The initial state.\n",
        "        \"\"\"\n",
        "        self.rng = np.random.default_rng(self.rng_seed)\n",
        "        return self.reset()\n",
        "\n",
        "    # ----------------------------- state & action spaces ----------------------\n",
        "\n",
        "    # def get_states(self) -> list[str]:\n",
        "    #     \"\"\"\n",
        "    #     Return all state keys as strings plus terminal '0'.\n",
        "\n",
        "    #     Returns:\n",
        "    #         A list of state identifiers.\n",
        "    #     \"\"\"\n",
        "    #     # Cartesian product of q=1..max_inv and t=1..max_t\n",
        "    #     grid = pd.MultiIndex.from_product(\n",
        "    #         [range(1, self.max_inv + 1), range(1, self.max_t + 1)],\n",
        "    #         names=[\"q\", \"t\"],\n",
        "    #     ).to_frame(index=False)\n",
        "    #     states = [f\"{int(row.q)},{int(row.t)}\" for row in grid.itertuples(index=False)]\n",
        "    #     return states + [\"0\"]\n",
        "\n",
        "    def get_actions(self, s) -> list:\n",
        "        \"\"\"\n",
        "        Return available actions for state s.\n",
        "\n",
        "        Args:\n",
        "            s: The state.\n",
        "\n",
        "        Returns:\n",
        "            A list of actions for the given state.\n",
        "        \"\"\"\n",
        "        assert len(s) == 2\n",
        "        q, t = s[0], s[1]\n",
        "        assert np.isscalar(t)\n",
        "        assert 1 <= t <= self.max_t\n",
        "        if t == self.max_t:\n",
        "            return [self.scrap_price]\n",
        "        assert np.isscalar(q)\n",
        "        assert 1 <= q <= self.max_inv\n",
        "        return self.prices\n",
        "\n",
        "    # ----------------------------- demand model --------------------------------\n",
        "\n",
        "    def get_demand(self, price: float, t: int) -> int:\n",
        "        \"\"\"\n",
        "        Sample a stochastic demand for a given price and week.\n",
        "\n",
        "        Piecewise base demand:\n",
        "          - linear between (10,20) and (12,12)\n",
        "          - linear between (12,12) and (15,10)\n",
        "          - log tail beyond 15\n",
        "        Early-season uplift for t <= max_t/2.\n",
        "\n",
        "        Args:\n",
        "            price: The price.\n",
        "            t: The week.\n",
        "\n",
        "        Returns:\n",
        "            The sampled demand as an integer.\n",
        "        \"\"\"\n",
        "        # l1: between price 10..12\n",
        "        # l2: between price 12..15\n",
        "        # l3: beyond 15, log decay anchored at (15,10)\n",
        "        if price <= 12.0:\n",
        "            a = (20.0 - 12.0) / (10.0 - 12.0)\n",
        "            b = 20.0 - a * 10.0\n",
        "            d = a * price + b\n",
        "            d_s = d * self.rng.uniform(0.75, 1.25)\n",
        "        elif 12.0 <= price <= 15.0:\n",
        "            a = (12.0 - 10.0) / (12.0 - 15.0)\n",
        "            b = 12.0 - a * 12.0\n",
        "            d = a * price + b\n",
        "            d_s = d * self.rng.uniform(0.75, 1.25)\n",
        "        else:\n",
        "            d = -4.0 * np.log(price - 15.0 + 1.0) + 10.0\n",
        "            d_s = d * self.rng.uniform(1.0, 2.0)\n",
        "        if t <= self.max_t / 2.0:\n",
        "            d_s *= self.rng.uniform(1.0, 1.2)\n",
        "        return int(round(max(0.0, d_s)))\n",
        "\n",
        "    # ----------------------------- single-step API for RLAgent -----------------\n",
        "\n",
        "    def get_step(self, s, a: float) -> list:\n",
        "        \"\"\"\n",
        "        One simulated step: given (s,a) -> (s_n, a_n, terminal)\n",
        "\n",
        "        Args:\n",
        "            s: The current state (list).\n",
        "            a: The action taken (scalar).\n",
        "\n",
        "        Returns:\n",
        "            The next state, reward, and terminal flag.\n",
        "        \"\"\"\n",
        "        assert np.isscalar(a)\n",
        "        assert len(s) == 2\n",
        "        q, t = s[0], s[1]\n",
        "        assert np.isscalar(q)\n",
        "        assert np.isscalar(t)\n",
        "        if q == 0:\n",
        "            return (None, 0.0, True)\n",
        "        if t == self.max_t:\n",
        "            r = self.scrap_price * q\n",
        "            return (None, r, True)\n",
        "        price = float(a)\n",
        "        d = self.get_demand(price, t)\n",
        "        sold = min(q, d)\n",
        "        r = price * sold\n",
        "        if t == 1:\n",
        "            r -= q * self.purchase_price\n",
        "        q_n = q - sold\n",
        "        if q_n == 0:\n",
        "            return (None, r, True)\n",
        "        return ([q_n, t+1], r, False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMVp_JSq5d6X"
      },
      "source": [
        "We define a method `get_step` for getting the next reward and state in an episode. Moreover, a boolean is returned, which is true if we reach the terminal state. Note, here we have no information about transistion probabilities, possible states etc.\n",
        "\n",
        "Let us have a deeper view of the environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glMq2lMK6rD8"
      },
      "outputs": [],
      "source": [
        "# define candidate sale prices\n",
        "prices = [10, 15, 20, 25]\n",
        "\n",
        "\n",
        "## Test 1\n",
        "# instantiate environment\n",
        "env = RLEnvSeasonal(\n",
        "    max_inv=100,       # maximum inventory\n",
        "    max_t=4,          # selling horizon (weeks)\n",
        "    scrap_price=5.0,   # scrap value at final week\n",
        "    purchase_price=14.0,  # cost of each unit purchased at t=1\n",
        "    prices=prices,      # allowable selling prices\n",
        "    seed=876\n",
        ")\n",
        "\n",
        "# Try to pick some state and actions\n",
        "print(\"Start episode:\")\n",
        "s = [100,1]\n",
        "print(\"Start state:\", s)\n",
        "for _ in range(100):\n",
        "    actions = env.get_actions(s)\n",
        "    print(\"Possible actions\", actions)\n",
        "    a = env.rng.choice(actions)\n",
        "    print(\"Pick price:\", a)\n",
        "    s_n, r, done = env.get_step(s, a)\n",
        "    print(\"Next state, reward, done:\", s_n, r, done)\n",
        "    s = s_n\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "## Test 2\n",
        "# instantiate environment\n",
        "env = RLEnvSeasonal(\n",
        "    max_inv=100,       # maximum inventory\n",
        "    max_t=15,          # selling horizon (weeks)\n",
        "    scrap_price=5.0,   # scrap value at final week\n",
        "    purchase_price=14.0,  # cost of each unit purchased at t=1\n",
        "    prices=prices,      # allowable selling prices\n",
        "    seed=876\n",
        ")\n",
        "\n",
        "# Try to pick some state and actions\n",
        "print(\"\\nStart episode:\")\n",
        "s = env.reset()\n",
        "print(\"Start state:\", s)\n",
        "for _ in range(100):\n",
        "    actions = env.get_actions(s)\n",
        "    print(\"Possible actions\", actions)\n",
        "    if 25 in actions:\n",
        "        a = 25\n",
        "    else:\n",
        "        a = env.rng.choice(actions)\n",
        "    print(\"Pick price:\", a)\n",
        "    s_n, r, done = env.get_step(s, a)\n",
        "    print(\"Next state, reward, done:\", s_n, r, done)\n",
        "    s = s_n\n",
        "    if done:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your turn\n",
        "\n",
        "Explain the output.\n",
        "\n",
        "*Your comments*"
      ],
      "metadata": {
        "id": "SFgTkTLNoyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Control using function approximation and tile coding\n",
        "\n",
        "Let us try to solve the control problem using tile coding. First, we need the tile coder class (use as is):"
      ],
      "metadata": {
        "id": "T3gkwIM5pbA0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33fc4d64"
      },
      "source": [
        "#@title TileCoder class (use as is)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from plotnine import (\n",
        "    ggplot, aes, geom_rect, geom_text, geom_vline,\n",
        "    scale_y_reverse, labs, theme_bw, theme\n",
        ")\n",
        "import hashlib\n",
        "from typing import Iterable, Sequence, Tuple, Union\n",
        "\n",
        "\n",
        "# =============================================================\n",
        "# A practical TileCoder for RL (1D and ND)\n",
        "# - Deterministic, evenly spaced offsets by default\n",
        "# - Optional randomness via seed\n",
        "# - Returns sparse active indices; optional dense/hashed encoding\n",
        "# - Supports wrapping (per-dimension or global)\n",
        "# - Exposes 1D attributes so existing plotting utilities work\n",
        "# =============================================================\n",
        "class TileCoder:\n",
        "    \"\"\"\n",
        "    A practical TileCoder for RL (1D and ND).\n",
        "\n",
        "    - Deterministic, evenly spaced offsets by default\n",
        "    - Optional randomness via seed\n",
        "    - Returns sparse active indices\n",
        "    - Supports wrapping (per-dimension or global)\n",
        "    - Exposes 1D attributes so existing plotting utilities work\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_tilings: int,\n",
        "        tiles_per_dim: Union[int, Sequence[int]],\n",
        "        ranges: Union[None, Sequence[Tuple[float, float]]] = None,\n",
        "        wrap: Union[bool, Sequence[bool]] = False,\n",
        "        seed: Union[None, int] = None,\n",
        "        deterministic: bool = True,\n",
        "        hash_size: Union[None, int] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        A practical TileCoder for RL (1D and ND).\n",
        "\n",
        "        Args:\n",
        "            n_tilings (int): Number of tilings.\n",
        "            tiles_per_dim (Union[int, Sequence[int]]): Tiles per dimension.\n",
        "            ranges (Union[None, Sequence[Tuple[float, float]]], optional): Ranges for normalization to [0,1]. Defaults to None\n",
        "            wrap (Union[bool, Sequence[bool]], optional): Wrap flags per dimension. Defaults to False\n",
        "            seed (Union[None, int], optional): Random seed. Defaults to None\n",
        "            deterministic (bool, optional): Use deterministic offsets. Defaults to True\n",
        "            hash_size (Union[None, int], optional): Hash size for sparse encoding. Defaults to None\n",
        "        \"\"\"\n",
        "        assert n_tilings >= 1\n",
        "        self.n_tilings = int(n_tilings)\n",
        "\n",
        "        if isinstance(tiles_per_dim, Iterable) and not isinstance(tiles_per_dim, (str, bytes)):\n",
        "            self.tiles_per_dim = np.array(list(tiles_per_dim), dtype=int)\n",
        "        else:\n",
        "            self.tiles_per_dim = np.array([int(tiles_per_dim)], dtype=int)\n",
        "        assert np.all(self.tiles_per_dim >= 2)\n",
        "\n",
        "        self.d = int(self.tiles_per_dim.size)\n",
        "\n",
        "        # Ranges for normalization to [0,1]\n",
        "        if ranges is None:\n",
        "            self.ranges = np.array([(0.0, 1.0)] * self.d, dtype=float)\n",
        "        else:\n",
        "            assert len(ranges) == self.d\n",
        "            self.ranges = np.array(ranges, dtype=float)\n",
        "            assert np.all(self.ranges[:, 1] > self.ranges[:, 0])\n",
        "\n",
        "        # Wrap flags per dimension\n",
        "        if isinstance(wrap, Iterable) and not isinstance(wrap, (str, bytes)):\n",
        "            wrap = list(wrap)\n",
        "            assert len(wrap) == self.d\n",
        "            self.wrap = np.array(wrap, dtype=bool)\n",
        "        else:\n",
        "            self.wrap = np.array([bool(wrap)] * self.d, dtype=bool)\n",
        "\n",
        "        self.hash_size = None if hash_size is None else int(hash_size)\n",
        "        self.deterministic = bool(deterministic)\n",
        "        self.seed = seed\n",
        "\n",
        "        # Precompute strides for mixed-radix indexing\n",
        "        self.tiles_strides = np.ones(self.d, dtype=int)\n",
        "        for i in range(self.d - 2, -1, -1):\n",
        "            self.tiles_strides[i] = self.tiles_strides[i + 1] * self.tiles_per_dim[i + 1]\n",
        "        self.tiles_per_tiling = int(np.prod(self.tiles_per_dim))  # for 1D this equals tiles_per_dim\n",
        "\n",
        "        # Offsets per tiling and dimension, as fractions of the unit interval\n",
        "        self.offsets = self._make_offsets()\n",
        "\n",
        "        # Expose attributes used by the existing 1D plotting helpers\n",
        "        if self.d == 1:\n",
        "            self.tiles_per_tiling_1d = int(self.tiles_per_dim[0])\n",
        "            self.offsets_1d = self.offsets[:, 0]\n",
        "            # Back-compat attribute names expected by the plotting code\n",
        "            self.tiles_per_tiling = self.tiles_per_tiling_1d\n",
        "            # plot helpers look for 'offsets' as 1D array; keep original 2D in _offsets2d\n",
        "            self._offsets2d = self.offsets\n",
        "            self.offsets = self.offsets_1d\n",
        "\n",
        "    @property\n",
        "    def n_features(self) -> int:\n",
        "        if self.hash_size is not None:\n",
        "            return self.hash_size\n",
        "        return int(self.n_tilings * np.prod(self.tiles_per_dim))\n",
        "\n",
        "    # ------------------------\n",
        "    # Offset generation\n",
        "    # ------------------------\n",
        "    def _make_offsets(self) -> np.ndarray:\n",
        "        \"\"\"Create an (n_tilings, d) array of offsets in [0, 1/tiles_i).\"\"\"\n",
        "        widths = 1.0 / self.tiles_per_dim.astype(float)\n",
        "        if self.deterministic:\n",
        "            # Evenly spaced along each dimension by fraction t/n_tilings of a bin width\n",
        "            t = np.arange(self.n_tilings, dtype=float).reshape(-1, 1)\n",
        "            base = (t / self.n_tilings)  # shape (n_tilings, 1)\n",
        "            return (base * widths)  # broadcast to (n_tilings, d)\n",
        "        else:\n",
        "            rng = np.random.default_rng(self.seed)\n",
        "            return rng.uniform(0.0, widths, size=(self.n_tilings, self.d))\n",
        "\n",
        "    # ------------------------\n",
        "    # Public API\n",
        "    # ------------------------\n",
        "    def active_indices(self, x: Union[float, Sequence[float]]) -> np.ndarray:\n",
        "        \"\"\"Return the global feature indices (length n_tilings) for input x.\n",
        "        x can be a scalar (1D) or a sequence of length d.\n",
        "        \"\"\"\n",
        "        u = self._normalize_to_unit(x)  # in [0,1]^d\n",
        "        inds = np.empty(self.n_tilings, dtype=np.int64)\n",
        "        for t in range(self.n_tilings):\n",
        "            idxs = self._tile_indices_for_tiling(u, t)\n",
        "            if self.hash_size is None:\n",
        "                inds[t] = t * self.tiles_per_tiling + self._linearize_indices(idxs)\n",
        "            else:\n",
        "                inds[t] = self._hash_index(t, idxs)\n",
        "        return inds\n",
        "\n",
        "    def encode_sparse(self, x: Union[float, Sequence[float]]):\n",
        "        \"\"\"Return (indices, values) for sparse features with value 1.0 for each tiling.\"\"\"\n",
        "        inds = self.active_indices(x)\n",
        "        vals = np.ones_like(inds, dtype=float)\n",
        "        return inds, vals\n",
        "\n",
        "    def encode_dense(self, x: Union[float, Sequence[float]]) -> np.ndarray:\n",
        "        \"\"\"Dense binary feature vector (mostly for debugging).\"\"\"\n",
        "        size = self.n_features\n",
        "        vec = np.zeros(size, dtype=float)\n",
        "        inds = self.active_indices(x)\n",
        "        vec[inds] = 1.0\n",
        "        return vec\n",
        "\n",
        "    # ------------------------\n",
        "    # Internals\n",
        "    # ------------------------\n",
        "    def _normalize_to_unit(self, x: Union[float, Sequence[float]]) -> np.ndarray:\n",
        "        x = np.array([x], dtype=float) if np.isscalar(x) else np.array(x, dtype=float)\n",
        "        assert x.size == self.d, f\"Expected input of dimension {self.d}, got {x.size}\"\n",
        "        lo = self.ranges[:, 0]\n",
        "        hi = self.ranges[:, 1]\n",
        "        u = (x - lo) / (hi - lo)\n",
        "        # Clamp to [0,1] to avoid numerical overflow in min/floor\n",
        "        return np.clip(u, 0.0, 1.0)\n",
        "\n",
        "    def _tile_indices_for_tiling(self, u: np.ndarray, t: int) -> np.ndarray:\n",
        "        offs = self.offsets[t]\n",
        "        v = u + offs\n",
        "        idxs = np.empty(self.d, dtype=int)\n",
        "        for i in range(self.d):\n",
        "            if self.wrap[i]:\n",
        "                w = v[i] % 1.0\n",
        "            else:\n",
        "                w = min(v[i], 1.0 - 1e-12)\n",
        "            idxs[i] = int(np.floor(w * self.tiles_per_dim[i]))\n",
        "        return idxs\n",
        "\n",
        "    def _linearize_indices(self, idxs: np.ndarray) -> int:\n",
        "        return int(np.dot(idxs, self.tiles_strides))\n",
        "\n",
        "    def _hash_index(self, t: int, idxs: np.ndarray) -> int:\n",
        "        # Deterministic hash of (t, idxs...)\n",
        "        payload = np.array([t, *idxs.tolist()], dtype=np.int64).tobytes()\n",
        "        h = hashlib.sha256(payload).digest()\n",
        "        return int.from_bytes(h[:8], 'little') % int(self.hash_size)\n",
        "\n",
        "    # ------------------------\n",
        "    # 1D plotting helpers as methods (plotnine)\n",
        "    # ------------------------\n",
        "    def _assert_1d(self):\n",
        "        assert self.d == 1, \"plot_1d is only available for 1D tile coders\"\n",
        "\n",
        "    def _wrap_flag_1d(self) -> bool:\n",
        "        return bool(self.wrap[0])\n",
        "\n",
        "    def _bin_bounds_for_tiling_1d(self, offset: float) -> list:\n",
        "        \"\"\"Compute (start,end) intervals for bins in one tiling.\n",
        "        Honors wrap setting; non-wrap lets the last tile extend to 1.0.\n",
        "        \"\"\"\n",
        "        wrap = self._wrap_flag_1d()\n",
        "        T = int(self.tiles_per_tiling)\n",
        "        width = 1.0 / T\n",
        "        if wrap:\n",
        "            starts = (np.arange(T) * width - offset) % 1.0\n",
        "            ends = (starts + width) % 1.0\n",
        "            return list(zip(starts, ends))\n",
        "        else:\n",
        "            out = []\n",
        "            for b in range(T):\n",
        "                if b < T - 1:\n",
        "                    s = b * width - offset\n",
        "                    e = (b + 1) * width - offset\n",
        "                else:\n",
        "                    s = (T - 1) * width - offset\n",
        "                    e = 1.0\n",
        "                s = max(0.0, s)\n",
        "                e = min(1.0, e)\n",
        "                out.append((s, e))\n",
        "            return out\n",
        "\n",
        "    def _active_tile_for_tiling_1d(self, z: float, offset: float) -> int:\n",
        "        wrap = self._wrap_flag_1d()\n",
        "        T = int(self.tiles_per_tiling)\n",
        "        val = (z + offset) % 1.0 if wrap else min(z + offset, 1.0 - 1e-12)\n",
        "        return int(np.floor(val * T))\n",
        "\n",
        "    def build_tile_df_1d(self, z: float = 0.37) -> pd.DataFrame:\n",
        "        self._assert_1d()\n",
        "        wrap = self._wrap_flag_1d()\n",
        "        recs = []\n",
        "        for t in range(self.n_tilings):\n",
        "            bins = self._bin_bounds_for_tiling_1d(self.offsets[t])\n",
        "            active_tile = self._active_tile_for_tiling_1d(z, self.offsets[t])\n",
        "            for b, (s, e) in enumerate(bins):\n",
        "                is_active = (b == active_tile)\n",
        "                if wrap and s > e:\n",
        "                    recs.append(dict(\n",
        "                        tiling=t, tile=b, xmin=0.0, xmax=e, ymin=t-0.45, ymax=t+0.45,\n",
        "                        xcenter=(0.0 + e) / 2.0, ycenter=t, active=is_active\n",
        "                    ))\n",
        "                    recs.append(dict(\n",
        "                        tiling=t, tile=b, xmin=s, xmax=1.0, ymin=t-0.45, ymax=t+0.45,\n",
        "                        xcenter=(s + 1.0) / 2.0, ycenter=t, active=is_active\n",
        "                    ))\n",
        "                    continue\n",
        "                if e <= s:\n",
        "                    continue\n",
        "                recs.append(dict(\n",
        "                    tiling=t, tile=b, xmin=s, xmax=e, ymin=t-0.45, ymax=t+0.45,\n",
        "                    xcenter=(s + e) / 2.0, ycenter=t, active=is_active\n",
        "                ))\n",
        "        return pd.DataFrame(recs)\n",
        "\n",
        "    def plot_1d(self, z: float = 0.37, title: Union[None, str] = None):\n",
        "        \"\"\"Return a plotnine ggplot object visualizing 1D tiles and active bin per tiling.\"\"\"\n",
        "        self._assert_1d()\n",
        "        df = self.build_tile_df_1d(z)\n",
        "        ttl = title if title else f\"TileCoder (wrap={'True' if self._wrap_flag_1d() else 'False'})\"\n",
        "        p = (\n",
        "            ggplot(df, aes(xmin='xmin', xmax='xmax', ymin='ymin', ymax='ymax', fill='active'))\n",
        "            + geom_rect(alpha=0.35, color='black', size=0.2)\n",
        "            + geom_text(aes(x='xcenter', y='ycenter', label='tile'), size=6)\n",
        "            + geom_vline(xintercept=z, linetype='dashed')\n",
        "            + scale_y_reverse()\n",
        "            + labs(title=f\"{ttl} — z={z:.2f}\", x='z in [0,1]', y='Tiling index')\n",
        "            + theme_bw()\n",
        "            + theme(figure_size=(10, 2))\n",
        "        )\n",
        "        return p\n",
        "\n",
        "    def show_1d(self, z: float = 0.37, title: Union[None, str] = None):\n",
        "        \"\"\"Convenience: build and immediately render the 1D plot (if in an environment that supports .show()).\"\"\"\n",
        "        return self.plot_1d(z, title).show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we are going to define the function approximation, i.e. $\\hat q(s, a)$ that is going to be used as input argument in `ep_semi_grad_sarsa`. Note this class must have methods `eval` and `train`."
      ],
      "metadata": {
        "id": "99QdonTiqDWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Function approx with tiles\n",
        "\n",
        "class FuncApproxTiles:\n",
        "    \"\"\"\n",
        "    Function approximator with tile coding for the seasonal inventory control problem.\n",
        "\n",
        "    Assume tile coder (the same tiles are used) for each discrete action.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, actions, step_size, nb_tilings, nb_tiles, init_val, seed=None, deterministic=False, decay_fct=1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            env: The environment.\n",
        "            actions: A list of possible actions, e.g [15, 20, 25].\n",
        "            step_size: tep size, will be adjusted for nb_tilings automatically\n",
        "            nb_tilings: Number of tilings per action.\n",
        "            nb_tiles: Number of tiles per dimension.\n",
        "            init_val: initial action-values q(s,a).\n",
        "            seed: Seed used by the tile class.\n",
        "            deterministic: If true use determenistic offsets.\n",
        "            decay_fct: Decay factor for step size given a state, i.e. use\n",
        "                decay_factor^num_of_visits * alpha as step size.\n",
        "        \"\"\"\n",
        "        self._n_dim = 2\n",
        "        self.alpha = step_size / nb_tilings\n",
        "        self.decay_fct = decay_fct\n",
        "        self._num_tilings_per_action = nb_tilings\n",
        "        # Ensure scrap only once\n",
        "        base_actions = [a for a in actions if a != env.scrap_price]\n",
        "        actions = [env.scrap_price] + base_actions\n",
        "        self.actions = actions\n",
        "        self.act2idx = {a: i for i, a in enumerate(actions)}\n",
        "        # feasible actions tile coder (use a single one = same tiles for all actions)\n",
        "        # note for scrap action only tiles active for t = max_t will be updated\n",
        "        self.tc = TileCoder(\n",
        "            n_tilings=nb_tilings,\n",
        "            tiles_per_dim=[nb_tiles, nb_tiles],\n",
        "            ranges=[(1.0, float(env.max_inv)), (1.0, float(env.max_t))],\n",
        "            wrap=[False, False],\n",
        "            deterministic=deterministic,\n",
        "            seed=seed\n",
        "        )\n",
        "        self._init_val = float(init_val)\n",
        "        ## feature matrix (a row for each action)\n",
        "        self.w = np.zeros((len(actions), self.tc.n_features), dtype=np.float32) + self._init_val / nb_tilings\n",
        "        self.s_ctr = np.zeros((env.max_inv, env.max_t), dtype=int)  # to store visits to s - 0 indexed\n",
        "\n",
        "    def reset_weights(self, keep_init=True):\n",
        "        if keep_init:\n",
        "            self.w[:] = self._init_val / self._num_tilings_per_action\n",
        "        else:\n",
        "            self.w.fill(0.0)\n",
        "\n",
        "    def reset_s_ctr(self):\n",
        "        self.s_ctr.fill(0)\n",
        "\n",
        "    def reset(self):\n",
        "        self.reset_weights()\n",
        "        self.reset_s_ctr()\n",
        "\n",
        "    def eval(self, state, action):\n",
        "        \"\"\"\n",
        "        Return Q(s, a) for a specific action.\n",
        "\n",
        "        Args:\n",
        "            state: The state.\n",
        "            action: The action.\n",
        "        \"\"\"\n",
        "        assert len(state) == self._n_dim\n",
        "        assert np.isscalar(action)\n",
        "        a = self.act2idx[action]\n",
        "        idx = self.tc.active_indices(state)\n",
        "        return float(self.w[a, idx].sum())\n",
        "\n",
        "    def train(self, state, action, target):\n",
        "        \"\"\"\n",
        "        Update Q(s, a) for a specific action using semi-gradient TD.\n",
        "\n",
        "        Args:\n",
        "            state: The state.\n",
        "            action: The action.\n",
        "            target: The target value.\n",
        "        \"\"\"\n",
        "        assert len(state) == self._n_dim\n",
        "        assert np.isscalar(action)\n",
        "        assert np.isscalar(target)\n",
        "        self.s_ctr[state[0] - 1, state[1] - 1] += 1\n",
        "        a = self.act2idx[action]\n",
        "        idx = self.tc.active_indices(state)\n",
        "        pred = self.w[a, idx].sum()\n",
        "        err = target - pred\n",
        "        alpha = self.alpha * (self.decay_fct ** (float(self.s_ctr[state[0] - 1, state[1] - 1]) - 1))\n",
        "        # alpha = self.alpha\n",
        "        self.w[a, idx] += alpha * err\n",
        "\n",
        "## Testing\n",
        "q_hat = FuncApproxTiles(env, actions=prices, step_size=0.1, nb_tilings=8, nb_tiles=4, init_val=0)\n",
        "print(\"Active tiles for state [100,1]:\", q_hat.tc.active_indices([100,1]))\n",
        "print(\"Active tiles for state [10,15]:\", q_hat.tc.active_indices([10,15]))"
      ],
      "metadata": {
        "id": "GdlSfuCG1yRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Your turn\n",
        "\n",
        "1) How many tilings are there for each action?\n",
        "\n",
        "   *Your comments*\n",
        "\n",
        "2) How many tiles are there in each tiling?\n",
        "\n",
        "   *Your comments*\n",
        "\n",
        "3) Are parameters (weights) estimated with different tilings for each action?\n",
        "\n",
        "   *Your comments*"
      ],
      "metadata": {
        "id": "P1PYJlbcsXbx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to estimate $q(s,a)$."
      ],
      "metadata": {
        "id": "MSdCCgX1sxA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate environment\n",
        "env = RLEnvSeasonal(\n",
        "    max_inv=100,       # maximum inventory\n",
        "    max_t=15,          # selling horizon (weeks)\n",
        "    scrap_price=5.0,   # scrap value at final week\n",
        "    purchase_price=14.0,  # cost of each unit purchased at t=1\n",
        "    prices=prices,     # allowable selling prices\n",
        "    seed = 14233\n",
        ")\n",
        "\n",
        "ep_semi_grad_sarsa(env, episodes=20, gamma=1.0, eps=0.1, q_hat=q_hat)"
      ],
      "metadata": {
        "id": "Pg1CiLW2r206"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, this doesn't give us any information. We need a callback function to store information during the algorithm run.\n",
        "\n",
        "First, we would like to calculate the RMSE so we load the optimal policy for comparison:"
      ],
      "metadata": {
        "id": "5PgnDHU4v3GP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title MDP solution\n",
        "\n",
        "dat_mdp = pd.read_csv('https://drive.google.com/uc?id=1dY6r4xKFwIv3DBEHiOnehbiVEGAhqQax', index_col = False)\n",
        "dat_mdp\n",
        "## store as array for later use when calc RMSE\n",
        "v_mdp_grid = dat_mdp.pivot(index='q', columns='t', values='v').values\n",
        "vmin, vmax = np.nanmin(v_mdp_grid), np.nanmax(v_mdp_grid)  # min and max value\n",
        "vrange = vmax - vmin if vmax > vmin else 1.0  # range if zero set to 1 to avoid division by zero"
      ],
      "metadata": {
        "id": "K6-GnbgDuw6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define the callback function and other helpers:"
      ],
      "metadata": {
        "id": "RhmD31-TvMkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def callback(ep, func_approx, trace, info, eval_every=50):\n",
        "    \"\"\"\n",
        "    Callback for episodic RL that computes RMSE and normalized RMSE vs. MDP optimal values.\n",
        "    Requires global: env, v_mdp_grid, vrange\n",
        "    \"\"\"\n",
        "    if ep == 1 or ep % eval_every == 0:\n",
        "        # Evaluate learned Q(s,a) → V(s) = max_a Q(s,a)\n",
        "        q_grid, greedy_actions = eval_state_action_space(func_approx, env)\n",
        "        # rmse's\n",
        "        diff = (q_grid - v_mdp_grid)\n",
        "        diff2 = diff ** 2\n",
        "        rmse = np.sqrt(np.mean(diff2))\n",
        "        nrmse = rmse / vrange\n",
        "        w = func_approx.s_ctr.astype(float)\n",
        "        wsum = w.sum()\n",
        "        wrmse = np.sqrt((w * diff2).sum() / wsum)\n",
        "\n",
        "        val = {\n",
        "            'ep': ep,\n",
        "            'q': q_grid,\n",
        "            'a': greedy_actions,\n",
        "            'vt1': q_grid[:, 0],\n",
        "            'rmse': float(rmse),\n",
        "            'nrmse': float(nrmse),\n",
        "            'wrmse': float(wrmse),\n",
        "            'epsilon': info.get('epsilon'),\n",
        "            'steps': info.get('steps'),\n",
        "        }\n",
        "        trace.append(val)\n",
        "\n",
        "\n",
        "def eval_state_action_space(q_hat, env):\n",
        "    \"\"\"\n",
        "    Evaluate Q(s,a) over all non-terminal states s=[q,t] with q=1..max_inv, t=1..max_t\n",
        "    and all feasible actions from env.get_actions(s).\n",
        "\n",
        "    Args:\n",
        "        q_hat: Function approximator with method: q_hat.eval(state, action) -> float\n",
        "        env: Environment with members env.max_inv, env.max_t, env.get_actions(state)\n",
        "\n",
        "    Returns:\n",
        "        q_grid: np.ndarray shape (max_inv, max_t) with max_a Q(s,a)\n",
        "        greedy: pd.DataFrame with columns ['q','t','a'] for argmax_a Q(s,a)\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for q in range(1, env.max_inv + 1):\n",
        "        for t in range(1, env.max_t + 1):\n",
        "            s = [q, t]\n",
        "            for a in env.get_actions(s):             # includes [scrap_price] at t==max_t\n",
        "                rows.append((q, t, float(a), q_hat.eval(s, a)))\n",
        "\n",
        "    dat = pd.DataFrame(rows, columns=['q', 't', 'a', 'q_hat'])\n",
        "\n",
        "    # Greedy action per state\n",
        "    idx = dat.groupby(['q', 't'])['q_hat'].idxmax()\n",
        "    greedy = dat.loc[idx, ['q', 't', 'a']].reset_index(drop=True)\n",
        "\n",
        "    # Max-Q grid (q as rows, t as columns)\n",
        "    q_max = dat.groupby(['q', 't'])['q_hat'].max().reset_index()\n",
        "    q_grid = q_max.pivot(index='q', columns='t', values='q_hat').values\n",
        "\n",
        "    return q_grid, greedy\n",
        "\n",
        "# Testing (keeping the original testing code)\n",
        "q_hat.reset() # Assuming q_hat is defined in the global scope\n",
        "trace = []\n",
        "ep_semi_grad_sarsa(env, episodes=100, gamma=1.0, eps=0.1, q_hat=q_hat, s_start=[[65,1]], callback=callback, trace=trace)\n",
        "print(trace[2])"
      ],
      "metadata": {
        "id": "UIDaFj3Tv3eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Your turn\n",
        "\n",
        "Expain the output stored in `trace`. What is the effect of argument `s_start=[[65,1]]` to the algorithm?\n",
        "\n",
        "*Your comments*"
      ],
      "metadata": {
        "id": "5zP7DPvrJQqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now plot the information in trace:"
      ],
      "metadata": {
        "id": "TlHTQ9_l7qaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from plotnine import ggplot, aes, geom_point, theme\n",
        "\n",
        "def plot_trace(trace, dat_mdp):\n",
        "    \"\"\"\n",
        "    Plot the max-Q surface per snapshot in `trace`,\n",
        "    plus an optional line plot of V(q, t=1) at `start_state`.\n",
        "    \"\"\"\n",
        "    num_3d = len(trace)\n",
        "    num_plots = num_3d + 1\n",
        "\n",
        "    # last trace and optimal q-values\n",
        "    plot_two_snapshots_plotly(trace, dat_mdp, env)\n",
        "\n",
        "    # last trace and optimal actions\n",
        "    dat_a = trace[-1]['a'].copy()  # DataFrame with ['q','t','a']\n",
        "    dat_a['alg'] = 'approx'\n",
        "    dat_a = dat_a >> bind_rows(dat_mdp >> rename(a = X.action) >> select(~X.v) >> mutate(alg = 'mdp'))\n",
        "    dat_a['a'] = pd.Categorical(dat_a['a'])\n",
        "    pt = (\n",
        "        ggplot(dat_a, aes(x = \"t\", y = \"q\", color = \"a\"))\n",
        "        + geom_point()\n",
        "        + facet_wrap('alg')\n",
        "        + theme(figure_size=(10, 5), legend_position='bottom')\n",
        "        + labs(title = \"Actions\")\n",
        "    )\n",
        "    pt.show()\n",
        "\n",
        "    # RMSEs\n",
        "    plot_rmse(trace)\n",
        "\n",
        "    # visited states\n",
        "    rows, cols = q_hat.s_ctr.shape\n",
        "    row_indices, col_indices = np.meshgrid(np.arange(rows), np.arange(cols), indexing='ij')\n",
        "    row_indices_flat = row_indices.flatten()\n",
        "    col_indices_flat = col_indices.flatten()\n",
        "    flat = q_hat.s_ctr.flatten()\n",
        "    dat_s = pd.DataFrame({\n",
        "        'q': row_indices_flat+1,\n",
        "        't': col_indices_flat+1,\n",
        "        'n': flat\n",
        "    })\n",
        "    dat_s = dat_s >> mask(X.n > 0)\n",
        "    pt = (\n",
        "        ggplot(dat_s, aes(x = 't', y = 'q', size = 'n'))\n",
        "        + geom_point(alpha = 0.5)\n",
        "        + theme(figure_size=(10,5), legend_position='bottom')\n",
        "        + labs(title = \"Visited states\")\n",
        "    )\n",
        "    pt.show()\n",
        "\n",
        "def plot_rmse(trace):\n",
        "    ep = [t['ep'] for t in trace]\n",
        "    rmse = [t['rmse'] for t in trace]\n",
        "    nrmse = [t['nrmse'] for t in trace]\n",
        "    wrmse = [t['wrmse'] for t in trace]\n",
        "    dat = pd.DataFrame({'ep': ep, 'rmse': rmse, 'measure': 'rmse'})\n",
        "    dat = dat >> bind_rows(pd.DataFrame({'ep': ep, 'rmse': nrmse, 'measure': 'nrmse'}))\n",
        "    dat = dat >> bind_rows(pd.DataFrame({'ep': ep, 'rmse': wrmse, 'measure': 'wrmse'}))\n",
        "    pt = (\n",
        "        ggplot(dat, aes(x = 'ep', y = 'rmse'))\n",
        "        + geom_line()\n",
        "        + facet_wrap('measure', scales=\"free\")\n",
        "        + theme(figure_size=(10,5))\n",
        "    )\n",
        "    pt.show()\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "def plot_q_3d_plotly(q_arr, env, title=\"Max Q-value per state\"):\n",
        "    \"\"\"Interactive 3D surface for max Q-values using Plotly.\"\"\"\n",
        "    # Clean up NaNs\n",
        "    if np.isnan(q_arr).all():\n",
        "        q_plot = np.zeros_like(q_arr)\n",
        "    else:\n",
        "        finite = q_arr[np.isfinite(q_arr)]\n",
        "        fill = float(finite.min()) if finite.size else 0.0\n",
        "        q_plot = np.nan_to_num(q_arr, nan=fill)\n",
        "\n",
        "    x = np.arange(1, env.max_inv + 1)   # inventory\n",
        "    y = np.arange(1, env.max_t + 1)     # time\n",
        "    Y, X = np.meshgrid(y, x)\n",
        "\n",
        "    fig = go.Figure(\n",
        "        data=[go.Surface(\n",
        "            x=X, y=Y, z=q_plot,\n",
        "            colorscale=\"Viridis\",\n",
        "            colorbar=dict(title=\"Q\"),\n",
        "            showscale=True,\n",
        "        )]\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        scene=dict(\n",
        "            xaxis_title=\"Inventory (q)\",\n",
        "            yaxis_title=\"Time (t)\",\n",
        "            zaxis_title=\"Max Q-value\"\n",
        "        ),\n",
        "        width=800,\n",
        "        height=600,\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "\n",
        "def plot_two_snapshots_plotly(trace, dat_mdp, env, id=-1):\n",
        "    \"\"\"\n",
        "    Plot the a Q-surface of a snapshot in `trace`,\n",
        "    and the optimal (MDP) Q-surface.\n",
        "    \"\"\"\n",
        "    fig = make_subplots(\n",
        "        rows=1, cols=2,\n",
        "        specs=[[{\"type\": \"surface\"}, {\"type\": \"surface\"}]],\n",
        "        subplot_titles=[f\"Episode {trace[id]['ep']}\", \"Optimal (MDP)\"]\n",
        "    )\n",
        "\n",
        "    # --- Data prep ---\n",
        "    q_arr = np.nan_to_num(trace[id]['q'])\n",
        "    tmp = dat_mdp.pivot(index='q', columns='t', values='v').values\n",
        "    x = np.arange(1, env.max_inv + 1)\n",
        "    y = np.arange(1, env.max_t + 1)\n",
        "    Y, X = np.meshgrid(y, x)\n",
        "\n",
        "    # --- Compute global z-range ---\n",
        "    zmin = np.nanmin([q_arr.min(), tmp.min()])\n",
        "    zmax = np.nanmax([q_arr.max(), tmp.max()])\n",
        "\n",
        "    # --- Surfaces ---\n",
        "    fig.add_trace(\n",
        "        go.Surface(x=X, y=Y, z=q_arr, colorscale=\"Viridis\",\n",
        "                   cmin=zmin, cmax=zmax, showscale=False),\n",
        "        row=1, col=1\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Surface(x=X, y=Y, z=tmp, colorscale=\"Viridis\",\n",
        "                   cmin=zmin, cmax=zmax, showscale=True, colorbar_title=\"Value\"),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "    # --- Layout with shared z-axis range ---\n",
        "    fig.update_layout(\n",
        "        width=1100, height=500,\n",
        "        scene=dict(\n",
        "            xaxis_title=\"Inventory (q)\",\n",
        "            yaxis_title=\"Time (t)\",\n",
        "            zaxis_title=\"Value\",\n",
        "            zaxis=dict(range=[zmin, zmax])\n",
        "        ),\n",
        "        scene2=dict(\n",
        "            xaxis_title=\"Inventory (q)\",\n",
        "            yaxis_title=\"Time (t)\",\n",
        "            zaxis_title=\"Value\",\n",
        "            zaxis=dict(range=[zmin, zmax])\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "plot_trace(trace, dat_mdp)\n"
      ],
      "metadata": {
        "id": "l9tGFOjY7nay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Your turn\n",
        "\n",
        "Explain what is plotted.\n",
        "\n",
        "*Your comments*"
      ],
      "metadata": {
        "id": "1_BQCG_uASWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us put all the code into a singe code cell:"
      ],
      "metadata": {
        "id": "iduWo4s_MQ7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define candidate sale prices\n",
        "prices = [10, 15, 20, 25]\n",
        "\n",
        "# instantiate environment\n",
        "env = RLEnvSeasonal(\n",
        "    max_inv=100,       # maximum inventory\n",
        "    max_t=15,          # selling horizon (weeks)\n",
        "    scrap_price=5.0,   # scrap value at final week\n",
        "    purchase_price=14.0,  # cost of each unit purchased at t=1\n",
        "    prices=prices,      # allowable selling prices\n",
        "    seed=876\n",
        ")\n",
        "\n",
        "# define function approx (step_size will be 0.4/nb_tilings)\n",
        "q_hat = FuncApproxTiles(env, actions=prices, step_size=1, nb_tilings=8, nb_tiles=8, init_val=0, deterministic=True, seed=53487)\n",
        "\n",
        "def callback1(ep, func_approx, trace, info, eval_every=1000):\n",
        "    callback(ep, func_approx, trace, info, eval_every)\n",
        "\n",
        "# train\n",
        "q_hat.reset_s_ctr()\n",
        "q_hat.reset()\n",
        "trace = []\n",
        "ep_semi_grad_sarsa(\n",
        "    env, episodes=5000, gamma=1.0, eps=0.3, q_hat=q_hat,\n",
        "    # s_start=[start_state],\n",
        "    callback=callback1, trace=trace,\n",
        "    max_steps=env.max_t,        # harmless safety\n",
        "    # eps_decay=decay_factor(0.00000000001, 0.3, 5000)\n",
        ")\n",
        "\n",
        "# plot results\n",
        "plot_trace(trace, dat_mdp)"
      ],
      "metadata": {
        "id": "Eib1IbeQJzi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Your turn\n",
        "\n",
        "Explain the output.\n",
        "\n",
        "- How good are our state-value estimates compared to optimal?\n",
        "\n",
        "  *Your comments*\n",
        "\n",
        "- How good are our action estimates compared to optimal?\n",
        "\n",
        "  *Your comments*\n",
        "\n",
        "- Do we visit all states during sampling?\n",
        "\n",
        "  *Your comments*\n",
        "  \n",
        "- How could we change the algorithm so visit more states during sampling?\n",
        "\n",
        "  *Your comments*\n",
        "\n",
        "- Which policy have we tried to find estimates for?\n",
        "\n",
        "  *Your comments*"
      ],
      "metadata": {
        "id": "sWnzITfniS_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Your turn\n",
        "\n",
        "Could other starting estimates of the q-values be useful? Test by running the code with your choice.   "
      ],
      "metadata": {
        "id": "8JU_9dVnOzGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "Your comments\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "gJw9U5r-PEja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently, we estimate the eps-soft policy. However, we may use a decay to let epsilon (exploration) decrease over the episodes."
      ],
      "metadata": {
        "id": "t9TmWmujfS5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decay_factor(goal_eps, start_eps, ep):\n",
        "    \"\"\"\n",
        "    Decay function for epsilon.\n",
        "\n",
        "    Args:\n",
        "        goal_eps (float): Final epsilon value wanted in the last episode.\n",
        "        start_eps (float): Initial epsilon value.\n",
        "        ep (int): Total number of episodes.\n",
        "\n",
        "    Returns:\n",
        "        decay factor to use to get to goal_eps in ep steps.\n",
        "    \"\"\"\n",
        "    return (goal_eps/start_eps)**(1/ep)\n"
      ],
      "metadata": {
        "id": "1dNOANRefkU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Your turn\n",
        "\n",
        "Use the decay function to set factor that must be used as argument `eps_decay` to `ep_semi_grad_sarsa`. Note you may here start with a high epsilon and set it to decay to e.g. 0.0001 in the end.\n",
        "\n",
        "- Did your results improve?"
      ],
      "metadata": {
        "id": "_l2uzdTQf0UU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "Your comments\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "eK1twqdygA6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Episodic semi-gradient expected SARSA\n",
        "\n",
        "Using Q-learning may not work since this is an off-policy algorithm, which may diverge due to the “deadly triad” (off-policy + bootstrapping + approximation). This is true even with linear features and fixed $\\epsilon$-greedy behavior; there is no general convergence guarantee.\n",
        "\n",
        "However, we may modify the algorithm to use expected SARSA instead."
      ],
      "metadata": {
        "id": "NuSPM3bMX7DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Episodic semi-gradient expected SARSA\n",
        "\n",
        "def ep_semi_grad_exp_sarsa(\n",
        "    env,\n",
        "    episodes,\n",
        "    gamma,\n",
        "    eps,\n",
        "    q_hat,\n",
        "    s_start=None,\n",
        "    callback=None,\n",
        "    trace=None,\n",
        "    max_steps=None,\n",
        "    eps_decay=None  # float or callable\n",
        "):\n",
        "    \"\"\"\n",
        "    Episodic semi-gradient Expected SARSA for RLEnvSeasonal-like env.\n",
        "\n",
        "    Expects:\n",
        "      env.reset() -> [q, t]\n",
        "      env.get_actions(s) -> list of feasible actions\n",
        "      env.get_step(s, a) -> (s_next, r, done)\n",
        "      q_hat.eval(s, a) -> float\n",
        "      q_hat.train(s, a, target) -> None\n",
        "    \"\"\"\n",
        "\n",
        "    def argmax_rand(values):\n",
        "        m = np.max(values)\n",
        "        idx = np.flatnonzero(values == m)\n",
        "        return np.random.choice(idx)\n",
        "\n",
        "    def eps_greedy_action(s, eps_curr):\n",
        "        actions = env.get_actions(s)\n",
        "        if np.random.rand() < eps_curr:\n",
        "            return float(np.random.choice(actions))\n",
        "        qvals = np.array([q_hat.eval(s, a) for a in actions], dtype=float)\n",
        "        return float(actions[argmax_rand(qvals)])\n",
        "\n",
        "    def expected_q(s, eps_curr):\n",
        "        \"\"\"\n",
        "        E[ Q(s, A) ] where A ~ epsilon-greedy over actions in s.\n",
        "        \"\"\"\n",
        "        actions = env.get_actions(s)\n",
        "        nA = len(actions)\n",
        "        qvals = np.array([q_hat.eval(s, a) for a in actions], dtype=float)\n",
        "\n",
        "        # greedy(s)\n",
        "        greedy_idx = argmax_rand(qvals)\n",
        "        greedy_q = qvals[greedy_idx]\n",
        "\n",
        "        # prob of each action under epsilon-greedy\n",
        "        # P(a) = eps/nA for all a, plus (1-eps) for greedy\n",
        "        base_p = eps_curr / nA\n",
        "        probs = np.full(nA, base_p, dtype=float)\n",
        "        probs[greedy_idx] += (1.0 - eps_curr)\n",
        "\n",
        "        # expected Q\n",
        "        return float(np.dot(probs, qvals))\n",
        "\n",
        "    def pick_start():\n",
        "        if s_start is None:\n",
        "            return env.reset()\n",
        "        try:\n",
        "            if isinstance(s_start, (str, bytes)):\n",
        "                return s_start\n",
        "            return list(s_start)[np.random.randint(len(s_start))]\n",
        "        except TypeError:\n",
        "            return s_start\n",
        "\n",
        "    eps_curr = float(eps)\n",
        "\n",
        "    for ep in tqdm(range(1, episodes + 1), desc=\"Episode\", unit=\"episode\",\n",
        "                   bar_format='{l_bar}{bar}| {n}/{total} [{elapsed}<{remaining}, {rate_fmt}{postfix}]'):\n",
        "        s = pick_start()\n",
        "        a = eps_greedy_action(s, eps_curr)\n",
        "        steps = 0\n",
        "\n",
        "        while True:\n",
        "            s_n, r, done = env.get_step(s, a)\n",
        "\n",
        "            if done:\n",
        "                # terminal target\n",
        "                q_hat.train(s, a, r)\n",
        "                break\n",
        "            else:\n",
        "                # Expected SARSA target\n",
        "                exp_q_next = expected_q(s_n, eps_curr)\n",
        "                target = r + gamma * exp_q_next\n",
        "                q_hat.train(s, a, target)\n",
        "\n",
        "                # next behavior action (still epsilon-greedy)\n",
        "                s, a = s_n, eps_greedy_action(s_n, eps_curr)\n",
        "\n",
        "            steps += 1\n",
        "            if max_steps is not None and steps >= max_steps:\n",
        "                break\n",
        "\n",
        "        # callback at the end of the episode\n",
        "        if callback is not None:\n",
        "            info = {\"epsilon\": eps_curr, \"steps\": steps}\n",
        "            callback(ep, q_hat, trace, info)\n",
        "\n",
        "        # optional epsilon schedule\n",
        "        if eps_decay is not None:\n",
        "            if callable(eps_decay):\n",
        "                eps_curr = float(eps_decay(ep, eps_curr))\n",
        "            else:\n",
        "                eps_curr = eps_curr * float(eps_decay)\n",
        "            eps_curr = min(1.0, max(0.0, eps_curr))\n"
      ],
      "metadata": {
        "id": "8JWIvb--YEYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Your turn\n",
        "\n",
        "- Run exp SARSA with your preferred parameters.\n",
        "- Are your results better?\n"
      ],
      "metadata": {
        "id": "ekJeLDLkaSw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "env.reset_rng()\n",
        "\n",
        "start_alph = 1/8  # overall alpha / nb tilings\n",
        "goal_alph = 0.01\n",
        "visits = 300  # approx max number of visits per state\n",
        "q_hat = FuncApproxTiles(\n",
        "    env, actions=prices,\n",
        "    step_size=1,\n",
        "    nb_tilings=8, nb_tiles=8, init_val=900,\n",
        "    deterministic=True,\n",
        "    decay_fct=decay_factor(goal_alph, start_alph, visits)\n",
        ")\n",
        "\n",
        "# train\n",
        "trace = []\n",
        "start_eps = 1\n",
        "goal_eps = 0.0001\n",
        "episodes = 5000\n",
        "# Your code\n",
        "\n",
        "# plot results\n",
        "plot_trace(trace, dat_mdp)\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "Your comments\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "LSnyKd0ZYKRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises"
      ],
      "metadata": {
        "id": "JdmH2DYfAUWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise (seasonal inventory)\n",
        "\n",
        "Solve the seasonal inventory and sales planning problem in Example 9.4.4.\n",
        "\n",
        "Use a Fourier basis as function approximation to estimate the action-values."
      ],
      "metadata": {
        "id": "Uxw48WuHAUWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "# Your code\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "Your comments\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "ZncEgp-9Aboi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise (car rental)\n",
        "\n",
        "Consider the [Car rental problem](https://bss-osca.github.io/rl/07_mdp-2.html#sec-mdp-2-car). Our goad is to use function approximation to estimate the optimal state value function.\n",
        "\n",
        "An environment is given below."
      ],
      "metadata": {
        "id": "E2Zu_el74U7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Car rental environment\n",
        "\n",
        "import numpy as np\n",
        "from typing import Callable, Tuple, List, Dict\n",
        "\n",
        "\n",
        "class RLEnvCar:\n",
        "    \"\"\"\n",
        "    Car rental environment (two locations) with Poisson demand/returns.\n",
        "\n",
        "    States are tuples (x, y) with x,y ∈ {0..20}.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 lD: List[float] = [3, 4],\n",
        "                 lH: List[float] = [3, 2],\n",
        "                 seed: int | None = None):\n",
        "        self.lD = list(lD)\n",
        "        self.lH = list(lH)\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.cars_max = 20  # max number of cars at each location\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the environment.\"\"\"\n",
        "        return (10, 10)\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    def get_states(self) -> List[Tuple[int, int]]:\n",
        "        \"\"\"Return all possible states as (x, y) tuples.\"\"\"\n",
        "        return [(x, y) for x in range(21) for y in range(21)]\n",
        "\n",
        "    def get_actions(self, s: Tuple[int, int]) -> List[int]:\n",
        "        \"\"\"Return feasible integer actions for state (x, y).\"\"\"\n",
        "        x, y = s\n",
        "        low = -min(5, y, 20 - x)\n",
        "        high = min(5, x, 20 - y)\n",
        "        return list(range(low, high + 1))\n",
        "\n",
        "    def get_step(self, s: Tuple[int, int], a: int) -> Dict[str, float | Tuple[int, int]]:\n",
        "        \"\"\"\n",
        "        Take one step in the environment.\n",
        "\n",
        "        Args:\n",
        "            s: Current state (x, y)\n",
        "            a: Action, cars moved from 1→2 (negative = 2→1)\n",
        "\n",
        "        Returns:\n",
        "            tuple (next_state, reward).\n",
        "        \"\"\"\n",
        "        x, y = s\n",
        "        low = -min(5, y, 20 - x)\n",
        "        high = min(5, x, 20 - y)\n",
        "\n",
        "        if not (low <= a <= high):\n",
        "            raise ValueError(f\"Action {a} not feasible in state {s}\")\n",
        "\n",
        "        # Apply overnight move\n",
        "        x_bar = x - a\n",
        "        y_bar = y + a\n",
        "\n",
        "        # Demand and returns\n",
        "        d_x = int(self.rng.poisson(self.lD[0]))\n",
        "        d_y = int(self.rng.poisson(self.lD[1]))\n",
        "        h_x = int(self.rng.poisson(self.lH[0]))\n",
        "        h_y = int(self.rng.poisson(self.lH[1]))\n",
        "\n",
        "        # Rentals served\n",
        "        served_x = min(d_x, x_bar)\n",
        "        served_y = min(d_y, y_bar)\n",
        "\n",
        "        # Next state (capped)\n",
        "        x_next = min(20, x_bar - served_x + h_x)\n",
        "        y_next = min(20, y_bar - served_y + h_y)\n",
        "\n",
        "        reward = 10.0 * (served_x + served_y) - 2.0 * abs(a)\n",
        "\n",
        "        return (x_next, y_next), float(reward)\n"
      ],
      "metadata": {
        "id": "NuUbt07W6Edu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us create an instance of the environment:"
      ],
      "metadata": {
        "id": "iQ9tZed-SHMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = RLEnvCar(seed=42)\n",
        "print(env.get_step((10, 10), 3))"
      ],
      "metadata": {
        "id": "lgyY9-EK7FzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q1\n",
        "\n",
        "What is the return output of method `get_step`?\n",
        "\n",
        "*Your comment*"
      ],
      "metadata": {
        "id": "2flIKyEd7Dhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Differential (average-reward) semi-gradient Expected SARSA\n",
        "\n",
        "To do function approximation, we modify the episodic expected SARSA version to a continuous task version:"
      ],
      "metadata": {
        "id": "AL-bnQFT76X2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Differential (average-reward) semi-gradient Expected SARSA\n",
        "\n",
        "import numpy as np\n",
        "from typing import Any, Callable, Optional, Tuple, Dict\n",
        "\n",
        "def cont_diff_expected_sarsa(\n",
        "    env: Any,\n",
        "    total_steps: int,\n",
        "    beta: float,\n",
        "    eps: float = 0.1,\n",
        "    q_hat: Any = None,\n",
        "    s_start: Optional[Tuple[int, int]] = None,\n",
        "    callback: Optional[Callable[[int, Any, Dict[str, float]], None]] = None,\n",
        "    callback_every: int = 0,\n",
        "    trace = None,\n",
        "    eps_decay: Optional[Callable[[int, float], float] | float] = None,\n",
        "    rng: Optional[np.random.Generator] = None,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Differential (average-reward) semi-gradient Expected SARSA for continuing tasks.\n",
        "\n",
        "    TD target:\n",
        "        target = (r - R_bar) + gamma * E_{a'~pi_eps}[ Q(s', a') ]\n",
        "    TD error:\n",
        "        delta  = target - Q(s, a)\n",
        "\n",
        "    Policy is epsilon-greedy over feasible actions with tie-aware greedy mass\n",
        "    (ties share the (1 - eps) probability).\n",
        "\n",
        "    Expected interfaces:\n",
        "        - env.get_actions(s) -> list of feasible actions\n",
        "        - env.get_step(s, a) -> (s_next, r)\n",
        "        - q_hat.eval(s, a) -> float\n",
        "        - q_hat.train(s, a, target) -> None  # uses its own stepsize internally\n",
        "\n",
        "    Args:\n",
        "        env: Environment with get_actions/get_step and (optionally) reset().\n",
        "        total_steps: Number of environment steps (continuing task).\n",
        "        beta: Stepsize for the average reward estimate R_bar.\n",
        "        eps: Epsilon for epsilon-greedy behavior.\n",
        "        q_hat: Function approximator with eval()/train().\n",
        "        s_start: Optional start state; if None, uses env.reset().\n",
        "        callback: Optional callback(step, q_hat, info) called periodically.\n",
        "        callback_every: Call callback every N steps (0 disables).\n",
        "        trace: passed through to callback\n",
        "        eps_decay: Float multiplier (e.g., 0.999) or callable f(step, eps)->new_eps.\n",
        "        rng: Optional NumPy Generator.\n",
        "\n",
        "    Returns:\n",
        "        float: Final estimate of the average reward R_bar.\n",
        "    \"\"\"\n",
        "    if rng is None:\n",
        "        rng = np.random.default_rng()\n",
        "\n",
        "    def argmax_random_tie(values: np.ndarray) -> int:\n",
        "        m = np.max(values)\n",
        "        ties = np.flatnonzero(values == m)\n",
        "        return int(rng.choice(ties))\n",
        "\n",
        "    def epsilon_greedy_action(s: Tuple[int, int], eps_curr: float) -> int:\n",
        "        actions = env.get_actions(s)\n",
        "        if not actions:\n",
        "            raise RuntimeError(f\"No feasible actions in state: {s}\")\n",
        "        if rng.random() < eps_curr:\n",
        "            return actions[int(rng.integers(len(actions)))]\n",
        "        qvals = np.array([q_hat.eval(s, a) for a in actions], dtype=float)\n",
        "        return actions[argmax_random_tie(qvals)]\n",
        "\n",
        "    def expected_q_under_eps_greedy(s: Tuple[int, int], eps_curr: float) -> float:\n",
        "        actions = env.get_actions(s)\n",
        "        nA = len(actions)\n",
        "        if nA == 0:\n",
        "            return 0.0\n",
        "        qvals = np.array([q_hat.eval(s, a) for a in actions], dtype=float)\n",
        "        greedy_val = qvals.max()\n",
        "        greedy_mask = (qvals == greedy_val)\n",
        "        base = eps_curr / nA\n",
        "        probs = np.full(nA, base, dtype=float)\n",
        "        probs[greedy_mask] += (1.0 - eps_curr) / greedy_mask.sum()\n",
        "        return float(probs @ qvals)\n",
        "\n",
        "    # Initialize starting state\n",
        "    s = env.reset() if s_start is None else s_start\n",
        "\n",
        "    # Running average reward\n",
        "    bar_R = 0.0\n",
        "    eps_curr = float(eps)\n",
        "\n",
        "    # First action\n",
        "    a = epsilon_greedy_action(s, eps_curr)\n",
        "\n",
        "    for step in tqdm(range(1, total_steps + 1), desc=\"Step\", unit=\" step\",\n",
        "                   bar_format='{l_bar}{bar}| {n}/{total} [{elapsed}<{remaining}, {rate_fmt}{postfix}]'):\n",
        "        s_next, r = env.get_step(s, a)\n",
        "\n",
        "        # Differential Expected SARSA target and TD error\n",
        "        exp_q_next = expected_q_under_eps_greedy(s_next, eps_curr)\n",
        "        q_sa = q_hat.eval(s, a)\n",
        "        target = (r - bar_R) + exp_q_next\n",
        "        delta = target - q_sa\n",
        "\n",
        "        # Update average reward estimate first (classic differential TD style)\n",
        "        bar_R += beta * delta\n",
        "\n",
        "        # Semi-gradient update for Q (q_hat handles its own stepsize)\n",
        "        q_hat.train(s, a, target)\n",
        "\n",
        "        # Next state/action\n",
        "        s = s_next\n",
        "        a = epsilon_greedy_action(s, eps_curr)\n",
        "\n",
        "        # Optional epsilon schedule\n",
        "        if eps_decay is not None:\n",
        "            eps_curr = float(eps_decay(step + 1, eps_curr)) if callable(eps_decay) \\\n",
        "                       else float(eps_curr * eps_decay)\n",
        "            eps_curr = min(1.0, max(0.0, eps_curr))\n",
        "\n",
        "        # Optional callback (fire after step+1 transitions)\n",
        "        if (step == 0 or (callback_every and ((step) % callback_every == 0))) and (callback is not None):\n",
        "            info = {\n",
        "                \"epsilon\": eps_curr,\n",
        "                \"bar_R\": bar_R,\n",
        "                \"step\": step,\n",
        "                \"beta\": beta,\n",
        "            }\n",
        "            callback(q_hat, info, trace)\n",
        "\n",
        "    return float(bar_R)"
      ],
      "metadata": {
        "id": "6dO21eL38JNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Q2\n",
        "\n",
        "Explain the main differences compared to the episodic version.\n",
        "\n",
        "*Your comment*\n",
        "\n",
        "What is the effect of `eps_decay`?\n",
        "\n",
        "*Your comment*"
      ],
      "metadata": {
        "id": "zR565iG2ShGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution (example)\n",
        "\n",
        "eps_curr = 0.1\n",
        "eps_decay = 0.999995\n",
        "\n",
        "for step in (1, 10, 1000, 5000, 50000, 100000, 500000):\n",
        "    eps_curr = eps_curr * (eps_decay ** (step - 1))\n",
        "    print(f'Step = {step}, eps_curr = {eps_curr}')"
      ],
      "metadata": {
        "id": "TGkA7SvCkJTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Function approximation with tiles\n",
        "\n",
        "Let us try to estimate using tile coding."
      ],
      "metadata": {
        "id": "I7etksP8QXeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Function approx with tiles\n",
        "\n",
        "class FuncApproxTiles:\n",
        "    \"\"\"\n",
        "    Function approximator with 2D tile coding.\n",
        "\n",
        "    Assume tile coder (the same tiles/areas are used) for each discrete action.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, step_size, nb_tilings, nb_tiles, init_val, seed=None, deterministic=False, decay_fct=1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            env: The environment.\n",
        "            step_size: Step size, will be adjusted for nb_tilings automatically\n",
        "            nb_tilings: Number of tilings per action.\n",
        "            nb_tiles: Number of tiles per dimension.\n",
        "            init_val: initial action-values q(s,a).\n",
        "            seed: Seed used by the tile class.\n",
        "            deterministic: If true use determenistic offsets.\n",
        "            decay_fct: Decay factor for step size given a state, i.e. use\n",
        "                decay_factor^num_of_visits * alpha - 1 as step size.\n",
        "        \"\"\"\n",
        "        self._n_dim = 2\n",
        "        self.alpha = step_size / nb_tilings\n",
        "        self.decay_fct = decay_fct\n",
        "        self._num_tilings_per_action = nb_tilings\n",
        "        states =  env.get_states()\n",
        "        actions = list({a for s in states for a in env.get_actions(s)})\n",
        "        self.actions = actions\n",
        "        self.act2idx = {a: i for i, a in enumerate(actions)}\n",
        "        self.st2idx = {s: i for i, s in enumerate(states)}\n",
        "        # feasible actions tile coder (use a single one = same tiles for all actions)\n",
        "        self.tc = TileCoder(\n",
        "            n_tilings=nb_tilings,\n",
        "            tiles_per_dim=[nb_tiles, nb_tiles],\n",
        "            ranges=[(1.0, float(env.cars_max)), (1.0, float(env.cars_max))],\n",
        "            wrap=[False, False],\n",
        "            deterministic=deterministic,\n",
        "            seed=seed\n",
        "        )\n",
        "        self._init_val = float(init_val)\n",
        "        ## feature matrix (a row for each action)\n",
        "        self.w = np.zeros((len(actions), self.tc.n_features), dtype=np.float32) + self._init_val / nb_tilings\n",
        "        self.s_ctr = np.zeros(len(states), dtype=int)  # to store visits to s (use st2idx to find idx)\n",
        "\n",
        "    def reset_weights(self, keep_init=True):\n",
        "        if keep_init:\n",
        "            self.w[:] = self._init_val / self._num_tilings_per_action\n",
        "        else:\n",
        "            self.w.fill(0.0)\n",
        "\n",
        "    def reset_s_ctr(self):\n",
        "        self.s_ctr.fill(0)\n",
        "\n",
        "    def reset(self):\n",
        "        self.reset_weights()\n",
        "        self.reset_s_ctr()\n",
        "\n",
        "    def eval(self, state, action):\n",
        "        \"\"\"\n",
        "        Return Q(s, a) for a specific action.\n",
        "\n",
        "        Args:\n",
        "            state: The state.\n",
        "            action: The action.\n",
        "        \"\"\"\n",
        "        assert len(state) == self._n_dim\n",
        "        assert np.isscalar(action)\n",
        "        a = self.act2idx[action]\n",
        "        idx = self.tc.active_indices(state)\n",
        "        return float(self.w[a, idx].sum())\n",
        "\n",
        "    def train(self, state, action, target):\n",
        "        \"\"\"\n",
        "        Update Q(s, a) for a specific action using semi-gradient TD.\n",
        "\n",
        "        Args:\n",
        "            state: The state.\n",
        "            action: The action.\n",
        "            target: The target value.\n",
        "        \"\"\"\n",
        "        assert len(state) == self._n_dim\n",
        "        assert np.isscalar(action)\n",
        "        assert np.isscalar(target)\n",
        "        self.s_ctr[self.st2idx[state]] += 1\n",
        "        a = self.act2idx[action]\n",
        "        idx = self.tc.active_indices(state)\n",
        "        pred = self.w[a, idx].sum()\n",
        "        err = target - pred\n",
        "        alpha = self.alpha * (self.decay_fct ** (float(self.s_ctr[self.st2idx[state]]) - 1))\n",
        "        self.w[a, idx] += alpha * err\n",
        "\n",
        "## Testing\n",
        "q_hat = FuncApproxTiles(env, step_size=1, nb_tilings=8, nb_tiles=4, init_val=0, decay_fct = 0.999)\n",
        "print(\"Active tiles for state [1,1]:\", q_hat.tc.active_indices([1,1]))\n",
        "print(\"Active tiles for state [10,15]:\", q_hat.tc.active_indices([10,15]))"
      ],
      "metadata": {
        "id": "I5KGM_FGTd0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Q3\n",
        "\n",
        "Explain the function class.\n",
        "\n",
        "- How are tiles used for each action?\n",
        "\n",
        "  *Your comments*\n",
        "\n",
        "- Which step-size alpha is used for estimation?\n",
        "\n",
        "  *Your comments*\n",
        "\n",
        "- What is the purpose of the argument `decay_fct`?\n",
        "\n",
        "  *Your comments*"
      ],
      "metadata": {
        "id": "PmxuFzx-etNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution (example)\n",
        "\n",
        "decay_fct = 0.999\n",
        "alpha = 1/8\n",
        "for i in (0,1,10,50,100,200,500):\n",
        "    print(f'Visits = {i+1}, step-size = {alpha * decay_fct ** i}')"
      ],
      "metadata": {
        "id": "ym8d_488h4wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q4\n",
        "\n",
        "Run differential (average-reward) semi-gradient Expected SARSA over 20 steps using code:"
      ],
      "metadata": {
        "id": "6gBpfCIEjl4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_bar_R = cont_diff_expected_sarsa(\n",
        "    env=env,\n",
        "    total_steps=20,\n",
        "    beta=0.01,        # avg-reward stepsize (try 0.005–0.02)\n",
        "    eps=0.1,\n",
        "    q_hat=q_hat,\n",
        "    s_start=(10, 10),\n",
        "    eps_decay=0.999995,\n",
        "    callback_every=10,\n",
        "    callback=lambda qhat, info, trace: print(\n",
        "        f\"[{info['step']}] bar_R={info['bar_R']:.3f} eps={info['epsilon']:.3f}\"\n",
        "    ),\n",
        ")\n",
        "print(\"Final average reward estimate:\", final_bar_R)"
      ],
      "metadata": {
        "id": "i4blZ-Wmec_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain the arguments used as input and the output.\n",
        "\n",
        "*Your comments*"
      ],
      "metadata": {
        "id": "2XZowLPnoHc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us define a callback that stores info about the solution and some helper functions for evaluating the approximation."
      ],
      "metadata": {
        "id": "-OYkEyRwvX4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def callback(func_approx, info, trace):\n",
        "    \"\"\"\n",
        "    Callback for differential (average-reward) semi-gradient Expected SARSA.\n",
        "    \"\"\"\n",
        "    step = info['step']\n",
        "    # Evaluate learned Q(s,a) → V(s) = max_a Q(s,a)\n",
        "    q_grid, greedy_actions = eval_state_action_space(func_approx, env)\n",
        "\n",
        "    val = {\n",
        "        'q': q_grid,\n",
        "        'a': greedy_actions,\n",
        "        'epsilon': info.get('epsilon'),\n",
        "        'step': step,\n",
        "        'bar_R': info.get('bar_R'),\n",
        "    }\n",
        "    trace.append(val)\n",
        "\n",
        "\n",
        "def eval_state_action_space(q_hat, env):\n",
        "    \"\"\"\n",
        "    Evaluate Q(s,a).\n",
        "\n",
        "    Args:\n",
        "        q_hat: Function approximator with method: q_hat.eval(state, action) -> float\n",
        "        env: Environment with members env.max_inv, env.max_t, env.get_actions(state)\n",
        "\n",
        "    Returns:\n",
        "        q_grid: np.ndarray shape with max_a Q(s,a)\n",
        "        greedy: pd.DataFrame with columns ['x','y','a'] for argmax_a Q(s,a)\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for x in range(env.cars_max+1):\n",
        "        for y in range(env.cars_max+1):\n",
        "            s = [x, y]\n",
        "            for a in env.get_actions(s):             # includes [scrap_price] at t==max_t\n",
        "                rows.append((x, y, int(a), q_hat.eval(s, a)))\n",
        "\n",
        "    dat = pd.DataFrame(rows, columns=['x', 'y', 'a', 'q_hat'])\n",
        "\n",
        "    # Greedy action per state\n",
        "    idx = dat.groupby(['x', 'y'])['q_hat'].idxmax()\n",
        "    greedy = dat.loc[idx, ['x', 'y', 'a']].reset_index(drop=True)\n",
        "\n",
        "    # Max-Q grid (q as rows, t as columns)\n",
        "    q_max = dat.groupby(['x', 'y'])['q_hat'].max().reset_index()\n",
        "    q_grid = q_max.pivot(index='x', columns='y', values='q_hat').values\n",
        "\n",
        "    return q_grid, greedy\n",
        "\n",
        "def plot_trace(trace, dat_mdp):\n",
        "    \"\"\"\n",
        "    Plot the max-Q surface for the last snapshot in `trace`, the corresponding\n",
        "    actions, the visited states and the average reward.\n",
        "    \"\"\"\n",
        "    # last trace and optimal q-values\n",
        "    plot_q_val(trace, dat_mdp, env)\n",
        "\n",
        "    # last trace and optimal actions\n",
        "    dat_a = trace[-1]['a'].copy()  # DataFrame with ['x','y','a']\n",
        "    dat_a['a'] = pd.Categorical(dat_a['a'])\n",
        "    pt = (\n",
        "        ggplot(dat_a, aes(x = \"x\", y = \"y\", color = \"a\"))\n",
        "        + geom_point()\n",
        "        + theme(figure_size=(10, 5), legend_position='bottom')\n",
        "        + labs(title = \"Actions\")\n",
        "    )\n",
        "    pt.show()\n",
        "\n",
        "    # visited states\n",
        "    dat_s = [ [s[0], s[1], q_hat.s_ctr[q_hat.st2idx[s]]] for s in env.get_states()]\n",
        "    dat_s = pd.DataFrame(dat_s, columns=['x', 'y', 'n'])\n",
        "    dat_s = dat_s >> mask(X.n > 0)\n",
        "    pt = (\n",
        "        ggplot(dat_s, aes(x = 'x', y = 'y', size = 'n'))\n",
        "        + geom_point(alpha = 0.5)\n",
        "        + theme(figure_size=(10, 5), legend_position='bottom')\n",
        "        + labs(title = \"Visited states\")\n",
        "    )\n",
        "    pt.show()\n",
        "\n",
        "    # average reward\n",
        "    step = [t['step'] for t in trace]\n",
        "    r = [t['bar_R'] for t in trace]\n",
        "    dat = pd.DataFrame({'step': step, 'ave_reward': r})\n",
        "    pt = (\n",
        "        ggplot(dat, aes(x = 'step', y = 'r'))\n",
        "        + geom_line()\n",
        "        + theme(figure_size=(10, 5), legend_position='bottom')\n",
        "    )\n",
        "    pt.show()\n",
        "\n",
        "def plot_q_val(trace, dat_mdp, env, id=-1):\n",
        "    \"\"\"\n",
        "    Plot the a Q-surface of a snapshot in `trace`.\n",
        "    \"\"\"\n",
        "    fig = make_subplots(\n",
        "        rows=1, cols=1,\n",
        "        specs=[[{\"type\": \"surface\"}]],\n",
        "        subplot_titles=[f\"Step {trace[id]['step']}\"]\n",
        "    )\n",
        "\n",
        "    # --- Data prep ---\n",
        "    q_arr = np.nan_to_num(trace[id]['q'])\n",
        "    # tmp = dat_mdp.pivot(index='x', columns='y', values='Value').values\n",
        "    x = np.arange(0, env.cars_max + 1)\n",
        "    y = np.arange(0, env.cars_max + 1)\n",
        "    Y, X = np.meshgrid(y, x)\n",
        "\n",
        "    # --- Compute global z-range ---\n",
        "    zmin = np.nanmin([q_arr.min()])\n",
        "    zmax = np.nanmax([q_arr.max()])\n",
        "\n",
        "    # --- Surfaces ---\n",
        "    fig.add_trace(\n",
        "        go.Surface(x=X, y=Y, z=q_arr, colorscale=\"Viridis\",\n",
        "                   cmin=zmin, cmax=zmax, showscale=False),\n",
        "        row=1, col=1\n",
        "    )\n",
        "    # fig.add_trace(\n",
        "    #     go.Surface(x=X, y=Y, z=tmp, colorscale=\"Viridis\",\n",
        "    #                cmin=zmin, cmax=zmax, showscale=True, colorbar_title=\"Value\"),\n",
        "    #     row=1, col=2\n",
        "    # )\n",
        "\n",
        "    # --- Layout with shared z-axis range ---\n",
        "    fig.update_layout(\n",
        "        width=700, height=700,\n",
        "        scene=dict(\n",
        "            xaxis_title=\"x\",\n",
        "            yaxis_title=\"y\",\n",
        "            zaxis_title=\"Value\",\n",
        "            zaxis=dict(range=[zmin, zmax])\n",
        "        ),\n",
        "        # scene2=dict(\n",
        "        #     xaxis_title=\"Inventory (q)\",\n",
        "        #     yaxis_title=\"Time (t)\",\n",
        "        #     zaxis_title=\"Value\",\n",
        "        #     zaxis=dict(range=[zmin, zmax])\n",
        "        # ),\n",
        "    )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "q_hat.reset()\n",
        "trace = []\n",
        "total_steps = 1000\n",
        "cont_diff_expected_sarsa(\n",
        "    env=env,\n",
        "    total_steps=total_steps,\n",
        "    beta=0.01,\n",
        "    eps=0.1,\n",
        "    q_hat=q_hat,\n",
        "    s_start=(10, 10),\n",
        "    eps_decay=0.999995,\n",
        "    callback_every=50,\n",
        "    callback=callback,\n",
        "    trace=trace\n",
        ")\n",
        "plot_trace(trace, dat_mdp)"
      ],
      "metadata": {
        "id": "f8Y8TjU-1uCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q5\n",
        "\n",
        "1) Give a short summary of all the functions and their purpose.\n",
        "\n",
        "   *Your comments*\n",
        "\n",
        "2) Is the output reasonable?\n",
        "\n",
        "   *Your comments*\n"
      ],
      "metadata": {
        "id": "DPcmO4aEwN98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q6\n",
        "\n",
        "* Run the algorithm using 100000 steps, with `callback_every = 5000`.\n",
        "* Comment on your results.\n",
        "* Give an interpretation of $q(s,a)$, e.g. that does it mean if $q((3,15), -4) = 10$ or $-5$?"
      ],
      "metadata": {
        "id": "PWfaTxODvtgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "# Your code\n",
        "\n",
        "display(Markdown(\"\"\"\n",
        "Your comments\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "rCBH_DPRvt3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q7\n",
        "\n",
        "- Could another function approximation be suitable here?\n",
        "\n",
        "  *Your comments*\n",
        "\n",
        "- Apply your suggestion to the problem.\n",
        "\n",
        "  *See code below*"
      ],
      "metadata": {
        "id": "S_4zjq-gLuSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "# Your code"
      ],
      "metadata": {
        "id": "i9wyTw6R8Bho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a2luU3YqCGzN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}