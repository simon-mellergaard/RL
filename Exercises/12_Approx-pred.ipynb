{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simon-mellergaard/RL/blob/main/Exercises/12_Approx-pred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlrET7d3WOQ2"
      },
      "source": [
        "# Notebook pre steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0vEJAmyWKPb",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Installations\n",
        "\n",
        "# ALWAYS SAVE YOUR OWN COPY OF THIS NOTEBOOK: File > Save a copy in Drive\n",
        "# IF DANISH MENU DO: Hjælp > Engelsk version\n",
        "\n",
        "# To clear output do: Edit > Clear all outputs\n",
        "\n",
        "## Useful shortscuts\n",
        "# Run current cell: Cmd+Enter\n",
        "# Run current cell and goto next: Shift+Enter\n",
        "# Run selection (or line if no selection): Cmd+Shift+Enter\n",
        "\n",
        "# install missing packages\n",
        "!pip install dfply\n",
        "!python --version\n",
        "\n",
        "from dfply import *\n",
        "from plotnine import *\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np  # RNG and vector ops\n",
        "import pandas as pd  # tabular outputs\n",
        "import math\n",
        "import random\n",
        "from IPython.display import display, Markdown\n",
        "# import json\n",
        "# from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# On-policy prediction with approximation"
      ],
      "metadata": {
        "id": "g-9CaYq_xMc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example - A Random Walk\n",
        "\n",
        "Consider a large random walk environment with 1,000 non-terminal states labeled 1 through 1000. Two terminal states lie just outside this range: state 0 on the left, which yields a reward of −1, and state 1001 on the right, which yields a reward of +1. Each episode begins in the center state, 500.  \n",
        "\n",
        "From any nonterminal state $s$, the agent flips a fair coin to choose left or right, then samples a jump size $k$ uniformly from $\\{1, 2, \\dots, 100\\}$. The agent then moves to state $s - k$ or $s + k$. If the resulting state lies outside $[1,1000]$, the episode terminates immediately with the corresponding terminal reward. All intermediate transitions give a reward of 0. The policy is fixed and uniform. That is, a random policy with $\\pi(a|s) = 0.5$ where $a$ is either left or right.\n",
        "\n",
        "Let us first define the environment:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LeL6-J1h_Mdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Environment (non-generic)\n",
        "\n",
        "class EnvRandomWalk:\n",
        "    \"\"\"1,000-state random walk with terminal rewards ±1.\n",
        "\n",
        "    States are 1..1000 (nonterminal). A step chooses left/right with prob 1/2\n",
        "    and a jump size k ~ Uniform{1..100}. Stepping <1 yields reward -1 and\n",
        "    termination; stepping >1000 yields reward +1 and termination.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_states=1000, max_jump=100, start_state=500, rng_seed = None):\n",
        "        self.n = n_states\n",
        "        self.max_jump = max_jump\n",
        "        self.start_state = start_state\n",
        "        self.rng_seed = rng_seed\n",
        "        self.rng = np.random.default_rng(self.rng_seed)\n",
        "        self.v_opt = None\n",
        "        self.dist = None\n",
        "\n",
        "    def reset(self):\n",
        "        return self.start_state\n",
        "\n",
        "    def reset_rng(self):\n",
        "        self.rng = np.random.default_rng(self.rng_seed)\n",
        "\n",
        "    def step(self, s: int):\n",
        "        \"\"\"\n",
        "        Takes a step in the environment.\n",
        "\n",
        "        Args:\n",
        "            s: The current\n",
        "\n",
        "        Returns:\n",
        "            A tuple containing:\n",
        "                s_next: The next state.\n",
        "                r: The reward for the transition.\n",
        "                done: A boolean indicating whether the episode is done.\n",
        "        \"\"\"\n",
        "        k = self.rng.integers(1, self.max_jump + 1)\n",
        "        direction = -1 if self.rng.random() < 0.5 else 1\n",
        "        s_next = s + direction * k\n",
        "        if s_next < 1:\n",
        "            return s_next, -1.0, True\n",
        "        if s_next > self.n:\n",
        "            return s_next, +1.0, True\n",
        "        return s_next, 0.0, False\n",
        "\n",
        "    def calc_v_opt(self):\n",
        "        \"\"\"\n",
        "        Calculates the optimal state values under the policy.\n",
        "\n",
        "        Returns:\n",
        "            A numpy array representing the optimal state values.\n",
        "        \"\"\"\n",
        "        n = self.n\n",
        "        A = np.eye(n, dtype=float)\n",
        "        b = np.zeros(n, dtype=float)\n",
        "        p_each = 1.0 / (2 * self.max_jump)\n",
        "\n",
        "        for s in range(1, n + 1):\n",
        "            row = s - 1\n",
        "            # Left jumps\n",
        "            left_overflow = max(0, self.max_jump - (s - 1))\n",
        "            if left_overflow > 0:\n",
        "                b[row] += (-1.0) * p_each * left_overflow\n",
        "            left_max_in = min(self.max_jump, s - 1)\n",
        "            for k in range(1, left_max_in + 1):\n",
        "                s2 = s - k\n",
        "                A[row, s2 - 1] -= p_each\n",
        "            # Right jumps\n",
        "            right_overflow = max(0, (s + self.max_jump) - n)\n",
        "            if right_overflow > 0:\n",
        "                b[row] += (+1.0) * p_each * right_overflow\n",
        "            right_max_in = min(self.max_jump, n - s)\n",
        "            for k in range(1, right_max_in + 1):\n",
        "                s2 = s + k\n",
        "                A[row, s2 - 1] -= p_each\n",
        "\n",
        "        self.v_opt = np.linalg.solve(A, b)\n",
        "\n",
        "    def calc_on_policy_distribution(self, episodes=100000):\n",
        "        \"\"\"\n",
        "        Calculates the on-policy distribution by simulating episodes.\n",
        "\n",
        "        Args:\n",
        "            episodes: The number of episodes to simulate.\n",
        "\n",
        "        Returns:\n",
        "            A numpy array representing the empirical on-policy distribution over states.\n",
        "        \"\"\"\n",
        "        # Reset visit counts before simulation\n",
        "        visit_counts = np.zeros(self.n, dtype=int)\n",
        "        for _ in range(episodes):\n",
        "            s = self.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                visit_counts[s - 1] += 1\n",
        "                s, r, done = self.step(s)\n",
        "        total_visits = visit_counts.sum()\n",
        "        self.dist = visit_counts / total_visits if total_visits > 0 else np.zeros(self.n)"
      ],
      "metadata": {
        "id": "_TSqLkkyxWdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To calculate the \"true\" state-value function under the policy, we set $v(0) = -1$ and $v(1001) = 1$. The  Bellman equations (@eq-) then become\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "v(s)\n",
        "    =& \\sum_{a} \\pi(a|s)(r(s,a) + \\sum_{s'} p(s'\\mid s,a) v(s')  \\\\\n",
        "    =& \\sum_{a} \\pi(a|s) \\sum_{s'\\notin T} p(s'\\mid s,a) v(s') + \\sum_{s' \\in T} p(s'\\mid s) r(s') \\\\\n",
        "    =& \\sum_{a} 0.5 \\sum_{s'\\notin T} p(s'\\mid s,a) v(s') + Pr(1001|s) - Pr(0|s)  \\\\\n",
        "    =& \\sum_{s'\\notin T} p(s'\\mid s,a) v(s') + Pr(1001|s) - Pr(0|s)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "If we move a bit around, we get:\n",
        "$$\n",
        "v_s - \\sum_{s'\\notin T} p(s'\\mid s,a) v(s') = Pr(1001|s) - Pr(0|s) \\\\\n",
        "$$\n",
        "That is, to find $v$, we may solve a set of equations $\\textbf A\\textbf v = \\textbf b$ where the entries are\n",
        "$$\n",
        "\\begin{align}\n",
        " b_s =& Pr(1001|s) - Pr(0|s) \\\\\n",
        " A_{s,s'} =& -p(s'\\mid s,a) \\text{ if } s\\neq s'\\\\\n",
        "A_{s,s'} =& 1-p(s'\\mid s,a) \\text{ if } s= s'\n",
        "\\end{align}\n",
        "$$\n",
        "This is implemented in a method of the environment and automatically calculated when making an instance of the environment.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mDhvS0TszUVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = EnvRandomWalk(rng_seed=3543)\n",
        "env.calc_on_policy_distribution()\n",
        "env.calc_v_opt()"
      ],
      "metadata": {
        "id": "9ldo9n4RJ9AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us make a plot of the true state values ($v_\\pi(s)$):"
      ],
      "metadata": {
        "id": "yKrRG3PwJ-io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'state': range(1, env.n + 1), 'mu': env.dist, 'v_opt': env.v_opt})\n",
        "\n",
        "pt = (\n",
        "    ggplot(df, aes(x='state', y='v_opt')) +\n",
        "    geom_line() +\n",
        "    labs(title='True state values', x='State ($s$)', y='$v_\\\\pi(s)$') +\n",
        "    theme_bw()\n",
        ")\n",
        "pt.show()\n",
        "\n",
        "pt = (\n",
        "    ggplot(df, aes(x='state', y='mu')) +\n",
        "    geom_line() +\n",
        "    labs(title='Empirical state distribution $\\\\mu(s)$', x='State', y='Frequency') +\n",
        "    theme_bw()\n",
        ")\n",
        "pt.show()\n"
      ],
      "metadata": {
        "id": "kjZ2PysW4C1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal is to estimate this value function $v(s)$ under this policy using function approximation. We will use the example to consider different linear function approximations."
      ],
      "metadata": {
        "id": "7Hyl2bmHlgTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## State aggregation\n",
        "\n",
        "State aggregation is a special case of linear function approximation in\n",
        "which states are grouped together, with one estimated value (one\n",
        "component of the weight vector w) for each group. That is, the value of\n",
        "a state is estimated as its group’s value.\n"
      ],
      "metadata": {
        "id": "8e10W_Qny9ZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example - A random walk\n",
        "\n",
        "The 1000 states are divided into 10 groups, i.e. group $0,\\ldots,9$ of 100 consecutive states, and the approximate value function is piecewise constant $\\hat v(s; \\textbf w) = w_{g(s)}$, where $g(s)$ denotes the index of the group containing state $s$. That is,\n",
        "$$\n",
        "g(s)=\\left\\lfloor\\frac{s-1}{100}​\\right\\rfloor,\n",
        "$$\n",
        "and $\\textbf w = (w_0, \\ldots, w_{9})$ where $w_i$ is the estimated state value of group $i$. This setup produces a piecewise constant approximation due to aggregation.\n",
        "\n",
        "Learning is performed with gradient Monte Carlo prediction. Each episode yields a return $G$, which is simply the terminal reward. For each visited state $s$ in the episode, the weight of its group is updated according to  \n",
        "\n",
        "$$\n",
        "w_{g(s)} \\leftarrow w_{g(s)} + \\alpha \\, (G - w_{g(s)}),\n",
        "$$\n",
        "where the step size is $\\alpha = 2 \\times 10^{-5}$ and the experiment runs for 100,000 episodes.  \n",
        "\n",
        "Let us make an agent for the problem:"
      ],
      "metadata": {
        "id": "pwaTP9K8Oprw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjFzfhuczM5x"
      },
      "source": [
        "#@title Agent (parent class)\n",
        "\n",
        "class AgentApprox:\n",
        "    \"\"\"\n",
        "    An agent for on-policy prediction with linear function approximation for\n",
        "    the random walk.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, len_w):\n",
        "        \"\"\"\n",
        "        Initializes the AgentApproxAggregation.\n",
        "\n",
        "        Args:\n",
        "            env: The environment to interact with.\n",
        "            groups: The number of groups for state aggregation.\n",
        "            alpha: The step size for the gradient Monte Carlo update.\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.len_w = len_w\n",
        "        self.w = np.zeros(self.len_w, dtype=float)\n",
        "\n",
        "    def reset_w(self, val = 0):\n",
        "        \"\"\"\n",
        "        Resets the weights to a given value.\n",
        "\n",
        "        Args:\n",
        "            val: The value to reset the weights to.\n",
        "        \"\"\"\n",
        "        self.w.fill(val)\n",
        "\n",
        "    def gradient(self, s):\n",
        "        \"\"\"\n",
        "        Calculates the gradient for a given state.\n",
        "\n",
        "        Args:\n",
        "            s: The state number (1-based).\n",
        "\n",
        "        Returns:\n",
        "            A numpy array representing the gradient.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Subclasses must implement gradient\")\n",
        "\n",
        "    def gradient_mc_episode(self, alpha):\n",
        "        \"\"\"\n",
        "        Runs a single episode using the current policy and updates the weights\n",
        "        using gradient Monte Carlo.\n",
        "\n",
        "        Args:\n",
        "            alpha: The step size for the gradient Monte Carlo update.\n",
        "        \"\"\"\n",
        "        s = self.env.reset()\n",
        "        trajectory = []\n",
        "        done = False\n",
        "        while not done:\n",
        "            trajectory.append(s)\n",
        "            s, r, done = self.env.step(s)\n",
        "        G = r\n",
        "        for s_vis in trajectory:\n",
        "            x_vis = self.gradient(s_vis)\n",
        "            v_hat = np.dot(self.w, x_vis)\n",
        "            self.w += alpha * (G - v_hat) * x_vis\n",
        "\n",
        "\n",
        "    def semi_gradient_td0_episode(self, alpha):\n",
        "        \"\"\"\n",
        "        Runs a single episode using the current policy and updates the weights\n",
        "        using semi-gradient TD(0).\n",
        "\n",
        "        Args:\n",
        "            alpha: The step size for the semi-gradient TD(0) update.\n",
        "        \"\"\"\n",
        "        s = self.env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            s_n, r, done = self.env.step(s)\n",
        "            x = self.gradient(s)\n",
        "            v_hat = np.dot(self.w, x)\n",
        "            if not done:\n",
        "                x_n = self.gradient(s_n)\n",
        "                v_hat_n = np.dot(self.w, x_n)\n",
        "                self.w += alpha * (r + v_hat_n - v_hat) * x\n",
        "                s = s_n\n",
        "            else:\n",
        "                self.w += alpha * (r - v_hat) * x  # state value is 0 for the terminal state\n",
        "\n",
        "\n",
        "    def calculate_rmsve(self):\n",
        "        \"\"\"\n",
        "        Calculates the Root Mean Squared Weighted Value Error (RMSWVE).\n",
        "        Assumes that the distribution and optimal state values are calculated\n",
        "        in the environment.\n",
        "\n",
        "        Return:\n",
        "            rmsve: The RMSWVE value.\n",
        "        \"\"\"\n",
        "        v_approx = self.get_approx_values()\n",
        "        return np.sqrt(np.sum(env.dist * (v_approx - env.v_opt)**2))\n",
        "\n",
        "    def gradient_mc(self, episodes, alpha, eval_every=1000):\n",
        "        \"\"\"\n",
        "        Runs multiple episodes, updates the weights, and tracks RMSVE using\n",
        "        gradient Monte Carlo.\n",
        "\n",
        "        Args:\n",
        "            episodes: The number of episodes to run.\n",
        "            eval_every: The number of episodes between RMSVE evaluations.\n",
        "            alpha: The step size for the gradient Monte Carlo update.\n",
        "\n",
        "        Returns:\n",
        "            A tuple containing:\n",
        "                rmsve_hist: A list of (episode, RMSVE) tuples.\n",
        "        \"\"\"\n",
        "        rmsve_hist = []\n",
        "\n",
        "        for ep in range(episodes):\n",
        "            self.gradient_mc_episode(alpha)\n",
        "            if (ep + 1) % eval_every == 0:\n",
        "                rmsve = self.calculate_rmsve()\n",
        "                rmsve_hist.append(((ep + 1), rmsve))\n",
        "\n",
        "        return rmsve_hist\n",
        "\n",
        "    def semi_gradient_td0(self, episodes, alpha, eval_every=1000):\n",
        "        \"\"\"\n",
        "        Runs multiple episodes, updates the weights, and tracks RMSVE using\n",
        "        gradient Monte Carlo.\n",
        "\n",
        "        Args:\n",
        "            episodes: The number of episodes to run.\n",
        "            eval_every: The number of episodes between RMSVE evaluations.\n",
        "            alpha: The step size for the semi-gradient TD(0) update.\n",
        "\n",
        "        Returns:\n",
        "            A tuple containing:\n",
        "                rmsve_hist: A list of (episode, RMSVE) tuples.\n",
        "        \"\"\"\n",
        "        rmsve_hist = []\n",
        "\n",
        "        for ep in range(episodes):\n",
        "            self.semi_gradient_td0_episode(alpha)\n",
        "            if (ep + 1) % eval_every == 0:\n",
        "                rmsve = self.calculate_rmsve()\n",
        "                rmsve_hist.append(((ep + 1), rmsve))\n",
        "\n",
        "        return rmsve_hist\n",
        "\n",
        "    def get_approx_values(self):\n",
        "        \"\"\"\n",
        "        Approximated state values.\n",
        "\n",
        "        Returns:\n",
        "            A numpy array containing the approximated state values.\n",
        "        \"\"\"\n",
        "        return np.array([np.dot(self.w, self.gradient(s)) for s in range(1, self.env.n + 1)])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the class is a parent class. Some methods have to be overrided based on which linear model is used. Here we use stage aggregation and define a child class:"
      ],
      "metadata": {
        "id": "kCP4JSi1CT_h"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0cbc16c"
      },
      "source": [
        "#@title Agent aggregation (non-generic)\n",
        "\n",
        "class AgentApproxAggregation(AgentApprox):\n",
        "    \"\"\"\n",
        "    An agent for on-policy prediction with state aggregation.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, groups=10, alpha=2e-5):\n",
        "        \"\"\"\n",
        "        Initializes the AgentApproxAggregation.\n",
        "\n",
        "        Args:\n",
        "            env: The environment to interact with.\n",
        "            groups: The number of groups for state aggregation.\n",
        "            alpha: The step size for the gradient Monte Carlo update.\n",
        "        \"\"\"\n",
        "        super().__init__(env, len_w = groups)\n",
        "        self.groups = groups\n",
        "        if env.n % groups != 0: # check if env.n / groups is an integer\n",
        "            raise ValueError(\"env.n must be divisible by groups\")\n",
        "        self.group_size = env.n // groups # Use integer division\n",
        "\n",
        "    def group_index(self, s: int) -> int:\n",
        "        \"\"\"\n",
        "        Calculates the group index for a given state.\n",
        "\n",
        "        Args:\n",
        "            s: The state number (1-based).\n",
        "\n",
        "        Returns:\n",
        "            The index of the group containing the state (0-based).\n",
        "        \"\"\"\n",
        "        return (s - 1) // self.group_size\n",
        "\n",
        "    def gradient_mc_episode(self, alpha):\n",
        "        \"\"\"\n",
        "        Runs a single episode using the current policy and updates the weights\n",
        "        using gradient Monte Carlo.\n",
        "        \"\"\"\n",
        "        s = self.env.reset()\n",
        "        trajectory = []\n",
        "        done = False\n",
        "        while not done:\n",
        "            trajectory.append(s)\n",
        "            s, r, done = self.env.step(s)\n",
        "        G = r\n",
        "        for s_vis in trajectory:\n",
        "            g = self.group_index(s_vis)\n",
        "            self.w[g] += alpha * (G - self.w[g])\n",
        "\n",
        "    def semi_gradient_td0_episode(self, alpha):\n",
        "        \"\"\"\n",
        "        Runs a single episode using the current policy and updates the weights\n",
        "        using semi-gradient TD(0).\n",
        "        \"\"\"\n",
        "        s = self.env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            sN, r, done = self.env.step(s)\n",
        "            g = self.group_index(s)\n",
        "            if not done:\n",
        "                gN = self.group_index(sN)\n",
        "                self.w[g] += alpha * (r + self.w[gN] - self.w[g])\n",
        "                s = sN\n",
        "            else:\n",
        "                self.w[g] += alpha * (r - self.w[g])  # self.w[gN] = 0 for the terminal state\n",
        "\n",
        "\n",
        "    def get_approx_values(self):\n",
        "        \"\"\"\n",
        "        Returns the approximated state values\n",
        "\n",
        "        Returns:\n",
        "            A numpy array containing the approximated state values.\n",
        "        \"\"\"\n",
        "        return np.array([self.w[self.group_index(s)] for s in range(1, self.env.n + 1)])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to do function approximation using state aggregation:"
      ],
      "metadata": {
        "id": "Zxp_otSCllq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_and_plot(env, agent, episodes, alpha, eval_every = 1000, show_pt_v = True, show_pt_rms = True, mc = True):\n",
        "    \"\"\"\n",
        "    Runs the experiment and plots the results.\n",
        "\n",
        "    Args:\n",
        "        env: The environment to interact with.\n",
        "        agent: The agent for function approximation.\n",
        "        episodes: The number of episodes to run.\n",
        "        alpha: The step size parameter.\n",
        "        eval_every: The number of episodes between RMSVE evaluations.\n",
        "        show_pt_v: Whether to show the plot of state values.\n",
        "        show_pt_rms: Whether to show the plot of RMSVE.\n",
        "        mc: Whether to use gradient Monte Carlo or semi-gradient TD(0).\n",
        "\n",
        "    \"\"\"\n",
        "    if mc:\n",
        "        rmsve_hist = agent.gradient_mc(episodes, alpha, eval_every)\n",
        "    else:\n",
        "        rmsve_hist = agent.semi_gradient_td0(episodes, alpha, eval_every)\n",
        "\n",
        "    # Data frame with results\n",
        "    states = range(1, env.n +1)\n",
        "    df = (pd.DataFrame({'state': states, 'v_opt': env.v_opt})\n",
        "        >> mutate(v_approx = agent.get_approx_values(),\n",
        "                pr_empi = env.dist)\n",
        "    )\n",
        "\n",
        "    # Plots\n",
        "    dat = df >> gather('variable', 'value', ['v_opt', 'v_approx'])\n",
        "    if show_pt_v:\n",
        "        pt = (\n",
        "            ggplot(dat, aes(x='state'))\n",
        "            + geom_line(aes(y='value', color='variable'))\n",
        "            + labs(title='State values (optimal and aggegated)', y = 'state value', color='')\n",
        "            + theme_bw()\n",
        "        )\n",
        "        pt.show()\n",
        "\n",
        "    # Plot RMSVE history\n",
        "    rmsve_df = pd.DataFrame(rmsve_hist, columns=['Episode', 'RMSVE'])\n",
        "    if show_pt_rms:\n",
        "        pt_rmsve = (\n",
        "            ggplot(rmsve_df, aes(x='Episode', y='RMSVE')) +\n",
        "            geom_line() +\n",
        "            labs(title='RMSVE vs Episodes (State Aggregation)', x='Episodes', y='RMSVE') +\n",
        "            theme_bw()\n",
        "        )\n",
        "        pt_rmsve.show()\n",
        "\n",
        "    print(\"RMS for last episode:\\n\", rmsve_df >> tail(n=1))\n",
        "    return rmsve_df"
      ],
      "metadata": {
        "id": "ehGdLEQVAtMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let us try to apply gradient MC:"
      ],
      "metadata": {
        "id": "bW4L7gTXLRkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = AgentApproxAggregation(env, groups=10)\n",
        "env.reset_rng()\n",
        "df = run_and_plot(env, agent, episodes=100000, mc = True, alpha=2e-5)"
      ],
      "metadata": {
        "id": "v5mK3Q05LV8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe that the approximated values do depend on the distribution of $\\mu$ in a group. Indeed, the effect of the distribution is on the leftmost groups, whose values are clearly shifted higher than the unweighted average of the true values of states within the group, and on the rightmost groups, whose values are clearly shifted lower. This is due to the states in these areas having the greatest asymmetry in their weightings by μ. For example, in the\n",
        "leftmost group, state 100 is weighted more than 3 times more strongly than state 1. Thus the estimate for the group is biased toward the true value of state 100, which is higher than the true value of state 1."
      ],
      "metadata": {
        "id": "SeIMT0YDaa57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we apply semi-gradiant TD(0):"
      ],
      "metadata": {
        "id": "vzb4ShJuLaeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent.reset_w(0)  # reset w = 0\n",
        "env.reset_rng()   # use the same episodes\n",
        "df = run_and_plot(env, agent, episodes=50000, mc = False, alpha=0.0005, eval_every = 1000)  # TD(0)"
      ],
      "metadata": {
        "id": "uK3Fke8iLgNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your turn\n",
        "\n",
        "- Is the approximation better than for gradient MC? Why?\n",
        "\n",
        "  *Answer here*\n",
        "\n",
        "- Do we find an approximation using semi-gradient TD(0) faster?\n",
        "\n",
        "  *Answer here*\n",
        "\n"
      ],
      "metadata": {
        "id": "u9PBzaeolvQX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us try to create 20 groups instead:"
      ],
      "metadata": {
        "id": "uNi8EXyzdGUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset_rng()\n",
        "agent = AgentApproxAggregation(env, groups=20, alpha=2e-5)\n",
        "df = run_and_plot(env, agent, episodes=100000, show_pt_rms=False, mc = True, alpha=2e-5)"
      ],
      "metadata": {
        "id": "ozup_V6FZywG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note here the approximation of the left most and right most groups are bad.\n",
        "\n",
        "### Your turn\n",
        "\n",
        "Why is this the case?\n",
        "\n",
        "  *Answer here*"
      ],
      "metadata": {
        "id": "mJhdMl-0oF6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Polynomials\n",
        "\n",
        "Polynomial features approximate $v_\\pi(s)$ by fitting a low-degree polynomial of *normalized* state variables. Note the model remains linear in parameters $\\textbf w$ even though it is nonlinear in $s$.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PVL0qUq5plhn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example - A random walk"
      ],
      "metadata": {
        "id": "UrU4-HdqO6gv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us try to apply function approximation to the random walk. First, consider the plot of $v^*$. Let us try to find a linear regression for it."
      ],
      "metadata": {
        "id": "FvxvVpaNTblD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do linear regression b + ax on v_opt\n",
        "a, b = np.polyfit(range(1, env.n + 1), env.v_opt, 1)\n",
        "print(f'a = {a:.6f}, b = {b:.6f}')\n",
        "# plot the line together with v_opt\n",
        "df = pd.DataFrame({'state': range(1, env.n + 1), 'v': env.v_opt, 'estimate': b + a * np.array(range(1, env.n + 1))})\n",
        "df = df >> gather('variable', 'value', ['v', 'estimate'])\n",
        "df\n",
        "pt = (\n",
        "    ggplot(df, aes(x='state', y='value', color='variable')) +\n",
        "    geom_line(alpha = 0.75)\n",
        ")\n",
        "pt.show()"
      ],
      "metadata": {
        "id": "hiOXJ-vHTtJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your turn\n",
        "\n",
        "Comment on the result.\n",
        "\n",
        "*Answer here*"
      ],
      "metadata": {
        "id": "PVHawyFWyK4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now using a polynomial with degree 1 we would like to estimate $a$ and $b$ using function approximation. We first need an agent:"
      ],
      "metadata": {
        "id": "IB2DgTrtVogt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqJdX_heeObO"
      },
      "source": [
        "#@title Agent polynomial (non-generic)\n",
        "\n",
        "class AgentApproxPoly(AgentApprox):\n",
        "    \"\"\"\n",
        "    An agent for on-policy prediction with polynomial features.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, degree):\n",
        "        \"\"\"\n",
        "        Initializes the AgentApproxPoly.\n",
        "\n",
        "        Args:\n",
        "            env: The environment to interact with.\n",
        "            degree: The degree of the polynomial basis functions.\n",
        "            alpha: The step size for the gradient Monte Carlo update.\n",
        "        \"\"\"\n",
        "        super().__init__(env, len_w = degree + 1)\n",
        "        self.degree = degree\n",
        "\n",
        "    def gradient(self, s: int):\n",
        "        \"\"\"\n",
        "        Generates polynomial features for a given state.\n",
        "\n",
        "        Args:\n",
        "            s: The state number (1-based).\n",
        "\n",
        "        Returns:\n",
        "            A numpy array of polynomial features.\n",
        "        \"\"\"\n",
        "        # Normalize state to be between 0 and 1\n",
        "        z = s / (self.env.n + 1.0)\n",
        "        return np.array([z**k for k in range(self.degree + 1)], dtype=float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "874ef8f0"
      },
      "source": [
        "env.reset_rng()\n",
        "agent = AgentApproxPoly(env, degree=1)\n",
        "run_and_plot(env, agent, episodes=100000, alpha=0.00001)\n",
        "print(b,a)\n",
        "print(agent.w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your turn\n",
        "\n",
        "Comment on the plots.\n",
        "\n",
        "*Answer here*\n",
        "\n",
        "Try to run 25000 episodes more with a lower step-size."
      ],
      "metadata": {
        "id": "tV0WNnKAy4nc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "# code here"
      ],
      "metadata": {
        "id": "Q85lzlMoOtc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your turn\n",
        "\n",
        "Try to use a polynomial of degree 4 instead using 100000 episodes and the same alpha. Comment on the plots and final RMS.\n",
        "  \n",
        "*Answer here*"
      ],
      "metadata": {
        "id": "bhjgokJc0JZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "# code here"
      ],
      "metadata": {
        "id": "xiEEG7-HzeMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us try to do semi-gradient TD(0) function estimation instead:"
      ],
      "metadata": {
        "id": "-ACnfEGnBLp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset_rng()\n",
        "agent = AgentApproxPoly(env, degree=1)\n",
        "run_and_plot(env, agent, episodes=25000, mc = False, alpha = 0.001)\n",
        "print(b,a)\n",
        "print(agent.w)"
      ],
      "metadata": {
        "id": "1iJjehIZSlRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your turn\n",
        "\n",
        "Comment on the plots and the final RMS.\n",
        "\n",
        "*Answer here*"
      ],
      "metadata": {
        "id": "Wv5HnWGQ1hzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fourier basis\n",
        "\n",
        "The *Fourier basis* approximates $v_\\pi(s)$ using global sine/cosine waves over a *normalized* state. The model remains *linear in parameters* even though it can represent highly nonlinear shapes. The Fourier series/transforms are widely used because if a function to be approximated is known, then essentially any function can be approximated as accurately as desired. In reinforcement learning, where the functions to be approximated are unknown, Fourier basis functions are of interest because they are easy to use and can perform well.\n"
      ],
      "metadata": {
        "id": "TtwDhdXQKjaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example - A random walk\n",
        "\n",
        "First, we define the agent:"
      ],
      "metadata": {
        "id": "JpSWJUELO8N4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Agent Fourier (non-generic)\n",
        "\n",
        "class AgentApproxFourier(AgentApprox):\n",
        "    \"\"\"\n",
        "    An agent for on-policy prediction with Fourier features.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, degree):\n",
        "        \"\"\"\n",
        "        Initializes the AgentApproxFourier.\n",
        "\n",
        "        Args:\n",
        "            env: The environment to interact with.\n",
        "            degree: The degree of the Fourier basis functions.\n",
        "            alpha: The step size for the gradient Monte Carlo update.\n",
        "        \"\"\"\n",
        "        super().__init__(env, len_w = degree + 1)\n",
        "        self.degree = degree\n",
        "\n",
        "\n",
        "    def gradient(self, s: int):\n",
        "        \"\"\"\n",
        "        Generates Fourier features for a given state.\n",
        "\n",
        "        Args:\n",
        "            s: The state number (1-based).\n",
        "\n",
        "        Returns:\n",
        "            A numpy array of Fourier features.\n",
        "        \"\"\"\n",
        "        # Normalize state to be between 0 and 1\n",
        "        z = s / (self.env.n + 1.0)\n",
        "        return np.array([np.cos(k * np.pi * z) for k in range(self.degree + 1)], dtype=float)"
      ],
      "metadata": {
        "id": "sR-aZkIKB2Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us estimate using gradient MC and degree = 20:"
      ],
      "metadata": {
        "id": "FzJBk6SGFozq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset_rng()\n",
        "agent = AgentApproxFourier(env, degree=20)\n",
        "df = run_and_plot(env, agent, episodes=10000, eval_every=500, alpha=0.00005)"
      ],
      "metadata": {
        "id": "DtTNh2t5h8yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your turn\n",
        "\n",
        "Comment on the plot and final RMS.\n",
        "\n",
        "*Answer here*"
      ],
      "metadata": {
        "id": "5oJ36F5vF4Ba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your turn\n",
        "\n",
        "Run semi-gradient estimation too with only 1000 episodes. Try different step-sizes.\n",
        "\n"
      ],
      "metadata": {
        "id": "7EziNq--GXYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "# code here"
      ],
      "metadata": {
        "id": "HBGgeiHiGf0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Tile coding\n",
        "\n",
        "Tile coding can be viewed as a structured, grid-based special case of coarse coding that is simple and fast. Here hyper-rectangular receptive fields arranged on offset grids. Overlap comes from having multiple tilings; each tiling itself does not overlap (it’s a partition of the state space), but the union of tilings does.\n",
        "\n",
        "That is, first create a *tiling* that partitions the space into non-overlapping *tiles*. Next, offset the tiling and hereby creating multiple tilings. Each tiling is offset slightly, so two nearby states are likely to share some tiles even if they fall on different sides of a grid boundary in one tiling.\n",
        "\n",
        "Because each update touches only $n$ parameters, tile/coarse coding works *extremely well* with incremental semi-gradient TD:\n",
        "$$\n",
        "\\mathbf w \\leftarrow \\mathbf w + \\alpha\\,\\delta_t\\,\\mathbf x(S_t),\\qquad\n",
        "\\delta_t = R_{t+1} + \\gamma\\,\\hat v(S_{t+1}) - \\hat v(S_t).\n",
        "$$\n",
        "\n",
        "*Step-size scaling.* A common heuristic is to scale the per-tiling step size like $\\alpha \\approx \\frac{\\alpha_0}{n}$ (since $n$ weights are updated per step). This keeps the *total* update magnitude roughly controlled as you add more tilings.\n",
        "\n",
        "**Choosing design parameters**\n",
        "\n",
        "*Number of tilings ($m$).* More tilings increase overlap and stability but cost more memory/compute.\n",
        "\n",
        "*Tiles per dimension.* Finer grids reduce bias (higher resolution) but increase variance and memory. Note that the function is piecewise-constant within each tile (per tiling). Summing across tilings reduces the effective piecewise step size, but if tiles are still large relative to the variation in $v_\\pi$, you will see bias.Smaller tiles and fewer overlapping tilings increase variance (fewer shared samples per parameter). More tilings and/or larger tiles reduce variance by pooling data.\n",
        "\n",
        "*Normalization.* Always normalize each state coordinate to a fixed range (e.g., $[0,1]$) before tiling so that “one tile” has a consistent meaning across features and tasks.\n",
        "\n",
        "\n",
        "\n",
        "<!-- *A practical loop.* Start with moderate $m$ (e.g., 8) and moderate resolution. If learning is noisy or unstable, *increase $m$* or *reduce $\\alpha$*. If predictions are too blocky (high bias), *increase resolution* (more tiles per dimension) or *add tilings*. -->\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tef8na90LRX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example - A random walk\n",
        "\n",
        "First, we need an implementation of tile coding. We use this class as a black box."
      ],
      "metadata": {
        "id": "Mbk5RbUaO9v1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33fc4d64"
      },
      "source": [
        "#@title TileCoder class (use as is)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from plotnine import (\n",
        "    ggplot, aes, geom_rect, geom_text, geom_vline,\n",
        "    scale_y_reverse, labs, theme_bw, theme\n",
        ")\n",
        "import hashlib\n",
        "from typing import Iterable, Sequence, Tuple, Union\n",
        "\n",
        "\n",
        "# =============================================================\n",
        "# A practical TileCoder for RL (1D and ND)\n",
        "# - Deterministic, evenly spaced offsets by default\n",
        "# - Optional randomness via seed\n",
        "# - Returns sparse active indices; optional dense/hashed encoding\n",
        "# - Supports wrapping (per-dimension or global)\n",
        "# - Exposes 1D attributes so existing plotting utilities work\n",
        "# =============================================================\n",
        "class TileCoder:\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_tilings: int,\n",
        "        tiles_per_dim: Union[int, Sequence[int]],\n",
        "        ranges: Union[None, Sequence[Tuple[float, float]]] = None,\n",
        "        wrap: Union[bool, Sequence[bool]] = False,\n",
        "        seed: Union[None, int] = None,\n",
        "        deterministic: bool = True,\n",
        "        hash_size: Union[None, int] = None,\n",
        "    ):\n",
        "        assert n_tilings >= 1\n",
        "        self.n_tilings = int(n_tilings)\n",
        "\n",
        "        if isinstance(tiles_per_dim, Iterable) and not isinstance(tiles_per_dim, (str, bytes)):\n",
        "            self.tiles_per_dim = np.array(list(tiles_per_dim), dtype=int)\n",
        "        else:\n",
        "            self.tiles_per_dim = np.array([int(tiles_per_dim)], dtype=int)\n",
        "        assert np.all(self.tiles_per_dim >= 2)\n",
        "\n",
        "        self.d = int(self.tiles_per_dim.size)\n",
        "\n",
        "        # Ranges for normalization to [0,1]\n",
        "        if ranges is None:\n",
        "            self.ranges = np.array([(0.0, 1.0)] * self.d, dtype=float)\n",
        "        else:\n",
        "            assert len(ranges) == self.d\n",
        "            self.ranges = np.array(ranges, dtype=float)\n",
        "            assert np.all(self.ranges[:, 1] > self.ranges[:, 0])\n",
        "\n",
        "        # Wrap flags per dimension\n",
        "        if isinstance(wrap, Iterable) and not isinstance(wrap, (str, bytes)):\n",
        "            wrap = list(wrap)\n",
        "            assert len(wrap) == self.d\n",
        "            self.wrap = np.array(wrap, dtype=bool)\n",
        "        else:\n",
        "            self.wrap = np.array([bool(wrap)] * self.d, dtype=bool)\n",
        "\n",
        "        self.hash_size = None if hash_size is None else int(hash_size)\n",
        "        self.deterministic = bool(deterministic)\n",
        "        self.seed = seed\n",
        "\n",
        "        # Precompute strides for mixed-radix indexing\n",
        "        self.tiles_strides = np.ones(self.d, dtype=int)\n",
        "        for i in range(self.d - 2, -1, -1):\n",
        "            self.tiles_strides[i] = self.tiles_strides[i + 1] * self.tiles_per_dim[i + 1]\n",
        "        self.tiles_per_tiling = int(np.prod(self.tiles_per_dim))  # for 1D this equals tiles_per_dim\n",
        "\n",
        "        # Offsets per tiling and dimension, as fractions of the unit interval\n",
        "        self.offsets = self._make_offsets()\n",
        "\n",
        "        # Expose attributes used by the existing 1D plotting helpers\n",
        "        if self.d == 1:\n",
        "            self.tiles_per_tiling_1d = int(self.tiles_per_dim[0])\n",
        "            self.offsets_1d = self.offsets[:, 0]\n",
        "            # Back-compat attribute names expected by the plotting code\n",
        "            self.tiles_per_tiling = self.tiles_per_tiling_1d\n",
        "            # plot helpers look for 'offsets' as 1D array; keep original 2D in _offsets2d\n",
        "            self._offsets2d = self.offsets\n",
        "            self.offsets = self.offsets_1d\n",
        "\n",
        "    @property\n",
        "    def n_features(self) -> int:\n",
        "        if self.hash_size is not None:\n",
        "            return self.hash_size\n",
        "        return int(self.n_tilings * np.prod(self.tiles_per_dim))\n",
        "\n",
        "    # ------------------------\n",
        "    # Offset generation\n",
        "    # ------------------------\n",
        "    def _make_offsets(self) -> np.ndarray:\n",
        "        \"\"\"Create an (n_tilings, d) array of offsets in [0, 1/tiles_i).\"\"\"\n",
        "        widths = 1.0 / self.tiles_per_dim.astype(float)\n",
        "        if self.deterministic:\n",
        "            # Evenly spaced along each dimension by fraction t/n_tilings of a bin width\n",
        "            t = np.arange(self.n_tilings, dtype=float).reshape(-1, 1)\n",
        "            base = (t / self.n_tilings)  # shape (n_tilings, 1)\n",
        "            return (base * widths)  # broadcast to (n_tilings, d)\n",
        "        else:\n",
        "            rng = np.random.default_rng(self.seed)\n",
        "            return rng.uniform(0.0, widths, size=(self.n_tilings, self.d))\n",
        "\n",
        "    # ------------------------\n",
        "    # Public API\n",
        "    # ------------------------\n",
        "    def active_indices(self, x: Union[float, Sequence[float]]) -> np.ndarray:\n",
        "        \"\"\"Return the global feature indices (length n_tilings) for input x.\n",
        "        x can be a scalar (1D) or a sequence of length d.\n",
        "        \"\"\"\n",
        "        u = self._normalize_to_unit(x)  # in [0,1]^d\n",
        "        inds = np.empty(self.n_tilings, dtype=np.int64)\n",
        "        for t in range(self.n_tilings):\n",
        "            idxs = self._tile_indices_for_tiling(u, t)\n",
        "            if self.hash_size is None:\n",
        "                inds[t] = t * self.tiles_per_tiling + self._linearize_indices(idxs)\n",
        "            else:\n",
        "                inds[t] = self._hash_index(t, idxs)\n",
        "        return inds\n",
        "\n",
        "    def encode_sparse(self, x: Union[float, Sequence[float]]):\n",
        "        \"\"\"Return (indices, values) for sparse features with value 1.0 for each tiling.\"\"\"\n",
        "        inds = self.active_indices(x)\n",
        "        vals = np.ones_like(inds, dtype=float)\n",
        "        return inds, vals\n",
        "\n",
        "    def encode_dense(self, x: Union[float, Sequence[float]]) -> np.ndarray:\n",
        "        \"\"\"Dense binary feature vector (mostly for debugging).\"\"\"\n",
        "        size = self.n_features\n",
        "        vec = np.zeros(size, dtype=float)\n",
        "        inds = self.active_indices(x)\n",
        "        vec[inds] = 1.0\n",
        "        return vec\n",
        "\n",
        "    # ------------------------\n",
        "    # Internals\n",
        "    # ------------------------\n",
        "    def _normalize_to_unit(self, x: Union[float, Sequence[float]]) -> np.ndarray:\n",
        "        x = np.array([x], dtype=float) if np.isscalar(x) else np.array(x, dtype=float)\n",
        "        assert x.size == self.d, f\"Expected input of dimension {self.d}, got {x.size}\"\n",
        "        lo = self.ranges[:, 0]\n",
        "        hi = self.ranges[:, 1]\n",
        "        u = (x - lo) / (hi - lo)\n",
        "        # Clamp to [0,1] to avoid numerical overflow in min/floor\n",
        "        return np.clip(u, 0.0, 1.0)\n",
        "\n",
        "    def _tile_indices_for_tiling(self, u: np.ndarray, t: int) -> np.ndarray:\n",
        "        offs = self.offsets[t] if self.d == 1 else self.offsets[t]\n",
        "        v = u + offs\n",
        "        idxs = np.empty(self.d, dtype=int)\n",
        "        for i in range(self.d):\n",
        "            if self.wrap[i]:\n",
        "                w = v[i] % 1.0\n",
        "            else:\n",
        "                w = min(v[i], 1.0 - 1e-12)\n",
        "            idxs[i] = int(np.floor(w * self.tiles_per_dim[i]))\n",
        "        return idxs\n",
        "\n",
        "    def _linearize_indices(self, idxs: np.ndarray) -> int:\n",
        "        return int(np.dot(idxs, self.tiles_strides))\n",
        "\n",
        "    def _hash_index(self, t: int, idxs: np.ndarray) -> int:\n",
        "        # Deterministic hash of (t, idxs...)\n",
        "        payload = np.array([t, *idxs.tolist()], dtype=np.int64).tobytes()\n",
        "        h = hashlib.sha256(payload).digest()\n",
        "        return int.from_bytes(h[:8], 'little') % int(self.hash_size)\n",
        "\n",
        "    # ------------------------\n",
        "    # 1D plotting helpers as methods (plotnine)\n",
        "    # ------------------------\n",
        "    def _assert_1d(self):\n",
        "        assert self.d == 1, \"plot_1d is only available for 1D tile coders\"\n",
        "\n",
        "    def _wrap_flag_1d(self) -> bool:\n",
        "        return bool(self.wrap[0])\n",
        "\n",
        "    def _bin_bounds_for_tiling_1d(self, offset: float) -> list:\n",
        "        \"\"\"Compute (start,end) intervals for bins in one tiling.\n",
        "        Honors wrap setting; non-wrap lets the last tile extend to 1.0.\n",
        "        \"\"\"\n",
        "        wrap = self._wrap_flag_1d()\n",
        "        T = int(self.tiles_per_tiling)\n",
        "        width = 1.0 / T\n",
        "        if wrap:\n",
        "            starts = (np.arange(T) * width - offset) % 1.0\n",
        "            ends = (starts + width) % 1.0\n",
        "            return list(zip(starts, ends))\n",
        "        else:\n",
        "            out = []\n",
        "            for b in range(T):\n",
        "                if b < T - 1:\n",
        "                    s = b * width - offset\n",
        "                    e = (b + 1) * width - offset\n",
        "                else:\n",
        "                    s = (T - 1) * width - offset\n",
        "                    e = 1.0\n",
        "                s = max(0.0, s)\n",
        "                e = min(1.0, e)\n",
        "                out.append((s, e))\n",
        "            return out\n",
        "\n",
        "    def _active_tile_for_tiling_1d(self, z: float, offset: float) -> int:\n",
        "        wrap = self._wrap_flag_1d()\n",
        "        T = int(self.tiles_per_tiling)\n",
        "        val = (z + offset) % 1.0 if wrap else min(z + offset, 1.0 - 1e-12)\n",
        "        return int(np.floor(val * T))\n",
        "\n",
        "    def build_tile_df_1d(self, z: float = 0.37) -> pd.DataFrame:\n",
        "        self._assert_1d()\n",
        "        wrap = self._wrap_flag_1d()\n",
        "        recs = []\n",
        "        for t in range(self.n_tilings):\n",
        "            bins = self._bin_bounds_for_tiling_1d(self.offsets[t])\n",
        "            active_tile = self._active_tile_for_tiling_1d(z, self.offsets[t])\n",
        "            for b, (s, e) in enumerate(bins):\n",
        "                is_active = (b == active_tile)\n",
        "                if wrap and s > e:\n",
        "                    recs.append(dict(\n",
        "                        tiling=t, tile=b, xmin=0.0, xmax=e, ymin=t-0.45, ymax=t+0.45,\n",
        "                        xcenter=(0.0 + e) / 2.0, ycenter=t, active=is_active\n",
        "                    ))\n",
        "                    recs.append(dict(\n",
        "                        tiling=t, tile=b, xmin=s, xmax=1.0, ymin=t-0.45, ymax=t+0.45,\n",
        "                        xcenter=(s + 1.0) / 2.0, ycenter=t, active=is_active\n",
        "                    ))\n",
        "                    continue\n",
        "                if e <= s:\n",
        "                    continue\n",
        "                recs.append(dict(\n",
        "                    tiling=t, tile=b, xmin=s, xmax=e, ymin=t-0.45, ymax=t+0.45,\n",
        "                    xcenter=(s + e) / 2.0, ycenter=t, active=is_active\n",
        "                ))\n",
        "        return pd.DataFrame(recs)\n",
        "\n",
        "    def plot_1d(self, z: float = 0.37, title: Union[None, str] = None):\n",
        "        \"\"\"Return a plotnine ggplot object visualizing 1D tiles and active bin per tiling.\"\"\"\n",
        "        self._assert_1d()\n",
        "        df = self.build_tile_df_1d(z)\n",
        "        ttl = title if title else f\"TileCoder (wrap={'True' if self._wrap_flag_1d() else 'False'})\"\n",
        "        p = (\n",
        "            ggplot(df, aes(xmin='xmin', xmax='xmax', ymin='ymin', ymax='ymax', fill='active'))\n",
        "            + geom_rect(alpha=0.35, color='black', size=0.2)\n",
        "            + geom_text(aes(x='xcenter', y='ycenter', label='tile'), size=6)\n",
        "            + geom_vline(xintercept=z, linetype='dashed')\n",
        "            + scale_y_reverse()\n",
        "            + labs(title=f\"{ttl} — z={z:.2f}\", x='z in [0,1]', y='Tiling index')\n",
        "            + theme_bw()\n",
        "            + theme(figure_size=(10, 2))\n",
        "        )\n",
        "        return p\n",
        "\n",
        "    def show_1d(self, z: float = 0.37, title: Union[None, str] = None):\n",
        "        \"\"\"Convenience: build and immediately render the 1D plot (if in an environment that supports .show()).\"\"\"\n",
        "        return self.plot_1d(z, title).show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us test the tile coder class by first creating four tilings with 10 tiles each. Note the deterministic argument which makes deterministic shifts. In the other example, five tilings are created with 20 tiles each using random shifts."
      ],
      "metadata": {
        "id": "e9QLVlqlNHF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tc = TileCoder(n_tilings=4, tiles_per_dim=10, deterministic=True)\n",
        "tc.show_1d(z=0.65, title='TileCoder (deterministic offset)')\n",
        "print(\"Active:\", tc.active_indices(0.65))\n",
        "tc = TileCoder(n_tilings=5, tiles_per_dim=20, deterministic=False)\n",
        "tc.show_1d(z=0.02, title='TileCoder (random offset)')\n",
        "print(\"Active:\", tc.active_indices(0.02))\n",
        "print(\"Number of features (length of w):\", tc.n_features)"
      ],
      "metadata": {
        "id": "CYOD1RcMM-Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let us create the agent."
      ],
      "metadata": {
        "id": "mYjwDtY0I3Up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Agent Tiles (non-generic)\n",
        "\n",
        "class AgentApproxTile(AgentApprox):\n",
        "    \"\"\"\n",
        "    An agent for on-policy prediction with tile features.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, n_tilings=8, tiles_per_tiling=10):\n",
        "        \"\"\"\n",
        "        Initializes the AgentApproxTile.\n",
        "\n",
        "        Args:\n",
        "            env: The environment to interact with.\n",
        "            n_tilings: The number of tilings.\n",
        "            tiles_per_tiling: The number of tiles per tiling.\n",
        "        \"\"\"\n",
        "        self.tc = TileCoder(n_tilings=n_tilings, tiles_per_dim=tiles_per_tiling)\n",
        "        super().__init__(env, len_w = self.tc.n_features)\n",
        "\n",
        "    def _to_unit(self, s: int):\n",
        "        return s / (self.env.n + 1.0)\n",
        "\n",
        "    def tile_feature_idx(self, s: int):\n",
        "        \"\"\"\n",
        "        Generates tile features for a given state.\n",
        "\n",
        "        Args:\n",
        "            s: The state number (1-based).\n",
        "\n",
        "        Returns:\n",
        "            A numpy array of tile features.\n",
        "        \"\"\"\n",
        "        z = self._to_unit(s)\n",
        "        return self.tc.active_indices(z)\n",
        "\n",
        "    def gradient_mc_episode(self, alpha):\n",
        "        \"\"\"\n",
        "        Runs a single episode using the current policy and updates the weights\n",
        "        using gradient Monte Carlo.\n",
        "\n",
        "        Args:\n",
        "            alpha: The overall step size for the gradient Monte Carlo update.\n",
        "                This is divided by the number of tilings.\n",
        "        \"\"\"\n",
        "        s = self.env.reset()\n",
        "        trajectory = []\n",
        "        done = False\n",
        "        while not done:\n",
        "            trajectory.append(s)\n",
        "            s, r, done = self.env.step(s)\n",
        "        G = r\n",
        "        for s_vis in trajectory:\n",
        "            idx = self.tile_feature_idx(s_vis)\n",
        "            v_hat = self.w[idx].sum()\n",
        "            self.w[idx] += (alpha / self.tc.n_tilings) * (G - v_hat)\n",
        "\n",
        "    def semi_gradient_td0_episode(self, alpha):\n",
        "        \"\"\"\n",
        "        Runs a single episode using the current policy and updates the weights\n",
        "        using semi-gradient TD(0).\n",
        "\n",
        "        Args:\n",
        "            alpha: The overall step size for the semi-gradient TD(0) update.\n",
        "                This is divided by the number of tilings.\n",
        "        \"\"\"\n",
        "        s = self.env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            s_n, r, done = self.env.step(s)\n",
        "            idx = self.tile_feature_idx(s)\n",
        "            v_hat = self.w[idx].sum()\n",
        "            if not done:\n",
        "                idx_n = self.tile_feature_idx(s_n)\n",
        "                v_hat_n = self.w[idx_n].sum()\n",
        "                self.w[idx] += (alpha / self.tc.n_tilings) * (r + v_hat_n - v_hat)\n",
        "                s = s_n\n",
        "            else:\n",
        "                self.w[idx] += (alpha / self.tc.n_tilings) * (r - v_hat)\n",
        "\n",
        "\n",
        "    def get_approx_values(self):\n",
        "        \"\"\"\n",
        "        Returns the approximated state values\n",
        "\n",
        "        Returns:\n",
        "            A numpy array containing the approximated state values.\n",
        "        \"\"\"\n",
        "        return np.array([self.w[self.tile_feature_idx(s)].sum() for s in range(1, self.env.n + 1)])"
      ],
      "metadata": {
        "id": "Gs3onEaVJJYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that $\\alpha$ is the overall step size and is divided by the number of tilings when used in the algorithms.\n",
        "\n",
        "Let us test the algorithm using code:"
      ],
      "metadata": {
        "id": "c-89Qv0CMfoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset_rng()\n",
        "agent = AgentApproxTile(env, n_tilings=25, tiles_per_tiling=5)\n",
        "df = run_and_plot(env, agent, episodes=10000, eval_every=500, alpha=0.0001)"
      ],
      "metadata": {
        "id": "GofG7P-pNIhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your turn\n",
        "\n",
        "Comment on the results.\n",
        "\n",
        "- How many parameters are there?\n",
        "  \n",
        "  *Answer here*\n",
        "\n",
        "- Where are the state value approx. poor? Why?\n",
        "\n",
        "  *Answer here*\n",
        "\n",
        "- What would you expect the results to be if using five tilings and 25 tiles per tiling?\n",
        "\n",
        "  *Answer here*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p-z63ggFNMQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "# code here"
      ],
      "metadata": {
        "id": "be-T6XmrOfbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting Step-Size Parameters Manually\n",
        "\n",
        "In tabular prediction, a single update with target $U_t$ is $$V(S_t) \\leftarrow V(S_t) + \\alpha\\,[\\,U_t - V(S_t)\\,].$$ Think of $\\alpha$ as how much you trust the newest target. Big enough to make visible progress, small enough to avoid creating noise. A step size of $\\alpha = 1/10$ would take about 10 experiences to converge approximately to their mean target, and if we wanted to learn in 100 experiences\n",
        "we would use $\\alpha = 1/100$.\n",
        "\n",
        "With general function approximation there is not such a clear notion of number of\n",
        "experiences with a state, as each state may be similar to and dissimilar from all the others\n",
        "to various degrees.\n",
        "Suppose you wanted to learn in about $\\tau$ experiences\n",
        "with substantially the same feature vector. Then, good rule of thumb for setting the step-size parameter of linear SGD methods is:\n",
        "\n",
        "\\begin{equation}\n",
        "\t\\alpha = (\\tau \\mathbb{E}[\\textbf{x}^T\\textbf{x}])^{-1}\n",
        "\\end{equation}\n",
        "\n",
        "where $\\textbf{x}$ is a random feature vector chosen from the same distribution as input vectors\n",
        "will be in the SGD. This method works best if the feature vectors do not vary greatly in\n",
        "length; ideally $\\textbf{x}^T\\textbf{x}$ is a constant.\n",
        "\n",
        "For instance, for tile coding with $n$ tillings we have that\n",
        "$$\\textbf{x}^T\\textbf{x}=n,$$ and\n",
        "$$\\alpha = (\\tau n)^{-1}.$$\n",
        "\n",
        "Given a scalar state and a polynomial of degree $d$ with no cross-effects we have that $$\\textbf{x}^T\\textbf{x}=(1, s, s^2, \\ldots s^d)^T(1, s, s^2, \\ldots s^d) = \\sum_{i=0}^{d} (s^i)^2.$$ If the state is normalised to [-1,1] and we assume a uniform distribution then\n",
        "$$\\mathbb{E}[(s^i)^2] = 1/(2i + 1),$$ and\n",
        "$$\\alpha = (\\tau\\sum_{i=0}^{d} 1/(2i+1)])^{-1}.$$\n",
        "\n"
      ],
      "metadata": {
        "id": "ehvyvh5RvYFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your turn\n",
        "\n",
        "Consider the random walk and assume that you want to estimate the state values in 10000 experiences.\n",
        "\n",
        "- Find a good step-size when using a 4-degree polynomial.\n",
        "\n",
        "  *Answer here*\n",
        "\n",
        "- Find a good step-size when using 20 tilings.\n",
        "\n",
        "  *Answer here*"
      ],
      "metadata": {
        "id": "19-13zjbQ59V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your turn\n",
        "\n",
        "Often you may also chose a decreasing step-size as you learn more. If your RMS decrease smoothly then you may have a to low step-size. If your RMS osciliate, then you may have a to high step-size.\n",
        "\n",
        "Consider the state aggregation case with 10 groups.\n",
        "\n",
        "- Start with 200 episodes and $\\alpha = 0.1$.\n",
        "- Afterwards find the standard deviation $\\delta$ of the last 4 RMS values and set your step-size to $\\delta/g$ where you choose $g$.\n",
        "- Run until you get an RMS value below 0.07.\n"
      ],
      "metadata": {
        "id": "vDcWUWimddP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution\n",
        "\n",
        "# code here"
      ],
      "metadata": {
        "id": "vvHJMQVpZB_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises"
      ],
      "metadata": {
        "id": "R5XqElk0IWgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "Show that tabular methods, such as those presented in Part I of this book, are a special case of linear function approximation. What would the feature vectors be?\n",
        "\n",
        "*Answer here*\n"
      ],
      "metadata": {
        "id": "4284ZTgLJes9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "Suppose we believe that one of the two state dimensions (let us say $x_1$) is more likely to have an effect on the value function than is the other ($x_2$)\n",
        "\n",
        "What kind of tilings could be used to take advantage of this prior knowledge?\n",
        "\n",
        "*Answer here*\n"
      ],
      "metadata": {
        "id": "EVT6Ll3gKqGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "Suppose you are using tile coding to transform a seven-dimensional continuous state space into binary feature vectors to estimate a state value function. You believe that the dimensions do not interact strongly, so you decide to use eight tilings of each dimension separately. That is, $7 \\cdot 8 = 56$ tilings. In addition, in case there are some pairwise interactions between the dimensions, you also take all pairs of dimensions and tile each pair conjunctively. You make\n",
        "two tilings for each pair of dimensions, making a grand total of 21 \\cdot 2 + 56 = 98 tilings.\n",
        "\n",
        "Given these feature vectors, you suspect that you still have to average out some noise, so you decide that you want learning to be gradual, taking about 10 presentations with the same feature vector first.\n",
        "\n",
        "What step-size parameter should you use? Why?\n",
        "\n",
        "*Answer here*"
      ],
      "metadata": {
        "id": "cCiViI-NTMFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "Consider the car rental problem described in [Exercise 7.8.2](https://bss-osca.github.io/rl/07_mdp-2.html#sec-mdp-2-car).\n",
        "\n",
        "Approximate the state value under the policy\n",
        "$$\n",
        "a = \\begin{cases}\n",
        "\\lfloor x/5\\rfloor + 1 & x > 5, y < 5 \\\\\n",
        "-(\\lfloor y/5\\rfloor + 1) & y > 5, x < 5 \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "The environment is given below."
      ],
      "metadata": {
        "id": "ADqD5YrhqcE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Car rental environment\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class RLEnvCar:\n",
        "    \"\"\"\n",
        "    Car rental environment (two locations) with Poisson demand/returns.\n",
        "\n",
        "    States:\n",
        "        Strings of the form \"x,y\" where x,y ∈ {0, …, 20} denote the number of\n",
        "        cars available at location 1 and 2 at the start of a day.\n",
        "\n",
        "    Actions:\n",
        "        Move a cars overnight from location 1 → 2 (a > 0) or 2 → 1 (a < 0).\n",
        "        The feasible action set for state (x, y) is:\n",
        "            a ∈ [ -min(5, y, 20 - x),  min(5, x, 20 - y) ]  (integers)\n",
        "        i.e., you can move at most 5 cars and cannot exceed capacity 20 at either site.\n",
        "\n",
        "    Dynamics (per step):\n",
        "        1) Apply action a: cars become x_bar = x - a, y_bar = y + a.\n",
        "        2) Demand at site i ~ Poisson(lD[i]); fulfilled up to available cars.\n",
        "        3) Returns at site i ~ Poisson(lH[i]); add to remaining cars.\n",
        "        4) Cap each site at 20 cars.\n",
        "\n",
        "    Reward:\n",
        "        +10 for each rental fulfilled at either site, minus a movement cost\n",
        "        of 2 per car moved:  r = 10 * (served_x + served_y) - 2 * |a|.\n",
        "\n",
        "    Notes:\n",
        "        - No terminal states; always returns a next state.\n",
        "        - Randomness comes from Poisson draws; set `seed` in __init__ for reproducibility.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lD = [3, 4], lH = [3, 2], seed = None):\n",
        "        \"\"\"\n",
        "        Initialize the environment.\n",
        "\n",
        "        Args:\n",
        "            lD: Mean demand rates [λ1, λ2] for locations 1 and 2.\n",
        "                Defaults to [3, 4].\n",
        "            lH: Mean return rates [λ1, λ2] for locations 1 and 2.\n",
        "                Defaults to [3, 2].\n",
        "            seed: Optional integer seed for the RNG.\n",
        "        \"\"\"\n",
        "        self.lD = list(lD)\n",
        "        self.lH = list(lH)\n",
        "        self.rng_seed = seed\n",
        "        self.rng = np.random.default_rng(self.rng_seed)\n",
        "        self.n_per_dim = 21\n",
        "        self.state_counts = np.zeros((self.n_per_dim, self.n_per_dim), dtype=int)\n",
        "\n",
        "    def reset(self):\n",
        "        self.state_counts = np.zeros((self.n_per_dim, self.n_per_dim), dtype=int)\n",
        "        self.rng = np.random.default_rng(self.rng_seed)\n",
        "\n",
        "    def get_dist(self):\n",
        "        mat = np.zeros((self.n_per_dim, self.n_per_dim))\n",
        "        for x in range(self.n_per_dim):\n",
        "            for y in range(self.n_per_dim):\n",
        "                mat[x, y] = self.state_counts[x, y]\n",
        "        return mat / mat.sum()\n",
        "\n",
        "    def get_states(self):\n",
        "        \"\"\"\n",
        "        Return all state keys.\n",
        "\n",
        "        Returns:\n",
        "            List of \"x,y\" strings for x,y in 0..20.\n",
        "        \"\"\"\n",
        "        return [f\"{x},{y}\" for x in range(21) for y in range(21)]\n",
        "\n",
        "    def get_actions(self, s):\n",
        "        \"\"\"\n",
        "        Return all feasible actions (as strings) for a given state.\n",
        "\n",
        "        Args:\n",
        "            s: State label \"x,y\".\n",
        "\n",
        "        Returns:\n",
        "            List of integer actions encoded as strings.\n",
        "        \"\"\"\n",
        "        x_str, y_str = s.split(\",\")\n",
        "        x = int(x_str)\n",
        "        y = int(y_str)\n",
        "\n",
        "        low = -min(5, y, 20 - x)\n",
        "        high = min(5, x, 20 - y)\n",
        "        return [str(a) for a in range(low, high + 1)]\n",
        "\n",
        "    def get_time_step_data(self, s, a):\n",
        "        \"\"\"\n",
        "        Compute one-step transition and reward from (s, a).\n",
        "\n",
        "        Args:\n",
        "            s: State label as int list.\n",
        "            a: Action label as int\n",
        "\n",
        "        Returns:\n",
        "            Dict with:\n",
        "                - 'r': reward (float).\n",
        "                - 'sN': next state as list (int)\n",
        "        \"\"\"\n",
        "        x, y = s\n",
        "        self.state_counts[x, y] += 1\n",
        "\n",
        "        # # Validate action feasibility (optional but helpful)\n",
        "        # feasible = set(self.get_actions(s))\n",
        "        # if str(a_int) not in feasible:\n",
        "        #     raise ValueError(f\"Action {a_int} not feasible in state {s}. Feasible: {sorted(feasible)}\")\n",
        "\n",
        "        # Apply move\n",
        "        x_bar = x - a\n",
        "        y_bar = y + a\n",
        "\n",
        "        # Sample demands and returns\n",
        "        d_x = int(self.rng.poisson(self.lD[0]))\n",
        "        d_y = int(self.rng.poisson(self.lD[1]))\n",
        "        h_x = int(self.rng.poisson(self.lH[0]))\n",
        "        h_y = int(self.rng.poisson(self.lH[1]))\n",
        "\n",
        "        # Rentals served (capped by available cars after moves)\n",
        "        served_x = min(d_x, x_bar)\n",
        "        served_y = min(d_y, y_bar)\n",
        "\n",
        "        # Cars remaining after rentals, then returns, then cap at 20\n",
        "        x_next = min(20, x_bar - served_x + h_x)\n",
        "        y_next = min(20, y_bar - served_y + h_y)\n",
        "\n",
        "        # Reward: rental revenue minus movement cost\n",
        "        reward = 10.0 * (served_x + served_y) - 2.0 * abs(a)\n",
        "\n",
        "        return [x_next, y_next], reward\n",
        "\n"
      ],
      "metadata": {
        "id": "UKHeByqwWRxD",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = RLEnvCar()"
      ],
      "metadata": {
        "id": "s1W5FC2SvEKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The true state values can be loaded for comparison:"
      ],
      "metadata": {
        "id": "4T1qW9wnyhq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title True state values\n",
        "\n",
        "# Read csv file\n",
        "df_mdp_policy = pd.read_csv('https://drive.google.com/uc?id=1PaMKISjMEcc4uw1Qw4bLhPVbir3-wru3')\n",
        "df_mdp_policy.head()\n",
        "\n",
        "pt = (\n",
        "    ggplot(df_mdp_policy, aes(\"x\", \"y\", label = \"action\", color = \"action\"))\n",
        "    + geom_label(size = 7)\n",
        "    + theme(legend_position='none')\n",
        "    + labs(title = \"Policy used\")\n",
        ")\n",
        "pt.show()\n",
        "\n",
        "df_mdp_policy['action'] = pd.Categorical(df_mdp_policy['action'])\n",
        "pt = (\n",
        "    ggplot(df_mdp_policy, aes(\"x\", \"y\", label = \"v_int\", color = \"action\"))\n",
        "    + geom_label(size = 7)\n",
        "    # + theme(legend_position='none')\n",
        "    + labs(title = \"State values of policy for gamma = 0.5\")\n",
        ")\n",
        "pt.show()"
      ],
      "metadata": {
        "id": "wU768nmZwuxm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What you have to do is\n",
        "\n",
        "1) Define an agent class `AgentCar` that uses one of the features (e.g polynomials of tiles)\n",
        "\n",
        "2) Define a function `run_and_plot_car` that estimates the parameters and plot the RMS.\n",
        "\n",
        "3) Run the function over a set of decreasing step sizes until get a good approximation."
      ],
      "metadata": {
        "id": "qK9Zx25owG0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution - Agent car with tiles (car rental problem)\n",
        "\n",
        "class AgentCar:\n",
        "    # your code"
      ],
      "metadata": {
        "id": "duF-QZk8t5Lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution - run_and_plot_car\n",
        "\n",
        "def run_and_plot_car(env, agent, samples, gamma, alpha, eval_every = 10, show_pt_rms = True):\n",
        "    # your code"
      ],
      "metadata": {
        "id": "T2-IX8HkXf_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Solution - Runs\n",
        "\n",
        "env.reset()\n",
        "agent = AgentCar(env, n_tilings=4, tiles_per_dim=[5, 5])\n",
        "\n",
        "## Set v_opt\n",
        "mat_v_opt = np.zeros(shape = (21, 21))\n",
        "for index, row in df_mdp_policy.iterrows():  # fill\n",
        "    x, y = int(row['x']), int(row['y'])\n",
        "    mat_v_opt[x, y] = row['v']\n",
        "agent.set_v_opt(mat_v_opt)\n",
        "\n",
        "# First run\n",
        "# your code"
      ],
      "metadata": {
        "id": "F8zDJn4L-ObO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}